{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data and AI Reference Architecture !!! Abstract: In this reference architecture, we define the architecture patterns and best practices for developing Data and AI intensive applications or Intelligent applications . We consider how data, data governance, analytics and machine learning practices join with the agile life cycle of cloud native application development to enable the development and delivery of Intelligent Applications . When we consider the Data and AI Reference Architecture in terms of developing intelligent applications it becomes apparent that we are looking at bringing together three architecture patterns: Cloud Native application architecture patterns Data architecture patterns AI architecture patterns We can think of the Data and AI reference architecture being the sum of these architecture patterns, plus the joins between them. The joins are also key to understanding how Software Engineers, Data Engineers, and Data Scientist, relate and work together in the development of such Intelligent applications . As we look to methodology for Developing such solutions we need to consider a prescriptive approach which brings those project stakeholders together to be successful. To do this we adopt a four layers approach: COLLECT data to make them easier to consume and accessible ORGANIZE data to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere INFUSE to operationalize AI with trust and transparency The figure below represents how those layers are related to each other: IBM Data and AI Conceptual Architecture Based on the above prescriptive approach, we cans see that a Data centric and AI reference architecture needs to implement the four layers as shown in the following diagram. This architecture diagram illustrates 1. strong data collection and management capabilities 1. inside a 'multi cloud data platform' (dark blue area) 1. on which AI capabilities are plugged in to support analyze done by data scientists (machine learning workbench and business analytics). The data platform addresses the data collection and transformation tasks to move data to a (cloud)local highly scalable data store . However we must also recognise that there are cases, where data movement can or must be avoided. For examples where: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) * network partition (data source unreliable e.g. remote IoT Gateway) In such cases the data platform provides a virtualization capability which can open a view on remote data sources without moving data. In Analyze data scientists need to perform: data analysis , which includes making sense of the data using data visualization . feature engineering to define the features they need to build an ML model. Then to build the model , the development environment provides the AI frameworks and helps the data scientist to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to a machine learning cluster. Once the model provides acceptable accuracy level, it can be published so that it can be consumed or infused within an application or as a service. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). Once the model is deployed, monitoring capabilities, ensures the model is still accurate and not biased. The intelligent application , is represented as a combination of capabilities at the top of the diagram, it can be an application we develop, a business process, an ERP or CRM application, etc. running anywhere on cloud fog, or mist computing. The intelligent application, accesses the deployed model, sing APIs, and may consumes pre-built models or Cognitive services, such as: speech to text and text to speech services image recognition , a tone analyzer services * NLU Natural Language Understanding and chatbot services. Data and AI reference architecture capabilities In this view of the reference architecture we have zoomed in a level to show the detail of how we realize the required capabilities. The boundary rectangles are color coded to map the higher level purposes: collect, organize, analyze and infuse with each icon representing a capability. This diagram becomes the foundation of the data AI reference architecture an we will expend details for Data preparation AI model development Application runtime environments in following notes. Mapping to products The following diagrams illustrate the product mapping to realize the required capability.","title":"Introduction"},{"location":"#data-and-ai-reference-architecture","text":"!!! Abstract: In this reference architecture, we define the architecture patterns and best practices for developing Data and AI intensive applications or Intelligent applications . We consider how data, data governance, analytics and machine learning practices join with the agile life cycle of cloud native application development to enable the development and delivery of Intelligent Applications . When we consider the Data and AI Reference Architecture in terms of developing intelligent applications it becomes apparent that we are looking at bringing together three architecture patterns: Cloud Native application architecture patterns Data architecture patterns AI architecture patterns We can think of the Data and AI reference architecture being the sum of these architecture patterns, plus the joins between them. The joins are also key to understanding how Software Engineers, Data Engineers, and Data Scientist, relate and work together in the development of such Intelligent applications . As we look to methodology for Developing such solutions we need to consider a prescriptive approach which brings those project stakeholders together to be successful. To do this we adopt a four layers approach: COLLECT data to make them easier to consume and accessible ORGANIZE data to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere INFUSE to operationalize AI with trust and transparency The figure below represents how those layers are related to each other:","title":"Data and AI Reference Architecture"},{"location":"#ibm-data-and-ai-conceptual-architecture","text":"Based on the above prescriptive approach, we cans see that a Data centric and AI reference architecture needs to implement the four layers as shown in the following diagram. This architecture diagram illustrates 1. strong data collection and management capabilities 1. inside a 'multi cloud data platform' (dark blue area) 1. on which AI capabilities are plugged in to support analyze done by data scientists (machine learning workbench and business analytics). The data platform addresses the data collection and transformation tasks to move data to a (cloud)local highly scalable data store . However we must also recognise that there are cases, where data movement can or must be avoided. For examples where: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) * network partition (data source unreliable e.g. remote IoT Gateway) In such cases the data platform provides a virtualization capability which can open a view on remote data sources without moving data. In Analyze data scientists need to perform: data analysis , which includes making sense of the data using data visualization . feature engineering to define the features they need to build an ML model. Then to build the model , the development environment provides the AI frameworks and helps the data scientist to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to a machine learning cluster. Once the model provides acceptable accuracy level, it can be published so that it can be consumed or infused within an application or as a service. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). Once the model is deployed, monitoring capabilities, ensures the model is still accurate and not biased. The intelligent application , is represented as a combination of capabilities at the top of the diagram, it can be an application we develop, a business process, an ERP or CRM application, etc. running anywhere on cloud fog, or mist computing. The intelligent application, accesses the deployed model, sing APIs, and may consumes pre-built models or Cognitive services, such as: speech to text and text to speech services image recognition , a tone analyzer services * NLU Natural Language Understanding and chatbot services.","title":"IBM Data and AI Conceptual Architecture"},{"location":"#data-and-ai-reference-architecture-capabilities","text":"In this view of the reference architecture we have zoomed in a level to show the detail of how we realize the required capabilities. The boundary rectangles are color coded to map the higher level purposes: collect, organize, analyze and infuse with each icon representing a capability. This diagram becomes the foundation of the data AI reference architecture an we will expend details for Data preparation AI model development Application runtime environments in following notes.","title":"Data and AI reference architecture capabilities"},{"location":"#mapping-to-products","text":"The following diagrams illustrate the product mapping to realize the required capability.","title":"Mapping to products"},{"location":"data/","text":"Data Platform details Data is a fundamental element of every business and therefore fundamental to our architecture. Data is our record of current state of the business, the history of what has happened, and is the base which enables us to predict what may happen in the future. However, on its own data doesn't do anything and to realise value from data we have to do something with it, we have to understand it, and act on it. Typically this requires something other than the data such as a computer program, a query, or a user (machine or person). Perhaps one of the biggest and most complex challenges comes with managing data. Data is inert; it is not self-organizing or even self-understanding. So how do we manage the data , how do we organize an attach meaning so that the data can easily be used bu the business, or a computer program etc. The DIKW pyramid , provides a simple visualisation of the value chain growing from data to Wisdom : Data is the base with the least amount of perceived usefulness. Information has higher value than data. Knowledge has higher value than information. Wisdom has the highest perceived value of all. To move up the value chain data requires something else such as a program, a machine, or even a person\u2014to add to understanding to it so that it becomes Information . By organizing and classifying information , the value chain expands from data and information to be regarded as knowledge . At the top of the data value chain is Wisdom . Wisdom comes from a combination of inert data, which is the fundamental raw material in the modern digital age, combined with a series of progressive traits such as: perspective context understanding learning * the ability to reason. With the advent of Cognitive computing and artificial intelligence these traits can now be attributed to both a person and a machine. The IBM AI Ladder loosely parallels the DIKW pyramid in that the AI Ladder represents a progressive movement towards value creation within an enterprise. Increased value can be gained from completing activities at each step of the AI Ladder, with the potential to recognize higher levels of value, the higher the ladder is climbed. The AI Ladder contains four discrete levels: Collect Organize Analyze Infuse. The first rung is Collect , a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. In linking to the DIKW pyramid we could say that, data lies below the first rung, recognizing the inert nature of data. The AI Ladder progresses through the rungs to infuse , a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities. Collect \u2013 Making Data Simple and Accessible The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above Organize \u2013 Trusted, Governed Analytics The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility Analyze \u2013 Insights On-Demand The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities, from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions Infuse \u2013 Operationalize AI with Trust and Transparency The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise Data as a differentiator Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects . Making data enabled and active There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions. Towards Data-Centricity Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"Data Platform Details"},{"location":"data/#data-platform-details","text":"Data is a fundamental element of every business and therefore fundamental to our architecture. Data is our record of current state of the business, the history of what has happened, and is the base which enables us to predict what may happen in the future. However, on its own data doesn't do anything and to realise value from data we have to do something with it, we have to understand it, and act on it. Typically this requires something other than the data such as a computer program, a query, or a user (machine or person). Perhaps one of the biggest and most complex challenges comes with managing data. Data is inert; it is not self-organizing or even self-understanding. So how do we manage the data , how do we organize an attach meaning so that the data can easily be used bu the business, or a computer program etc. The DIKW pyramid , provides a simple visualisation of the value chain growing from data to Wisdom : Data is the base with the least amount of perceived usefulness. Information has higher value than data. Knowledge has higher value than information. Wisdom has the highest perceived value of all. To move up the value chain data requires something else such as a program, a machine, or even a person\u2014to add to understanding to it so that it becomes Information . By organizing and classifying information , the value chain expands from data and information to be regarded as knowledge . At the top of the data value chain is Wisdom . Wisdom comes from a combination of inert data, which is the fundamental raw material in the modern digital age, combined with a series of progressive traits such as: perspective context understanding learning * the ability to reason. With the advent of Cognitive computing and artificial intelligence these traits can now be attributed to both a person and a machine. The IBM AI Ladder loosely parallels the DIKW pyramid in that the AI Ladder represents a progressive movement towards value creation within an enterprise. Increased value can be gained from completing activities at each step of the AI Ladder, with the potential to recognize higher levels of value, the higher the ladder is climbed. The AI Ladder contains four discrete levels: Collect Organize Analyze Infuse. The first rung is Collect , a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. In linking to the DIKW pyramid we could say that, data lies below the first rung, recognizing the inert nature of data. The AI Ladder progresses through the rungs to infuse , a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities.","title":"Data Platform details"},{"location":"data/#collect-making-data-simple-and-accessible","text":"The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above","title":"Collect \u2013 Making Data Simple and Accessible"},{"location":"data/#organize-trusted-governed-analytics","text":"The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility","title":"Organize \u2013 Trusted, Governed Analytics"},{"location":"data/#analyze-insights-on-demand","text":"The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities, from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions","title":"Analyze \u2013 Insights On-Demand"},{"location":"data/#infuse-operationalize-ai-with-trust-and-transparency","text":"The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise","title":"Infuse \u2013 Operationalize AI with Trust and Transparency"},{"location":"data/#data-as-a-differentiator","text":"Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects .","title":"Data as a differentiator"},{"location":"data/#making-data-enabled-and-active","text":"There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions.","title":"Making data enabled and active"},{"location":"data/#towards-data-centricity","text":"Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"Towards Data-Centricity"},{"location":"dataprinciples/","text":"Data and AI Architecture Principles As we look to the Data and AI reference architecture it becomes essential that we have deep understanding of the range of uses, how value is realized from data, the dependencies between data AI and applications, and the constraints which we may face in multi-cloud environments with compute power, data storage and network connectivity/latency. Through consideration of these aspects we have pulled out the following Guiding Principles which help to guide both the architecture patterns and our thinking or methodology for how to approach developing Intelligent applications : There exists a spectrum of concerns ranging from single source of truth to data hyper personalisation. Different roles require specialised data stores with redundancy and replication between them Different application patterns apply different data specialisations. There is a dependency between AI and Data Management. With an Intelligent Application there is a Data concern, an AI model management concern, a Multi cloud deployment concern. As you constrain scalability and network connectivity you also constrain data storage, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in a time series. Looking ahead with the modern digital business we will increasingly see our users have access to terabytes, petabytes, or even exabytes of data. If this data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed , the data is not just useless, it becomes a liability. Embracing this challenge by putting in the right systems, built on a well defined data an AI architecture and following proven methodologies for developing new Intelligent Applications will become an essential step for all enterprises.","title":"Data Principles"},{"location":"dataprinciples/#data-and-ai-architecture-principles","text":"As we look to the Data and AI reference architecture it becomes essential that we have deep understanding of the range of uses, how value is realized from data, the dependencies between data AI and applications, and the constraints which we may face in multi-cloud environments with compute power, data storage and network connectivity/latency. Through consideration of these aspects we have pulled out the following Guiding Principles which help to guide both the architecture patterns and our thinking or methodology for how to approach developing Intelligent applications : There exists a spectrum of concerns ranging from single source of truth to data hyper personalisation. Different roles require specialised data stores with redundancy and replication between them Different application patterns apply different data specialisations. There is a dependency between AI and Data Management. With an Intelligent Application there is a Data concern, an AI model management concern, a Multi cloud deployment concern. As you constrain scalability and network connectivity you also constrain data storage, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in a time series. Looking ahead with the modern digital business we will increasingly see our users have access to terabytes, petabytes, or even exabytes of data. If this data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed , the data is not just useless, it becomes a liability. Embracing this challenge by putting in the right systems, built on a well defined data an AI architecture and following proven methodologies for developing new Intelligent Applications will become an essential step for all enterprises.","title":"Data and AI Architecture Principles"},{"location":"deployment/","text":"Deployment","title":"Physical model execution"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"methodology/","text":"Methodology Warning UNDER CONSTRUCTION Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. !!! This is a full fledged Enterprise Methodology addressing concerns of multinational data center deployments. For the purpose of MVP (Minimum Viable Product) creation we recommend to start with the Lightweight IBM Cloud Garage Method for Data Science which one one hand is ideally suited for MVP / POC (Proof of Concept) work, on the other hand is compatible with the methodology described here so that scale-up is straightforward. This method always guarantees to have a deployable product at any point in time, a concept know if CICD (Continuous Integration, Continuous Delivery) and therefore is also compatible to such a software development process. Integrating data - devops and AI analytics methodologies Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. Personas Before going deeper into the different lifecycles, we need to define the personas involved in those lifecycles. The table below present the icons we are using in subsquent figures, with a short description for the role. Devops lifecycle The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before entering the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. AIOps lifecycle The ai-analytics development lifecycle (ADLC) supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, the ADLC relies on feedback mechanisms to help enhance machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. The ADLC iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. DataOps The data development lifecycle (DDLC) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DDLC is impacted by the SDLC and the ADLC. It iterates on both the incorporation of business requirements and on the manifestation of data. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. Integrating the cycles Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. Data Sciences Introduction The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper management, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories: Descriptive analytics This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years. Predictive analytics This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels. Prescriptive analytics Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis. Concepts Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification) Regression learning problem when the goal is to predict continuous value output Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. A feature is an attribute used as input for the model to train. Other names include dimension or column. Algorithm selection The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem. Another best practice is to use Linear Regression / Logistic Regression as baseline and try out other algorithms to improve on it. Challenges There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets. The Garage Method for Cloud with DataFirst Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem. Personas Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist Differences between analysts and data scientists The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Methodology"},{"location":"methodology/#methodology","text":"Warning UNDER CONSTRUCTION Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. !!! This is a full fledged Enterprise Methodology addressing concerns of multinational data center deployments. For the purpose of MVP (Minimum Viable Product) creation we recommend to start with the Lightweight IBM Cloud Garage Method for Data Science which one one hand is ideally suited for MVP / POC (Proof of Concept) work, on the other hand is compatible with the methodology described here so that scale-up is straightforward. This method always guarantees to have a deployable product at any point in time, a concept know if CICD (Continuous Integration, Continuous Delivery) and therefore is also compatible to such a software development process.","title":"Methodology"},{"location":"methodology/#integrating-data-devops-and-ai-analytics-methodologies","text":"Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value.","title":"Integrating data - devops and AI analytics methodologies"},{"location":"methodology/#personas","text":"Before going deeper into the different lifecycles, we need to define the personas involved in those lifecycles. The table below present the icons we are using in subsquent figures, with a short description for the role.","title":"Personas"},{"location":"methodology/#devops-lifecycle","text":"The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before entering the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion.","title":"Devops lifecycle"},{"location":"methodology/#aiops-lifecycle","text":"The ai-analytics development lifecycle (ADLC) supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, the ADLC relies on feedback mechanisms to help enhance machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. The ADLC iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method.","title":"AIOps lifecycle"},{"location":"methodology/#dataops","text":"The data development lifecycle (DDLC) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DDLC is impacted by the SDLC and the ADLC. It iterates on both the incorporation of business requirements and on the manifestation of data. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work.","title":"DataOps"},{"location":"methodology/#integrating-the-cycles","text":"Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling.","title":"Integrating the cycles"},{"location":"methodology/#data-sciences-introduction","text":"The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper management, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories:","title":"Data Sciences Introduction"},{"location":"methodology/#descriptive-analytics","text":"This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years.","title":"Descriptive analytics"},{"location":"methodology/#predictive-analytics","text":"This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels.","title":"Predictive analytics"},{"location":"methodology/#prescriptive-analytics","text":"Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis.","title":"Prescriptive analytics"},{"location":"methodology/#concepts","text":"Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification) Regression learning problem when the goal is to predict continuous value output Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. A feature is an attribute used as input for the model to train. Other names include dimension or column.","title":"Concepts"},{"location":"methodology/#algorithm-selection","text":"The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem. Another best practice is to use Linear Regression / Logistic Regression as baseline and try out other algorithms to improve on it.","title":"Algorithm selection"},{"location":"methodology/#challenges","text":"There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets.","title":"Challenges"},{"location":"methodology/#the-garage-method-for-cloud-with-datafirst","text":"Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"The Garage Method for Cloud with DataFirst"},{"location":"methodology/#personas_1","text":"Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist","title":"Personas"},{"location":"methodology/#differences-between-analysts-and-data-scientists","text":"The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Differences between analysts and data scientists"},{"location":"methodology/lightweight-architectural-decisions/","text":"Architectural Decisions Guidelines An architectural decisions guide for data science The IBM Data and Analytics Reference Architecture defines possible components on an abstract level. The goal of this article is to choose from the required components and assign real and concrete architectural components to it. The article contains information to complement the \u201cLightweight IBM Cloud Garage Method for data science\u201d article. The method is highly iterative, so any findings during this process can result in changes to architectural decisions. However, there are never any wrong architectural decisions because the decisions take into account all the knowledge available at a certain point in time. Therefore, it\u2019s important to document why a decision was made. The following figure shows the IBM Reference Architecture for Cloud Analytics. The following sections provide guidelines for each component. They explain what technology should be chosen and if the component needs to be included at all. Data Source The data source is an internal or external data source that includes relational databases; web pages; CSV, JSON, or text files; and video and audio data. Architectural decision guidelines With the data source, there is not much to decide because in most cases, the type and structure of a data source is already defined and controlled by stakeholders. However, if there is some control over the process, the following principles should be considered: What does the delivery point look like? Enterprise data mostly lies in relational databases serving OLTP systems. It\u2019s typically a bad practice to access those systems directly, even in read-only mode because ETL processes are running SQL queries against those systems, which can hinder performance. One exception is IBM Db2 Workload Manager because it allows OLAP and ETL workloads to run in parallel with an OLTP workload without performance degradation of OLTP queries using intelligent scheduling and prioritizing mechanisms. You can read more about IBM Db2 Workload Manager. Does real-time data need to be considered? Real-time data comes in various shapes and delivery methods. The most prominent include MQTT telemetry and sensor data (for example, data from the IBM Watson IoT Platform), a simple REST HTTP endpoint that needs to be polled, or a TCP or UDP socket. If no downstream real-time processing is required, that data can be staged (for example, using Cloud Object Store). If downstream real-time processing is necessary, read the section on Streaming analytics further down in this article. Enterprise data Cloud-based solutions tend to extend the enterprise data model. Therefore, it might be necessary to continuously transfer subsets of enterprise data to the cloud or access those in real time through a VPN API gateway. Architectural decision guidelines Moving enterprise data to the cloud can be costly. Therefore, it should be considered only if necessary. For example, if user data is handled in the cloud is it sufficient to store an anonymized primary key. If transfer of enterprise data to the cloud is unavoidable, privacy concerns and regulations must be addressed. Then, there are two ways to access it. Batch sync from an enterprise data center to the cloud Real-time access to subsets of data using VPN and an API gateway Technology guidelines Secure gateway Secure gateway lets cloud applications access specified hosts and ports in a private data center though an outbound connection. Therefore, no external inbound access is required. You can go to the Secure Gateway service on IBM Cloud for more information. Lift Lift allows you to migrate on-premises data to cloud databases in a very efficient manner. Read more about the IBM Lift CLI . Rocket Mainframe Data The Rocket Mainframe Data service uses similar functions for batch-style data integration as Lift, but is dedicated to IBM Mainframes. You can read more information about the service. Streaming analytics The current state-of-the-art is batch processing. But, sometimes the value of a data product can be increased tremendously by adding real-time analytics capabilities because most of world\u2019s data loses value within seconds. Think of stock market data or the fact that a vehicle camera captures a pedestrian crossing a street. A streaming analytics system allows for real-time data processing. Think of it like running data against a continuous query instead of running a query against a finite data set. Architectural decision guidelines There is a relatively limited set of technologies for real-time stream processing. The most important questions to be asked are: What throughput is required? What latency is accepted? Which data types must be supported? What type of algorithms run on the system? Only relational algebra or advanced modeling? What\u2019s the variance of the workload and what are the elasticity requirements? What type of fault tolerance and delivery guarantees are necessary? Technology Guidelines On IBM Cloud, there are many service offerings for real-time data processing that I explain in the following sections along with guidelines for when to use them. Apache Spark and Apache Spark Structured Streaming Apache Spark is often the primary choice when it comes to cluster-grade data processing and machine learning. If you\u2019re already using it for batch processing, Apache Spark Structured Streaming should be the first thing to evaluate. This way, you can have technology homogeneity and batch and streaming jobs can be run together (for example, joining a stream of records against a reference table). What throughput is required? Apache Spark Structured Streaming supports the same throughput as in batch mode. What latency is accepted? In Apache Spark v2.3, the Continuous Processing mode has been introduced, brining latency down to one millisecond. Which data types must be supported? Apache Spark is strong at structured and semi-structured data. Audio and video data can\u2019t benefit from Apache Spark\u2019s accelerators Tungsten and Catalyst. What type of algorithms run on the system? Only relational algebra or advanced modeling? Apache Spark Structured Streaming supports relational queries as well as machine learning, but machine learning is only supported on sliding and tumbling windows. What\u2019s the variance of the workload and what are the elasticity requirements? Through their fault tolerant nature, Apache Spark clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? Apache Spark Structured Streaming supports exactly once delivery guarantees and depending on the type of data source, complete crash fault tolerance. IBM Streams IBM Streams is a fast streaming engine. Originally designed for low-latency, high throughput network monitoring applications, IBM Streams has its roots in cybersecurity. What throughput is required? IBM Streams can handle high data throughput rates, up to millions of events or messages per second. What latency is accepted? IBM Streams latency goes down to microseconds. Which data types must be supported? Through IBM Streams binary transfer mode, any type of data type can be supported. What type of algorithms run on the system? Only relational algebra or advanced modeling? IBM Streams in its core supports all relational algebra. Also, through toolkits, various machine learning algorithms can be used. Toolkits are an open system, and there are many third-party toolkits. What\u2019s the variance of the workload and what are the elasticity requirements? Through its fault tolerant nature, IBM Streams clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? IBM Streams supports exactly once delivery guarantees and complete crash fault tolerance. NodeRED NodeRED is a lightweight streaming engine. Implemented on top of Node.js in JavaScript, it can even run on a 64 MB memory footprint (for example, running on a Raspberry PI). What throughput is required? NodeRED\u2019s throughput is bound to processing capabilities of a single CPU core, through Node.js\u2019s event processing nature. For increased throughput, multiple instances of NodeRED have been used in parallel. Parallelization is not built in and needs to be provided by the application developer. What latency is accepted? Latency is also dependent on the CPU configuration and on the throughput because high throughput congests the event queue and increases latency. Which data types must be supported? NodeRED best supports JSON streams, although any data type can be nested into JSON. What type of algorithms run on the system? Only relational algebra or advanced modeling? NodeRED has one of the most extensive ecosystems of open source third-party modules. Although advanced machine learning is not supported natively, there are plans by IBM to add those. What\u2019s the variance of the workload and what are the elasticity requirements? Because parallelization is a responsibility of the application developer, for independent computation a round-robin load balancing scheme supports linear scalability and full elasticity. What type of fault tolerance and delivery guarantees are necessary? NodeRED has no built-in fault tolerance and no delivery guarantees. Apache Nifi Apache Nifi is maintained by Hortonworks and is part of the IBM Analytics Engine Service. What throughput is required? Nifi can handle hundreds of MBs on a single node and can be configured to handle multiple GBs in cluster mode. What latency is accepted? Nifi\u2019s latency is in seconds. Through message periodization, the tradeoff between throughput and latency can be tweaked. Which data types must be supported? Nifi best supports structured data streams, although any data type can be nested. What type of algorithms run on the system? Only relational algebra or advanced modeling? Nifi supports relational algebra out of the box, but custom processors can be built. What\u2019s the variance of the workload and what are the elasticity requirements? Nifi can be easily scaled up without restarts, but scaling down requires stopping and starting the Nifi system. What type of fault tolerance and delivery guarantees are necessary? Nifi supports end-to-end guaranteed exactly once delivery. Also, fault tolerance can be configured, but automatic recovery is not possible. Another important feature is backpressure and pressure release, which causes the upstream nodes to stop accepting new data and discarding unprocessed data if an age threshold is exceeded. Others There are also other technologies like Apache Kafka, Samza, Apache Flink, Apache Storm, Total.js Flow, Eclipse Kura, and Flogo that might be worth looking at if the ones mentioned don\u2019t meet all of your requirements. Data Integration In the data integration stage, data is cleansed, transformed, and if possible, downstream features are added Architectural Decision Guidelines There are numerous technologies for batch data processing, which is the technology used for data integration. The most important questions to be asked are: What throughput is required? Which data types must be supported? What source systems must be supported? What skills are required? Technology Guidelines IBM Cloud has many service offerings for data integration, and the following section explains them and gives guidelines on which one to use. Apache Spark Apache Spark is often the first choice when it comes to cluster-grade data processing and machine learning. Apache Spark is a flexible option that also supports writing integration processes in SQL. But it\u2019s missing a user interface. What throughput is required? Apache Spark scales linearly, so throughput is just a function of the cluster size. Which data types must be supported? Apache Spark works best with structured data, but binary data is supported as well. What source systems must be supported? Apache Spark can access various SQL and NoSQL data as well as file sources. A common data source architecture allows adding capabilities, and it has third-party project functions as well. What skills are required? Advanced SQL skills are required and you should have some familiarity with either Java programming, Scala, or Python. IBM Data Stage on Cloud IBM Data Stage is a sophisticated ETL (Extract Transform Load) tool. Its closed source and supports visual editing. What throughput is required? Data Stage can be used in cluster mode, which supports scale-out. Which data types must be supported? Data Stage has its roots in traditional Data Warehouse ETL and concentrates on structured data. What source systems must be supported? Again, Data Stage concentrates on relational database systems, but files can also be read, even on Object Store. In addition, data sources can be added using plug-ins that are implemented in the Java language. What skills are required? Because Data Stage is a visual editing environment, the learning curve is low. No programming skills are required. Others It\u2019s important to know that data integration is mostly done using ETL tools, plain SQL, or a combination of both. ETL tools are mature technology, and many ETL tools exist. On the other hand, if streaming analytics is part of the project, it\u2019s a good idea to check whether one of those technologies fits your requirements because reuse of such a system reduces technology heterogeneity. Data Repository This is the persistent storage for your data. Architectural decision guidelines There are lots of technologies for persisting data, and most of them are relational databases. The second largest group are NoSQL databases, with file systems (including Cloud Object Store) forming the last one. The most important questions to be asked are: What is the impact of storage cost? Which data types must be supported? How well must point queries (on fixed or dynamic dimensions) be supported? How well must range queries (on fixed or dynamic dimensions) be supported? How well must full table scans be supported? What skills are required? What\u2019s the requirement for fault tolerance and backup? What are the constant and peak ingestion rates? What amount of storage is needed? What does the growth pattern look like? What are the retention policies? Technology guidelines IBM cloud has numerous service offerings for SQL, NoSQL, and file storage. The following section explains them and provides guidelines on when to use which one. Relational databases Dash DB is the Db2 BLU on the Cloud offering from IBM that features column store, advanced compression, and execution on SIMD instructions sets (that is, vectorized processing). But there are other options in IBM Cloud such as Informix, PostgreSQL, and MySQL. What is the impact of storage cost? Relational databases (RDBMS) have the highest requirements on storage quality. Therefore, the cost of relational storage is always the highest. Which data types must be supported? Relational databases are meant for structured data. Although there are column data types for binary data that can be swapped for cheaper storage, this is just an add-on and not a core function of relational databases. How well must point queries (on fixed or dynamic dimensions) be supported? RDBMS are great for point queries because an index can be created on each column. How well must range queries (on fixed or dynamic dimensions) be supported? RDBMS are great for range queries because an index can be created on each column. How well must full table scans be supported? RDBMS are trying to avoid full table scans in their SQL query optimizers. Therefore, performance is not optimized for full table scans (for example, contaminating page caches). What skills are required? You should have SQL skills, and if a cloud offering isn\u2019t chosen, you should also have database administrator (DBA) skills for the specific database. What\u2019s the requirement for fault tolerance and backup? RDMBS support continuous backup and crash fault tolerance. For recovery, the system might need to go offline. What are the constant and peak ingestion rates? Inserts using SQL are relatively slow, especially if the target table contains many indexes that must be rebalanced and updated. Some RDBMS support bulk inserts from files by bypassing the SQL engine, but then the table usually needs to go offline for that period. What amount of storage is needed? RDMBS perform very well to around 1 TB of data. Going beyond that is complex and needs advanced cluster setups. What does the growth pattern look like? RDBMS support volume management, so continuous growth usually isn\u2019t a problem, even at run time. For shrinking, the system might need to be taken offline. What are the retention policies? RDBMS usually support automated retention mechanisms to delete old data automatically. NoSQL databases The most prominent NoSQL databases like Apache CouchDB, MongoDB, Redis, RethinkDB, ScyllaDB (Cassandra), and InfluxCloud are supported. What is the impact of storage cost? NoSQL databases are usually storage fault-tolerant, by default. Therefore, quality requirements on storage are less, which brings down storage cost. Which data types must be supported? Although NoSQL databases are meant for structured data as well, they usually use JSON as a storage format, which can be enriched with binary data. Although, a lot of binary data attached to a JSON document can bring the performance down as well. How well must point queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves point query performance. How well must range queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves range query performance. How well must full table scans be supported? NoSQL databases perform very well at full table scans. The performance is only limited by the I/O bandwidth to storage. What skills are required? Typically, special query language skills are required for the application developer and if a cloud offering isn\u2019t chosen, database administrator (DBA) skills are needed for the specific database. What\u2019s the requirement for fault tolerance and backup? NoSQL databases support backups in different ways. But some aren\u2019t supporting online backup. NoSQL databases are usually crash fault tolerant, but for recovery, the system might need to go offline. What are the constant and peak ingestion rates? Usually, no indexes need to be updated and data doesn\u2019t need to be mapped to pages. Ingestion rates are usually only bound to I/O performance of the storage system. What amount of storage is needed? RDMBS perform well to approximately 10 \u2013 100 TB of data. Cluster setups on NoSQL databases are much more straightforward than on RDBMS. Successful setups with >100 nodes and > 100.000 database reads and writes per second have been reported. What does the growth pattern look like? The growth of NoSQL databases is not a problem. Volumes can be added at run time. For shrinking, the system might need to be taken offline. What are the retention policies? NoSQL databases don\u2019t support automated retention mechanisms to delete old data automatically. Therefore, this must be implemented manually, resulting in range queries on the data corpus. Object storage Cloud object storage makes it possible to store practically limitless amounts of data. It is commonly used for data archiving and backup, for web and mobile applications, and as scalable, persistent storage for analytics. So, let\u2019s take a look. What is the impact of storage cost? Object storage is the cheapest option for storage. Which data types must be supported? Because object storage resembles a file system, any data type is supported. How well must point queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. How well must range queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. Therefore, range queries on a single defined column (for example, data) can be achieved through hierarchical folder structures. How well must full table scans be supported? Full table scans are bound only by the I/O bandwidth of the object storage. What skills are required? On a file level, working with object storage is much like working with any file system. Through Apache SparkSQL and IBM Cloud SQL Query, data in Object storage can be accessed with SQL. Because object storage is a cloud offering, no administrator skills are required. IBM Object Storage is available for on-premises as well using an appliance box. What\u2019s the requirement for fault tolerance and backup? Fault tolerance and backup is completely handled by the cloud provider. Object storage supports intercontinental data center replication for high-availability out of the box. What are the constant and peak ingestion rates? Ingestion rates to object storage is bound by the uplink speed to the object storage system. What\u2019s the amount of storage needed? Object storage scales to the petabyte range. What does the growth pattern look like? Growth and shrinking on object storage is fully elastic. What are the retention policies? Retention of data residing in object storage must be done manually. Hierarchical file and folder layout that is based on data and time helps here. Some object storage support automatic movement of infrequently accessed files to colder storage (colder means less cost, but also less performance, or even higher cost of accesses to files). Discovery and exploration This component allows for visualization and creation of metrics of data. Architectural decision guidelines In various process models, data visualization and exploration is one of the first steps. Similar tasks are also applied in traditional data warehousing and business intelligence. So when choosing a technology, ask the following questions: What type of visualizations are needed? Are interactive visualizations needed? Are coding skills available or required? What metrics can be calculated on the data? Do metrics and visualization need to be shared with business stakeholders? Technology guidelines IBM cloud has many service offerings for data exploration. Some of the offerings are open source, and some aren\u2019t. Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust are all open source and supported in IBM Cloud. Some of these components have overlapping features and some of them have complementary features. This can be determined by answering the architectural questions. What type of visualizations are needed? Matplotlib supports the widest range of possible visualizations including run chars, histograms, box-plots, and scatter plots. PixieDust (as of V1.1.11) supports tables, bar charts, line charts, scatter plots, pie charts, histograms, and maps. Are interactive visualizations needed? Matplotlib creates static plots and PixieDust supports interactive ones. Are coding skills available or required? Matplotlib requires coding skills, but PixieDust does not. For computing metrics, some code is necessary. What metrics can be calculated on the data? Using scikit-learn and pandas, all state-of-the-art metrics are supported. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of Jupyter Notebooks, also using a fine-grained user and access management system. SPSS Modeler SPSS Modeler is available in the cloud and also as stand-alone product. What type of visualizations are needed? SPSS Modeler supports the following visualizations out of the box: Bar Pie 3D Bar 3D Pie Line Area 3D Area Path Ribbon Surface Scatter Bubble Histogram Box Map You can get more information in the IBM Knowledge Center . Are interactive visualizations needed? SPSS Modeler Visualizations are not interactive. Are coding skills available or required? SPSS Modeler doesn\u2019t require any coding skills. What metrics can be calculated on the data? All state-of-the-art metrics are supported using the Data Audit node. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of SPSS Modeler Flows, also using a fine-grained user and access management system. However, those might not be suitable to stakeholders. Actionable insights This is where most of your work fits in. It\u2019s where you create and evaluate your machine learning and deep learning models. Architectural decision guidelines There are numerous technologies for creating and evaluating machine learning and deep learning models. Although different technologies differ in function and performance, those differences are usually miniscule. Therefore, the questions you should ask yourself are: What are the available skills regarding programming languages? What are the costs of skills regarding programming languages? What are the available skills regarding frameworks? What are the costs of skills regarding frameworks? Is model interchange required? Is parallel- or GPU-based training or scoring required? Do algorithms need to be tweaked or new algorithms be developed? Technology guidelines Because there\u2019s an abundance of open and closed source technologies, I\u2019m highlighting the most relevant ones in this article. Although it\u2019s the same for the other sections as well, decisions made in this section are very prone to change due to the iterative nature of this process model. Therefore, changing or combining multiple technologies is no problem, although the decisions that led to those changes should be explained and documented. SPSS Modeler This article has already introduced SPSS Modeler. What are the available skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the costs of skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the available skills regarding frameworks? SPSS is an industry leader, so skills are generally available. What are the costs of skills regarding frameworks? Expert costs are usually lower in UI-based tools than in programming frameworks. Is model interchange required? SPSS Modeler supports PMML. Is parallel- or GPU-based training or scoring required? SPSS Modeler supports scaling through IBM Analytics Server or IBM Watson Studio using Apache Spark. Do algorithms need to be tweaked or new algorithms be developed? SPSS Modeler algorithms can\u2019t be changed, but you can add algorithms or customizations using the R language. R/R-Studio R and R-Studio are standards for open source-based data science. They\u2019re supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? R programming skills are usually widely available because it\u2019s a standard programming language in many natural science-based university curriculums. It can be acquired rapidly because it is a procedural language with limited functional programming support. What are the costs of skills regarding programming languages? Costs of R programming are usually low. What are the available skills regarding frameworks? R is not only a programming language but also requires knowledge of tooling (R-Studio), and especially knowledge of the R library (CRAN) with 6000+ packages. What are the costs of skills regarding frameworks? Expert costs are correlated with knowledge of the CRAN library and years of experience and in the range of usual programmer costs. Is model interchange required? Some R libraries support exchange of models, but it is not standardized. Is parallel- or GPU-based training or scoring required? Some R libraries support scaling and GPU acceleration, but it is not standardized. Do algorithms need to be tweaked or new algorithms be developed? R needs algorithms to be implemented in C/C++ to run fast. So, tweaking and custom development usually involves C/C++ coding. Python, pandas and scikit-learn Although R and R-Studio have been the standard for open source-based data science for a while, Python, pandas, and scikit-learn are right behind them. Python is a much cleaner programming language than R and easier to learn. Pandas is the Python equivalent to R data frames, supporting relational access to data. Finally, scikit-learn nicely groups all necessary machine learning algorithms together. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Python skills are very widely available because Python is a clean and easy to learn programming language. What are the costs of skills regarding programming languages? Because of Python\u2019s properties mentioned above, the cost of Python programming skills is very low. What are the available skills regarding frameworks? Pandas and scikit-learn are very clean and easy-to-learn frameworks. Therefore, skills are widely available. What are the costs of skills regarding frameworks? Because of the properties mentioned above, the costs of skills are very low. Is model interchange required? All scikit-learn models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? Neither GPU nor scale-out is supported, although scale-up capabilities can be added individually to make use of multiple cores. Do algorithms need to be tweaked or new algorithms be developed? scikit-learn algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with the numpy library. Therefore, tweaking and adding algorithms is straightforward. Python, Apache Spark and SparkML Although Python, pandas, and scikit-learn are more widely adopted, the Apache Spark ecosystem is catching up, especially because of its scaling capabilities. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Apache Spark supports Python, Java programming, Scala, and R as programming languages. What are the costs of skills regarding programming languages? The costs depend on what programming language is used, with Python typically the cheapest. What are the available skills regarding frameworks? Apache Spark skills are in high demand and usually not available. What are the costs of skills regarding frameworks? Apache Spark skills are in high demand and are usually expensive. Is model interchange required? All SparkML models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. However, GPU\u2019s are only supported through third-party libraries. Do algorithms need to be tweaked or new algorithms be developed? As in scikit-learn, algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with built-in Apache Spark libraries. Therefore, tweaking and adding algorithms is straightforward. Apache SystemML When it comes to relational data processing, SQL is a leader, mainly because an optimizer takes care of optimal query executions. Think of SystemML as an optimizer for linear algebra that\u2019s capable of creating optimal execution plans for jobs running on data parallel frameworks like Apache Spark. What are the available skills regarding programming languages? SystemML has two domain-specific languages (DSL) with R and Python syntax. What are the costs of skills regarding programming languages? Although the DSLs are like R and Python, there is a learning curve involved. What are the available skills regarding frameworks? SystemML skills are very rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? SystemML models can be (de)serialized. PMML is not supported. SystemML can import and run Caffe2 and Keras models. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. SystemML uses this property. In addition, GPU\u2019s are supported as well. Do algorithms need to be tweaked or new algorithms be developed? Although SystemML comes with a large set of pre-implemented algorithms for machine learning and deep learning, its strengths are in tweaking existing algorithms or implementing new ones because the DSL allows for concentrating on the mathematical implementation of the algorithm. The rest is handled by the framework. This makes it an ideal choice for these kind of tasks. Keras and TensorFlow TensorFlow is one of the most widely used deep learning frameworks. At its core, it is a linear algebra library supporting automatic differentiation. TensorFlow\u2019s Python-driven syntax is relatively complex. Therefore, Keras provides an abstraction layer on top of TensorFlow. Both frameworks are seamlessly supported in IBM Cloud through Watson Studio and Watson Machine Learning. What are the available skills regarding programming languages? Python is the core programming language for Keras and TensorFlow. What are the costs of skills regarding programming languages? Because Python programmers are more abundant than Java/Scala programmers, the costs are less. What are the available skills regarding frameworks? Keras and TensorFlow skills are relatively rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? Keras and TensorFlow have their own model exchange formats. There are converters from and to ONNX. Is parallel- or GPU-based training or scoring required? Running TensorFlow on top of ApacheSpark is supported through TensorFrames and TensorSpark. Keras models can be run on ApacheSpark using DeepLearning4J and SystemML. Both of the latter frameworks also support GPUs. TensorFlow (and therefore, Keras) support GPU natively as well. Do algorithms need to be tweaked or new algorithms be developed? TensorFlow is a linear algebra execution engine. Therefore, it\u2019s optimally suited for tweaking and creating new algorithms. Keras is a very flexible deep learning library that supports many neural network layouts. Applications and Data Products Models are fine, but their value rises when they can be consumed by the ordinary business user. Therefore, you must create a data product. Data products don\u2019t necessarily need to stay on the cloud. They can be pushed to mobile or enterprise applications. Architectural decision guidelines In contrast to machine learning and deep learning frameworks, the space of frameworks to create data product is tiny. This might reflect what the current state-of-the-art technology in data science concentrates on. Depending on the requirements, data products are relatively visualization-centric after a lot of use input data has been gathered. They also might involve asynchronous workflows as batch data integration and model training and scoring is performed within the workflow. Questions to ask about the technology are: What skills are present for developing a data product? What skills are necessary for developing a data product? Is instant feedback required or is batch processing accepted? What\u2019s the degree of customization needed? * What\u2019s the target audience? Is cloud scale deployment for a public use base required? Technology guidelines Currently, only a limited set of frameworks and technologies is available in different categories. In the following section, I\u2019ve explained the most prominent examples. R-Shiny R-Shiny is a great framework for building data products. Closely tied to the R language, it enables data scientists to rapidly create a UI on top of existing R-scripts. What skills are present for developing a data product? R-Shiny requires R development skills. So, it best fits into an R development ecosystem. What skills are necessary for developing a data product? Although based on R, R-Shiny needs additional skills. For experienced R developers, the learning curve is steep and additional knowledge to acquire is minimal. Is instant feedback required or is batch processing accepted? The messaging model of R-Shiny supports instant UI feedback when server-side data structures are updated. Therefore, the response time is independent of the framework and should be considered and resolved programmatically on the server side. What\u2019s the degree of customization needed? Although R-Shiny is an extensible framework, extending it requires a deep understanding of the framework and R-Technology. Out of the box, there is a large set of UI widgets end elements supported, allowing for very customizable applications. If requirements go beyond those capabilities, costly extensions are required. What\u2019s the target audience? Is cloud scale deployment for a public use base required? R-Shiny applications look very professional, although quite distinguishable. Therefore, the target audience must accept the UI design limitations. R-Shiny is best dynamically scaled horizontally in a container environment like Kubernetes. Ideally, every user session runs in its own container because R and R-Shiny are very sensitive to main memory shortages Node-RED Although Node-RED is a no-code/low-code data flow/data integration environment, because of its modular nature it supports various extensions including the dash boarding extension. This extension allows for fast creation of user interfaces including advanced visualizations that are updated in real-time. What skills are present for developing a data product? Due to the completely graphical user interface-based software development approach, only basic programming skills are required to build data products with Node-RED. What skills are necessary for developing a data product? Any resource familiar with flow-based programming as used in many state-of-the-art ETL and data mining tools will have a fast start with Node-RED. Basic JavaScript knowledge is required for creating advanced flows and for extending the framework. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? Node-RED is a Node.js/JavaScript-based framework. Custom UI widgets require advanced Node.js development skills. What\u2019s the target audience? Is cloud scale deployment for a public use base required? The Node-RED dashboard can be deployed for a pubic user base as long as the limitations regarding UI customization are acceptable. Because Node.js runs on a single threaded event loop, scaling must be done horizontally, preferably using a containerized environment. Note: The Internet of Things Starter kit in IBM Cloud supports horizontal scaling out of the box. D3 When it comes to custom application development, D3 is one of the most prominent and most widely used visualization widget frameworks with a large open source ecosystem contributing a lot of widgets for every desirable use case. There\u2019s a good chance that you can find a D3 widget to fit your use case. What skills are present for developing a data product? D3 fits best into an existing, preferably JavaScript-based developer ecosystem, although JavaScript is only required on the client side. Therefore, on the server side, any REST-based endpoints in any programming language are supported. One example is REST endpoints accessed by a D3 UI provided by Apache Livy that encapsulates Apache Spark jobs. What skills are necessary for developing a data product? D3 requires sophisticated D3 skills and at least client-side JavaScript skills. Skills in a JavaScript AJAX framework like AngularJS are highly recommended. On the server side, capabilities of providing REST endpoints to the D3 applications are required. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? D3 applications usually are implemented from scratch. Therefore, this solution provides the most flexibility to the end user. What\u2019s the target audience? Is cloud scale deployment for a public use base required? As a cloud native application, a D3-based data product can provide all capabilities for horizontal and vertical scaling and full adoption to user requirements. Security, information governance and systems management This important step can be easily forgotten. It\u2019s important to control who has access to which information for many compliance regulations. In addition, modern data science architectures involve many components that require operational aspects as well. Architectural decision guidelines Data privacy is a major challenge in many data science projects. Questions that you should ask are: What granularity is required for managing user access to data assets? Are existing user registries required to be integrated? Who is taking care of operational aspects? What are the requirements for data retention? What level of security against attacks from hackers is required? Technology guidelines Again, there\u2019s a lot of software for this as well as ways to solve the requirements involved in this topic. I\u2019ve chosen representative examples for this article. Internet services Deploying a productive client-facing web application brings with it serious risks. IBM Cloud Internet Services provides global points of presence (PoPs). It includes domain name service (DNS), global load balancer (GLB), distributed denial of service (DDoS) protection, web application firewall (WAF), transport layer security (TLS), and caching. What level of security against attacks from hackers is required? Internet Services is using services from CloudFlare, the world leader in this space. IBM App ID Identity Management allows for cloud-based user and identity management for web and mobile applications, APIs, and back-end systems. Cloud users can sign up and sign in with App ID\u2019s scalable user registry or social login with Google or Facebook. Enterprise users can be integrated using SAML 2.0 federation. What granularity is required for managing user access to data assets? IBM App ID supports user management but no group/roles. Therefore, fine-grained access must be managed within the application. Are existing user registries required to be integrated? IBM App ID supports registry integration using SAML 2.0 federation. Object Storage Object Storage is a standard when it comes to modern, cost-effective cloud storage. What granularity is required for managing user access to data assets? IBM Identity and Access Management (IAM) integration allows for granular access control at the bucket-level using role-based policies. What are the requirements for data retention? Object storage supports different storage classes for frequently accessed data, occasionally accessed data, and long-term data retention with standard, vault, and cold vault. The Flex class allows for dynamic data access and automates this process. Physical deletion of data still must be triggered externally. Who is taking care of operational aspects? Regional and cross-region resiliency options allow for increased data reliability. What level of security against attacks from hackers is required? All data is encrypted at rest and in flight by default. Keys are automatically managed by default, but optionally can be self-managed or managed using IBM Key Protect. IBM Cloud PaaS/SaaS IBM Cloud PaaS/SaaS eliminates operational aspects from data science projects because all components involved are managed by IBM. Who is taking care of operational aspects? In PaaS/SaaS, cloud operational aspects are being taken care of by the cloud provider. Summary This article complements the \u201cLightweight IBM Cloud Garage Method for data science\u201d article and helps you with deployment considerations.","title":"Architectural Decisions Guidelines"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decisions-guidelines","text":"An architectural decisions guide for data science The IBM Data and Analytics Reference Architecture defines possible components on an abstract level. The goal of this article is to choose from the required components and assign real and concrete architectural components to it. The article contains information to complement the \u201cLightweight IBM Cloud Garage Method for data science\u201d article. The method is highly iterative, so any findings during this process can result in changes to architectural decisions. However, there are never any wrong architectural decisions because the decisions take into account all the knowledge available at a certain point in time. Therefore, it\u2019s important to document why a decision was made. The following figure shows the IBM Reference Architecture for Cloud Analytics. The following sections provide guidelines for each component. They explain what technology should be chosen and if the component needs to be included at all.","title":"Architectural Decisions Guidelines"},{"location":"methodology/lightweight-architectural-decisions/#data-source","text":"The data source is an internal or external data source that includes relational databases; web pages; CSV, JSON, or text files; and video and audio data.","title":"Data Source"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines","text":"With the data source, there is not much to decide because in most cases, the type and structure of a data source is already defined and controlled by stakeholders. However, if there is some control over the process, the following principles should be considered: What does the delivery point look like? Enterprise data mostly lies in relational databases serving OLTP systems. It\u2019s typically a bad practice to access those systems directly, even in read-only mode because ETL processes are running SQL queries against those systems, which can hinder performance. One exception is IBM Db2 Workload Manager because it allows OLAP and ETL workloads to run in parallel with an OLTP workload without performance degradation of OLTP queries using intelligent scheduling and prioritizing mechanisms. You can read more about IBM Db2 Workload Manager. Does real-time data need to be considered? Real-time data comes in various shapes and delivery methods. The most prominent include MQTT telemetry and sensor data (for example, data from the IBM Watson IoT Platform), a simple REST HTTP endpoint that needs to be polled, or a TCP or UDP socket. If no downstream real-time processing is required, that data can be staged (for example, using Cloud Object Store). If downstream real-time processing is necessary, read the section on Streaming analytics further down in this article.","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#enterprise-data","text":"Cloud-based solutions tend to extend the enterprise data model. Therefore, it might be necessary to continuously transfer subsets of enterprise data to the cloud or access those in real time through a VPN API gateway.","title":"Enterprise data"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_1","text":"Moving enterprise data to the cloud can be costly. Therefore, it should be considered only if necessary. For example, if user data is handled in the cloud is it sufficient to store an anonymized primary key. If transfer of enterprise data to the cloud is unavoidable, privacy concerns and regulations must be addressed. Then, there are two ways to access it. Batch sync from an enterprise data center to the cloud Real-time access to subsets of data using VPN and an API gateway","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines","text":"","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#secure-gateway","text":"Secure gateway lets cloud applications access specified hosts and ports in a private data center though an outbound connection. Therefore, no external inbound access is required. You can go to the Secure Gateway service on IBM Cloud for more information.","title":"Secure gateway"},{"location":"methodology/lightweight-architectural-decisions/#lift","text":"Lift allows you to migrate on-premises data to cloud databases in a very efficient manner. Read more about the IBM Lift CLI .","title":"Lift"},{"location":"methodology/lightweight-architectural-decisions/#rocket-mainframe-data","text":"The Rocket Mainframe Data service uses similar functions for batch-style data integration as Lift, but is dedicated to IBM Mainframes. You can read more information about the service.","title":"Rocket Mainframe Data"},{"location":"methodology/lightweight-architectural-decisions/#streaming-analytics","text":"The current state-of-the-art is batch processing. But, sometimes the value of a data product can be increased tremendously by adding real-time analytics capabilities because most of world\u2019s data loses value within seconds. Think of stock market data or the fact that a vehicle camera captures a pedestrian crossing a street. A streaming analytics system allows for real-time data processing. Think of it like running data against a continuous query instead of running a query against a finite data set.","title":"Streaming analytics"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_2","text":"There is a relatively limited set of technologies for real-time stream processing. The most important questions to be asked are: What throughput is required? What latency is accepted? Which data types must be supported? What type of algorithms run on the system? Only relational algebra or advanced modeling? What\u2019s the variance of the workload and what are the elasticity requirements? What type of fault tolerance and delivery guarantees are necessary?","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_1","text":"On IBM Cloud, there are many service offerings for real-time data processing that I explain in the following sections along with guidelines for when to use them.","title":"Technology Guidelines"},{"location":"methodology/lightweight-architectural-decisions/#apache-spark-and-apache-spark-structured-streaming","text":"Apache Spark is often the primary choice when it comes to cluster-grade data processing and machine learning. If you\u2019re already using it for batch processing, Apache Spark Structured Streaming should be the first thing to evaluate. This way, you can have technology homogeneity and batch and streaming jobs can be run together (for example, joining a stream of records against a reference table). What throughput is required? Apache Spark Structured Streaming supports the same throughput as in batch mode. What latency is accepted? In Apache Spark v2.3, the Continuous Processing mode has been introduced, brining latency down to one millisecond. Which data types must be supported? Apache Spark is strong at structured and semi-structured data. Audio and video data can\u2019t benefit from Apache Spark\u2019s accelerators Tungsten and Catalyst. What type of algorithms run on the system? Only relational algebra or advanced modeling? Apache Spark Structured Streaming supports relational queries as well as machine learning, but machine learning is only supported on sliding and tumbling windows. What\u2019s the variance of the workload and what are the elasticity requirements? Through their fault tolerant nature, Apache Spark clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? Apache Spark Structured Streaming supports exactly once delivery guarantees and depending on the type of data source, complete crash fault tolerance.","title":"Apache Spark and Apache Spark Structured Streaming"},{"location":"methodology/lightweight-architectural-decisions/#ibm-streams","text":"IBM Streams is a fast streaming engine. Originally designed for low-latency, high throughput network monitoring applications, IBM Streams has its roots in cybersecurity. What throughput is required? IBM Streams can handle high data throughput rates, up to millions of events or messages per second. What latency is accepted? IBM Streams latency goes down to microseconds. Which data types must be supported? Through IBM Streams binary transfer mode, any type of data type can be supported. What type of algorithms run on the system? Only relational algebra or advanced modeling? IBM Streams in its core supports all relational algebra. Also, through toolkits, various machine learning algorithms can be used. Toolkits are an open system, and there are many third-party toolkits. What\u2019s the variance of the workload and what are the elasticity requirements? Through its fault tolerant nature, IBM Streams clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? IBM Streams supports exactly once delivery guarantees and complete crash fault tolerance.","title":"IBM Streams"},{"location":"methodology/lightweight-architectural-decisions/#nodered","text":"NodeRED is a lightweight streaming engine. Implemented on top of Node.js in JavaScript, it can even run on a 64 MB memory footprint (for example, running on a Raspberry PI). What throughput is required? NodeRED\u2019s throughput is bound to processing capabilities of a single CPU core, through Node.js\u2019s event processing nature. For increased throughput, multiple instances of NodeRED have been used in parallel. Parallelization is not built in and needs to be provided by the application developer. What latency is accepted? Latency is also dependent on the CPU configuration and on the throughput because high throughput congests the event queue and increases latency. Which data types must be supported? NodeRED best supports JSON streams, although any data type can be nested into JSON. What type of algorithms run on the system? Only relational algebra or advanced modeling? NodeRED has one of the most extensive ecosystems of open source third-party modules. Although advanced machine learning is not supported natively, there are plans by IBM to add those. What\u2019s the variance of the workload and what are the elasticity requirements? Because parallelization is a responsibility of the application developer, for independent computation a round-robin load balancing scheme supports linear scalability and full elasticity. What type of fault tolerance and delivery guarantees are necessary? NodeRED has no built-in fault tolerance and no delivery guarantees.","title":"NodeRED"},{"location":"methodology/lightweight-architectural-decisions/#apache-nifi","text":"Apache Nifi is maintained by Hortonworks and is part of the IBM Analytics Engine Service. What throughput is required? Nifi can handle hundreds of MBs on a single node and can be configured to handle multiple GBs in cluster mode. What latency is accepted? Nifi\u2019s latency is in seconds. Through message periodization, the tradeoff between throughput and latency can be tweaked. Which data types must be supported? Nifi best supports structured data streams, although any data type can be nested. What type of algorithms run on the system? Only relational algebra or advanced modeling? Nifi supports relational algebra out of the box, but custom processors can be built. What\u2019s the variance of the workload and what are the elasticity requirements? Nifi can be easily scaled up without restarts, but scaling down requires stopping and starting the Nifi system. What type of fault tolerance and delivery guarantees are necessary? Nifi supports end-to-end guaranteed exactly once delivery. Also, fault tolerance can be configured, but automatic recovery is not possible. Another important feature is backpressure and pressure release, which causes the upstream nodes to stop accepting new data and discarding unprocessed data if an age threshold is exceeded.","title":"Apache Nifi"},{"location":"methodology/lightweight-architectural-decisions/#others","text":"There are also other technologies like Apache Kafka, Samza, Apache Flink, Apache Storm, Total.js Flow, Eclipse Kura, and Flogo that might be worth looking at if the ones mentioned don\u2019t meet all of your requirements.","title":"Others"},{"location":"methodology/lightweight-architectural-decisions/#data-integration","text":"In the data integration stage, data is cleansed, transformed, and if possible, downstream features are added","title":"Data Integration"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_3","text":"There are numerous technologies for batch data processing, which is the technology used for data integration. The most important questions to be asked are: What throughput is required? Which data types must be supported? What source systems must be supported? What skills are required?","title":"Architectural Decision Guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_2","text":"IBM Cloud has many service offerings for data integration, and the following section explains them and gives guidelines on which one to use.","title":"Technology Guidelines"},{"location":"methodology/lightweight-architectural-decisions/#apache-spark","text":"Apache Spark is often the first choice when it comes to cluster-grade data processing and machine learning. Apache Spark is a flexible option that also supports writing integration processes in SQL. But it\u2019s missing a user interface. What throughput is required? Apache Spark scales linearly, so throughput is just a function of the cluster size. Which data types must be supported? Apache Spark works best with structured data, but binary data is supported as well. What source systems must be supported? Apache Spark can access various SQL and NoSQL data as well as file sources. A common data source architecture allows adding capabilities, and it has third-party project functions as well. What skills are required? Advanced SQL skills are required and you should have some familiarity with either Java programming, Scala, or Python.","title":"Apache Spark"},{"location":"methodology/lightweight-architectural-decisions/#ibm-data-stage-on-cloud","text":"IBM Data Stage is a sophisticated ETL (Extract Transform Load) tool. Its closed source and supports visual editing. What throughput is required? Data Stage can be used in cluster mode, which supports scale-out. Which data types must be supported? Data Stage has its roots in traditional Data Warehouse ETL and concentrates on structured data. What source systems must be supported? Again, Data Stage concentrates on relational database systems, but files can also be read, even on Object Store. In addition, data sources can be added using plug-ins that are implemented in the Java language. What skills are required? Because Data Stage is a visual editing environment, the learning curve is low. No programming skills are required.","title":"IBM Data Stage on Cloud"},{"location":"methodology/lightweight-architectural-decisions/#others_1","text":"It\u2019s important to know that data integration is mostly done using ETL tools, plain SQL, or a combination of both. ETL tools are mature technology, and many ETL tools exist. On the other hand, if streaming analytics is part of the project, it\u2019s a good idea to check whether one of those technologies fits your requirements because reuse of such a system reduces technology heterogeneity.","title":"Others"},{"location":"methodology/lightweight-architectural-decisions/#data-repository","text":"This is the persistent storage for your data.","title":"Data Repository"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_4","text":"There are lots of technologies for persisting data, and most of them are relational databases. The second largest group are NoSQL databases, with file systems (including Cloud Object Store) forming the last one. The most important questions to be asked are: What is the impact of storage cost? Which data types must be supported? How well must point queries (on fixed or dynamic dimensions) be supported? How well must range queries (on fixed or dynamic dimensions) be supported? How well must full table scans be supported? What skills are required? What\u2019s the requirement for fault tolerance and backup? What are the constant and peak ingestion rates? What amount of storage is needed? What does the growth pattern look like? What are the retention policies?","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_3","text":"IBM cloud has numerous service offerings for SQL, NoSQL, and file storage. The following section explains them and provides guidelines on when to use which one.","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#relational-databases","text":"Dash DB is the Db2 BLU on the Cloud offering from IBM that features column store, advanced compression, and execution on SIMD instructions sets (that is, vectorized processing). But there are other options in IBM Cloud such as Informix, PostgreSQL, and MySQL. What is the impact of storage cost? Relational databases (RDBMS) have the highest requirements on storage quality. Therefore, the cost of relational storage is always the highest. Which data types must be supported? Relational databases are meant for structured data. Although there are column data types for binary data that can be swapped for cheaper storage, this is just an add-on and not a core function of relational databases. How well must point queries (on fixed or dynamic dimensions) be supported? RDBMS are great for point queries because an index can be created on each column. How well must range queries (on fixed or dynamic dimensions) be supported? RDBMS are great for range queries because an index can be created on each column. How well must full table scans be supported? RDBMS are trying to avoid full table scans in their SQL query optimizers. Therefore, performance is not optimized for full table scans (for example, contaminating page caches). What skills are required? You should have SQL skills, and if a cloud offering isn\u2019t chosen, you should also have database administrator (DBA) skills for the specific database. What\u2019s the requirement for fault tolerance and backup? RDMBS support continuous backup and crash fault tolerance. For recovery, the system might need to go offline. What are the constant and peak ingestion rates? Inserts using SQL are relatively slow, especially if the target table contains many indexes that must be rebalanced and updated. Some RDBMS support bulk inserts from files by bypassing the SQL engine, but then the table usually needs to go offline for that period. What amount of storage is needed? RDMBS perform very well to around 1 TB of data. Going beyond that is complex and needs advanced cluster setups. What does the growth pattern look like? RDBMS support volume management, so continuous growth usually isn\u2019t a problem, even at run time. For shrinking, the system might need to be taken offline. What are the retention policies? RDBMS usually support automated retention mechanisms to delete old data automatically.","title":"Relational databases"},{"location":"methodology/lightweight-architectural-decisions/#nosql-databases","text":"The most prominent NoSQL databases like Apache CouchDB, MongoDB, Redis, RethinkDB, ScyllaDB (Cassandra), and InfluxCloud are supported. What is the impact of storage cost? NoSQL databases are usually storage fault-tolerant, by default. Therefore, quality requirements on storage are less, which brings down storage cost. Which data types must be supported? Although NoSQL databases are meant for structured data as well, they usually use JSON as a storage format, which can be enriched with binary data. Although, a lot of binary data attached to a JSON document can bring the performance down as well. How well must point queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves point query performance. How well must range queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves range query performance. How well must full table scans be supported? NoSQL databases perform very well at full table scans. The performance is only limited by the I/O bandwidth to storage. What skills are required? Typically, special query language skills are required for the application developer and if a cloud offering isn\u2019t chosen, database administrator (DBA) skills are needed for the specific database. What\u2019s the requirement for fault tolerance and backup? NoSQL databases support backups in different ways. But some aren\u2019t supporting online backup. NoSQL databases are usually crash fault tolerant, but for recovery, the system might need to go offline. What are the constant and peak ingestion rates? Usually, no indexes need to be updated and data doesn\u2019t need to be mapped to pages. Ingestion rates are usually only bound to I/O performance of the storage system. What amount of storage is needed? RDMBS perform well to approximately 10 \u2013 100 TB of data. Cluster setups on NoSQL databases are much more straightforward than on RDBMS. Successful setups with >100 nodes and > 100.000 database reads and writes per second have been reported. What does the growth pattern look like? The growth of NoSQL databases is not a problem. Volumes can be added at run time. For shrinking, the system might need to be taken offline. What are the retention policies? NoSQL databases don\u2019t support automated retention mechanisms to delete old data automatically. Therefore, this must be implemented manually, resulting in range queries on the data corpus.","title":"NoSQL databases"},{"location":"methodology/lightweight-architectural-decisions/#object-storage","text":"Cloud object storage makes it possible to store practically limitless amounts of data. It is commonly used for data archiving and backup, for web and mobile applications, and as scalable, persistent storage for analytics. So, let\u2019s take a look. What is the impact of storage cost? Object storage is the cheapest option for storage. Which data types must be supported? Because object storage resembles a file system, any data type is supported. How well must point queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. How well must range queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. Therefore, range queries on a single defined column (for example, data) can be achieved through hierarchical folder structures. How well must full table scans be supported? Full table scans are bound only by the I/O bandwidth of the object storage. What skills are required? On a file level, working with object storage is much like working with any file system. Through Apache SparkSQL and IBM Cloud SQL Query, data in Object storage can be accessed with SQL. Because object storage is a cloud offering, no administrator skills are required. IBM Object Storage is available for on-premises as well using an appliance box. What\u2019s the requirement for fault tolerance and backup? Fault tolerance and backup is completely handled by the cloud provider. Object storage supports intercontinental data center replication for high-availability out of the box. What are the constant and peak ingestion rates? Ingestion rates to object storage is bound by the uplink speed to the object storage system. What\u2019s the amount of storage needed? Object storage scales to the petabyte range. What does the growth pattern look like? Growth and shrinking on object storage is fully elastic. What are the retention policies? Retention of data residing in object storage must be done manually. Hierarchical file and folder layout that is based on data and time helps here. Some object storage support automatic movement of infrequently accessed files to colder storage (colder means less cost, but also less performance, or even higher cost of accesses to files).","title":"Object storage"},{"location":"methodology/lightweight-architectural-decisions/#discovery-and-exploration","text":"This component allows for visualization and creation of metrics of data.","title":"Discovery and exploration"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_5","text":"In various process models, data visualization and exploration is one of the first steps. Similar tasks are also applied in traditional data warehousing and business intelligence. So when choosing a technology, ask the following questions: What type of visualizations are needed? Are interactive visualizations needed? Are coding skills available or required? What metrics can be calculated on the data? Do metrics and visualization need to be shared with business stakeholders?","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_4","text":"IBM cloud has many service offerings for data exploration. Some of the offerings are open source, and some aren\u2019t.","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#jupyter-python-pyspark-scikit-learn-pandas-matplotlib-pixiedust","text":"Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust are all open source and supported in IBM Cloud. Some of these components have overlapping features and some of them have complementary features. This can be determined by answering the architectural questions. What type of visualizations are needed? Matplotlib supports the widest range of possible visualizations including run chars, histograms, box-plots, and scatter plots. PixieDust (as of V1.1.11) supports tables, bar charts, line charts, scatter plots, pie charts, histograms, and maps. Are interactive visualizations needed? Matplotlib creates static plots and PixieDust supports interactive ones. Are coding skills available or required? Matplotlib requires coding skills, but PixieDust does not. For computing metrics, some code is necessary. What metrics can be calculated on the data? Using scikit-learn and pandas, all state-of-the-art metrics are supported. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of Jupyter Notebooks, also using a fine-grained user and access management system.","title":"Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust"},{"location":"methodology/lightweight-architectural-decisions/#spss-modeler","text":"SPSS Modeler is available in the cloud and also as stand-alone product. What type of visualizations are needed? SPSS Modeler supports the following visualizations out of the box: Bar Pie 3D Bar 3D Pie Line Area 3D Area Path Ribbon Surface Scatter Bubble Histogram Box Map You can get more information in the IBM Knowledge Center . Are interactive visualizations needed? SPSS Modeler Visualizations are not interactive. Are coding skills available or required? SPSS Modeler doesn\u2019t require any coding skills. What metrics can be calculated on the data? All state-of-the-art metrics are supported using the Data Audit node. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of SPSS Modeler Flows, also using a fine-grained user and access management system. However, those might not be suitable to stakeholders.","title":"SPSS Modeler"},{"location":"methodology/lightweight-architectural-decisions/#actionable-insights","text":"This is where most of your work fits in. It\u2019s where you create and evaluate your machine learning and deep learning models.","title":"Actionable insights"},{"location":"methodology/lightweight-architectural-decisions/#architectural-decision-guidelines_6","text":"There are numerous technologies for creating and evaluating machine learning and deep learning models. Although different technologies differ in function and performance, those differences are usually miniscule. Therefore, the questions you should ask yourself are: What are the available skills regarding programming languages? What are the costs of skills regarding programming languages? What are the available skills regarding frameworks? What are the costs of skills regarding frameworks? Is model interchange required? Is parallel- or GPU-based training or scoring required? Do algorithms need to be tweaked or new algorithms be developed?","title":"Architectural decision guidelines"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_5","text":"Because there\u2019s an abundance of open and closed source technologies, I\u2019m highlighting the most relevant ones in this article. Although it\u2019s the same for the other sections as well, decisions made in this section are very prone to change due to the iterative nature of this process model. Therefore, changing or combining multiple technologies is no problem, although the decisions that led to those changes should be explained and documented.","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#spss-modeler_1","text":"This article has already introduced SPSS Modeler. What are the available skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the costs of skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the available skills regarding frameworks? SPSS is an industry leader, so skills are generally available. What are the costs of skills regarding frameworks? Expert costs are usually lower in UI-based tools than in programming frameworks. Is model interchange required? SPSS Modeler supports PMML. Is parallel- or GPU-based training or scoring required? SPSS Modeler supports scaling through IBM Analytics Server or IBM Watson Studio using Apache Spark. Do algorithms need to be tweaked or new algorithms be developed? SPSS Modeler algorithms can\u2019t be changed, but you can add algorithms or customizations using the R language.","title":"SPSS Modeler"},{"location":"methodology/lightweight-architectural-decisions/#rr-studio","text":"R and R-Studio are standards for open source-based data science. They\u2019re supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? R programming skills are usually widely available because it\u2019s a standard programming language in many natural science-based university curriculums. It can be acquired rapidly because it is a procedural language with limited functional programming support. What are the costs of skills regarding programming languages? Costs of R programming are usually low. What are the available skills regarding frameworks? R is not only a programming language but also requires knowledge of tooling (R-Studio), and especially knowledge of the R library (CRAN) with 6000+ packages. What are the costs of skills regarding frameworks? Expert costs are correlated with knowledge of the CRAN library and years of experience and in the range of usual programmer costs. Is model interchange required? Some R libraries support exchange of models, but it is not standardized. Is parallel- or GPU-based training or scoring required? Some R libraries support scaling and GPU acceleration, but it is not standardized. Do algorithms need to be tweaked or new algorithms be developed? R needs algorithms to be implemented in C/C++ to run fast. So, tweaking and custom development usually involves C/C++ coding.","title":"R/R-Studio"},{"location":"methodology/lightweight-architectural-decisions/#python-pandas-and-scikit-learn","text":"Although R and R-Studio have been the standard for open source-based data science for a while, Python, pandas, and scikit-learn are right behind them. Python is a much cleaner programming language than R and easier to learn. Pandas is the Python equivalent to R data frames, supporting relational access to data. Finally, scikit-learn nicely groups all necessary machine learning algorithms together. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Python skills are very widely available because Python is a clean and easy to learn programming language. What are the costs of skills regarding programming languages? Because of Python\u2019s properties mentioned above, the cost of Python programming skills is very low. What are the available skills regarding frameworks? Pandas and scikit-learn are very clean and easy-to-learn frameworks. Therefore, skills are widely available. What are the costs of skills regarding frameworks? Because of the properties mentioned above, the costs of skills are very low. Is model interchange required? All scikit-learn models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? Neither GPU nor scale-out is supported, although scale-up capabilities can be added individually to make use of multiple cores. Do algorithms need to be tweaked or new algorithms be developed? scikit-learn algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with the numpy library. Therefore, tweaking and adding algorithms is straightforward.","title":"Python, pandas and scikit-learn"},{"location":"methodology/lightweight-architectural-decisions/#python-apache-spark-and-sparkml","text":"Although Python, pandas, and scikit-learn are more widely adopted, the Apache Spark ecosystem is catching up, especially because of its scaling capabilities. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Apache Spark supports Python, Java programming, Scala, and R as programming languages. What are the costs of skills regarding programming languages? The costs depend on what programming language is used, with Python typically the cheapest. What are the available skills regarding frameworks? Apache Spark skills are in high demand and usually not available. What are the costs of skills regarding frameworks? Apache Spark skills are in high demand and are usually expensive. Is model interchange required? All SparkML models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. However, GPU\u2019s are only supported through third-party libraries. Do algorithms need to be tweaked or new algorithms be developed? As in scikit-learn, algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with built-in Apache Spark libraries. Therefore, tweaking and adding algorithms is straightforward.","title":"Python, Apache Spark and SparkML"},{"location":"methodology/lightweight-architectural-decisions/#apache-systemml","text":"When it comes to relational data processing, SQL is a leader, mainly because an optimizer takes care of optimal query executions. Think of SystemML as an optimizer for linear algebra that\u2019s capable of creating optimal execution plans for jobs running on data parallel frameworks like Apache Spark. What are the available skills regarding programming languages? SystemML has two domain-specific languages (DSL) with R and Python syntax. What are the costs of skills regarding programming languages? Although the DSLs are like R and Python, there is a learning curve involved. What are the available skills regarding frameworks? SystemML skills are very rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? SystemML models can be (de)serialized. PMML is not supported. SystemML can import and run Caffe2 and Keras models. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. SystemML uses this property. In addition, GPU\u2019s are supported as well. Do algorithms need to be tweaked or new algorithms be developed? Although SystemML comes with a large set of pre-implemented algorithms for machine learning and deep learning, its strengths are in tweaking existing algorithms or implementing new ones because the DSL allows for concentrating on the mathematical implementation of the algorithm. The rest is handled by the framework. This makes it an ideal choice for these kind of tasks.","title":"Apache SystemML"},{"location":"methodology/lightweight-architectural-decisions/#keras-and-tensorflow","text":"TensorFlow is one of the most widely used deep learning frameworks. At its core, it is a linear algebra library supporting automatic differentiation. TensorFlow\u2019s Python-driven syntax is relatively complex. Therefore, Keras provides an abstraction layer on top of TensorFlow. Both frameworks are seamlessly supported in IBM Cloud through Watson Studio and Watson Machine Learning. What are the available skills regarding programming languages? Python is the core programming language for Keras and TensorFlow. What are the costs of skills regarding programming languages? Because Python programmers are more abundant than Java/Scala programmers, the costs are less. What are the available skills regarding frameworks? Keras and TensorFlow skills are relatively rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? Keras and TensorFlow have their own model exchange formats. There are converters from and to ONNX. Is parallel- or GPU-based training or scoring required? Running TensorFlow on top of ApacheSpark is supported through TensorFrames and TensorSpark. Keras models can be run on ApacheSpark using DeepLearning4J and SystemML. Both of the latter frameworks also support GPUs. TensorFlow (and therefore, Keras) support GPU natively as well. Do algorithms need to be tweaked or new algorithms be developed? TensorFlow is a linear algebra execution engine. Therefore, it\u2019s optimally suited for tweaking and creating new algorithms. Keras is a very flexible deep learning library that supports many neural network layouts.","title":"Keras and TensorFlow"},{"location":"methodology/lightweight-architectural-decisions/#applications-and-data-products","text":"Models are fine, but their value rises when they can be consumed by the ordinary business user. Therefore, you must create a data product. Data products don\u2019t necessarily need to stay on the cloud. They can be pushed to mobile or enterprise applications. Architectural decision guidelines In contrast to machine learning and deep learning frameworks, the space of frameworks to create data product is tiny. This might reflect what the current state-of-the-art technology in data science concentrates on. Depending on the requirements, data products are relatively visualization-centric after a lot of use input data has been gathered. They also might involve asynchronous workflows as batch data integration and model training and scoring is performed within the workflow. Questions to ask about the technology are: What skills are present for developing a data product? What skills are necessary for developing a data product? Is instant feedback required or is batch processing accepted? What\u2019s the degree of customization needed? * What\u2019s the target audience? Is cloud scale deployment for a public use base required?","title":"Applications and Data Products"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_6","text":"Currently, only a limited set of frameworks and technologies is available in different categories. In the following section, I\u2019ve explained the most prominent examples.","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#r-shiny","text":"R-Shiny is a great framework for building data products. Closely tied to the R language, it enables data scientists to rapidly create a UI on top of existing R-scripts. What skills are present for developing a data product? R-Shiny requires R development skills. So, it best fits into an R development ecosystem. What skills are necessary for developing a data product? Although based on R, R-Shiny needs additional skills. For experienced R developers, the learning curve is steep and additional knowledge to acquire is minimal. Is instant feedback required or is batch processing accepted? The messaging model of R-Shiny supports instant UI feedback when server-side data structures are updated. Therefore, the response time is independent of the framework and should be considered and resolved programmatically on the server side. What\u2019s the degree of customization needed? Although R-Shiny is an extensible framework, extending it requires a deep understanding of the framework and R-Technology. Out of the box, there is a large set of UI widgets end elements supported, allowing for very customizable applications. If requirements go beyond those capabilities, costly extensions are required. What\u2019s the target audience? Is cloud scale deployment for a public use base required? R-Shiny applications look very professional, although quite distinguishable. Therefore, the target audience must accept the UI design limitations. R-Shiny is best dynamically scaled horizontally in a container environment like Kubernetes. Ideally, every user session runs in its own container because R and R-Shiny are very sensitive to main memory shortages","title":"R-Shiny"},{"location":"methodology/lightweight-architectural-decisions/#node-red","text":"Although Node-RED is a no-code/low-code data flow/data integration environment, because of its modular nature it supports various extensions including the dash boarding extension. This extension allows for fast creation of user interfaces including advanced visualizations that are updated in real-time. What skills are present for developing a data product? Due to the completely graphical user interface-based software development approach, only basic programming skills are required to build data products with Node-RED. What skills are necessary for developing a data product? Any resource familiar with flow-based programming as used in many state-of-the-art ETL and data mining tools will have a fast start with Node-RED. Basic JavaScript knowledge is required for creating advanced flows and for extending the framework. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? Node-RED is a Node.js/JavaScript-based framework. Custom UI widgets require advanced Node.js development skills. What\u2019s the target audience? Is cloud scale deployment for a public use base required? The Node-RED dashboard can be deployed for a pubic user base as long as the limitations regarding UI customization are acceptable. Because Node.js runs on a single threaded event loop, scaling must be done horizontally, preferably using a containerized environment. Note: The Internet of Things Starter kit in IBM Cloud supports horizontal scaling out of the box.","title":"Node-RED"},{"location":"methodology/lightweight-architectural-decisions/#d3","text":"When it comes to custom application development, D3 is one of the most prominent and most widely used visualization widget frameworks with a large open source ecosystem contributing a lot of widgets for every desirable use case. There\u2019s a good chance that you can find a D3 widget to fit your use case. What skills are present for developing a data product? D3 fits best into an existing, preferably JavaScript-based developer ecosystem, although JavaScript is only required on the client side. Therefore, on the server side, any REST-based endpoints in any programming language are supported. One example is REST endpoints accessed by a D3 UI provided by Apache Livy that encapsulates Apache Spark jobs. What skills are necessary for developing a data product? D3 requires sophisticated D3 skills and at least client-side JavaScript skills. Skills in a JavaScript AJAX framework like AngularJS are highly recommended. On the server side, capabilities of providing REST endpoints to the D3 applications are required. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? D3 applications usually are implemented from scratch. Therefore, this solution provides the most flexibility to the end user. What\u2019s the target audience? Is cloud scale deployment for a public use base required? As a cloud native application, a D3-based data product can provide all capabilities for horizontal and vertical scaling and full adoption to user requirements.","title":"D3"},{"location":"methodology/lightweight-architectural-decisions/#security-information-governance-and-systems-management","text":"This important step can be easily forgotten. It\u2019s important to control who has access to which information for many compliance regulations. In addition, modern data science architectures involve many components that require operational aspects as well. Architectural decision guidelines Data privacy is a major challenge in many data science projects. Questions that you should ask are: What granularity is required for managing user access to data assets? Are existing user registries required to be integrated? Who is taking care of operational aspects? What are the requirements for data retention? What level of security against attacks from hackers is required?","title":"Security, information governance and systems management"},{"location":"methodology/lightweight-architectural-decisions/#technology-guidelines_7","text":"Again, there\u2019s a lot of software for this as well as ways to solve the requirements involved in this topic. I\u2019ve chosen representative examples for this article. Internet services Deploying a productive client-facing web application brings with it serious risks. IBM Cloud Internet Services provides global points of presence (PoPs). It includes domain name service (DNS), global load balancer (GLB), distributed denial of service (DDoS) protection, web application firewall (WAF), transport layer security (TLS), and caching. What level of security against attacks from hackers is required? Internet Services is using services from CloudFlare, the world leader in this space.","title":"Technology guidelines"},{"location":"methodology/lightweight-architectural-decisions/#ibm-app-id","text":"Identity Management allows for cloud-based user and identity management for web and mobile applications, APIs, and back-end systems. Cloud users can sign up and sign in with App ID\u2019s scalable user registry or social login with Google or Facebook. Enterprise users can be integrated using SAML 2.0 federation. What granularity is required for managing user access to data assets? IBM App ID supports user management but no group/roles. Therefore, fine-grained access must be managed within the application. Are existing user registries required to be integrated? IBM App ID supports registry integration using SAML 2.0 federation.","title":"IBM App ID"},{"location":"methodology/lightweight-architectural-decisions/#object-storage_1","text":"Object Storage is a standard when it comes to modern, cost-effective cloud storage. What granularity is required for managing user access to data assets? IBM Identity and Access Management (IAM) integration allows for granular access control at the bucket-level using role-based policies. What are the requirements for data retention? Object storage supports different storage classes for frequently accessed data, occasionally accessed data, and long-term data retention with standard, vault, and cold vault. The Flex class allows for dynamic data access and automates this process. Physical deletion of data still must be triggered externally. Who is taking care of operational aspects? Regional and cross-region resiliency options allow for increased data reliability. What level of security against attacks from hackers is required? All data is encrypted at rest and in flight by default. Keys are automatically managed by default, but optionally can be self-managed or managed using IBM Key Protect.","title":"Object Storage"},{"location":"methodology/lightweight-architectural-decisions/#ibm-cloud-paassaas","text":"IBM Cloud PaaS/SaaS eliminates operational aspects from data science projects because all components involved are managed by IBM. Who is taking care of operational aspects? In PaaS/SaaS, cloud operational aspects are being taken care of by the cloud provider.","title":"IBM Cloud PaaS/SaaS"},{"location":"methodology/lightweight-architectural-decisions/#summary","text":"This article complements the \u201cLightweight IBM Cloud Garage Method for data science\u201d article and helps you with deployment considerations.","title":"Summary"},{"location":"methodology/lightweight/","text":"The Lightweight IBM Cloud Garage Method for Data Science A process model to map individual technology components to the reference architecture. The lightweight IBM Cloud Garage Method for data science includes a process model to map individual technology components to the reference architecture. This method does not include any requirement engineering or design thinking tasks. Because it can be hard to initially define the architecture of a project, this method supports architectural changes during the process model. A separate companion article discusses the architectural decision guidelines . The Lightweight IBM Cloud Garage Method for Data Science Process Model This section introduces this lightweight process model. The first thing that you should notice is the similarity to the process models I introduced in my last article . With this model, there are no design tasks because this method is used for projects where the business expectations are already set. Finally, you should notice the increased granularity in the individual tasks. The reason for this is reuse. Every task has a clear purpose and a defined work product (for example, a Jupyter Notebook, a script, or a docker container hosting a scoring or training endpoint, depending on the architectural decisions made). The following sections explain the individual tasks. Initial Data Exploration This task is crucial for understanding your data. Data quality is the most important driver for success in any data science project. So, this task lets you address data quality from the beginning. This includes going back to the data owners and asking them for better quality data, if applicable. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas, Matplotlib IBM Watson Studio Jupyter Notebooks, Apache Spark, PixieDust IBM Watson Studio \u2013 IBM SPSS Modeler \u2013 Data Audit IBM SPSS Modeler stand-alone \u2013 Data Audit IBM Information Server \u2013 QualityStage Extract, transform, load (ETL) This task is an important step in transforming the data from the source system into data suitable for analytics. In traditional data warehousing, this process includes accessing the online transaction processing (OLTP) system\u2019s databases, transforming the data from a highly normalized data model into a Star or Snowflake Schema, and storing the data to a data warehouse. In data science projects, this step is usually much simpler. The data arrives in an exported format (for example, JSON or CSV). But, sometimes de-normalization must be done as well. The result usually ends up in a bulk storage like Cloud Object Store. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache Spark SQL IBM Watson Studio \u2013 Data Refinery IBM Information Server \u2013 Data Stage Feature creation This task transforms input columns of various relations into additional columns to improve model performance. A subset of those features can be created in an initial task (for example, one-hot encoding of categorical variables or normalization of numerical variables). Some others require business understanding or multiple iterations to be considered. This task is one of those benefiting the most from the highly iterative nature of this method. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache Spark SQL IBM Information Server \u2013 Data Stage Model definition This task defines the machine learning or deep learning model. Because this is a highly iterative method, various iterations within this task or including up- and downstream tasks are possible. I recommend starting with simple models first for baseline creation after those models are evaluated. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone Model Training This task trains the model. The task is set apart from model definition and evaluation for various reasons. First, training is a computationally intense task that might be scaled on computer clusters or GPUs. Therefore, an architectural cut is sometimes unavoidable. (For example, model definition happens in Keras, but training happens on a Keras model export using Apache SystemML on top of Apache Spark running on a GPU cluster.) In hyperparameter tuning and hyperparameter space exploration, the downstream task \u201cModel Evaluation\u201d can be part of this asset. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone Model evaluation This task evaluates the model\u2019s performance. Given the nature of the task, different metrics must be applied, for example, categorical-cross entropy for a multi-class classification problem. It\u2019s important to divide the data set into training, test, and validation (if cross-validation isn\u2019t used) and keep track of the performance of different feature engineering, model definition, and training parameters. Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone Model deployment This task deploys the model. The task depends heavily on the use case, especially, on the stakeholder\u2019s expectation on consuming the data product. So, valid ways of deployment include: An interactive Jupyter Notebook An export of an already run, static Jupyter Notebook or some type of report A REST endpoint allowing scoring (and training) of the model (for example, backed by a docker container running on Kubernetes) A full-fledged web or mobile application Tools guidance Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone IBM MAX (Model Asset Exchange) IBM FfDL (Fabric for DeepLearning) IBM Watson Machine Learning IBM Watson DeepLearning as a Service Project Asset Naming Convention Need a structure to name your assets? Here\u2019s our recommended convention. Note that we recommend to always use project_name, while the others are optional. [project_name].data_exp.<technology>.<version>.<extension> [project_name].etl.<technology>.<version>.<extension> [project_name].feature_eng.<technology>.<version>.<extension> [project_name].model_def.<technology>.<version>.<extension> [project_name].model_train.<technology>.<version>.<extension> [project_name].model_evaluate.<technology>.<version>.<extension> [project_name].model_deployment.<technology>.<version>.<extension> Summary This article information on the lightweight IBM Cloud Garage Method for data science. It included a process model to map individual technology components to the reference architecture. A separate companion article discusses the architectural decision guidelines .","title":"The Lightweight IBM Cloud Garage Method for Data Science"},{"location":"methodology/lightweight/#the-lightweight-ibm-cloud-garage-method-for-data-science","text":"A process model to map individual technology components to the reference architecture. The lightweight IBM Cloud Garage Method for data science includes a process model to map individual technology components to the reference architecture. This method does not include any requirement engineering or design thinking tasks. Because it can be hard to initially define the architecture of a project, this method supports architectural changes during the process model. A separate companion article discusses the architectural decision guidelines .","title":"The Lightweight IBM Cloud Garage Method for Data Science"},{"location":"methodology/lightweight/#the-lightweight-ibm-cloud-garage-method-for-data-science-process-model","text":"This section introduces this lightweight process model. The first thing that you should notice is the similarity to the process models I introduced in my last article . With this model, there are no design tasks because this method is used for projects where the business expectations are already set. Finally, you should notice the increased granularity in the individual tasks. The reason for this is reuse. Every task has a clear purpose and a defined work product (for example, a Jupyter Notebook, a script, or a docker container hosting a scoring or training endpoint, depending on the architectural decisions made). The following sections explain the individual tasks.","title":"The Lightweight IBM Cloud Garage Method for Data Science Process Model"},{"location":"methodology/lightweight/#initial-data-exploration","text":"This task is crucial for understanding your data. Data quality is the most important driver for success in any data science project. So, this task lets you address data quality from the beginning. This includes going back to the data owners and asking them for better quality data, if applicable.","title":"Initial Data Exploration"},{"location":"methodology/lightweight/#tools-guidance","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas, Matplotlib IBM Watson Studio Jupyter Notebooks, Apache Spark, PixieDust IBM Watson Studio \u2013 IBM SPSS Modeler \u2013 Data Audit IBM SPSS Modeler stand-alone \u2013 Data Audit IBM Information Server \u2013 QualityStage","title":"Tools guidance"},{"location":"methodology/lightweight/#extract-transform-load-etl","text":"This task is an important step in transforming the data from the source system into data suitable for analytics. In traditional data warehousing, this process includes accessing the online transaction processing (OLTP) system\u2019s databases, transforming the data from a highly normalized data model into a Star or Snowflake Schema, and storing the data to a data warehouse. In data science projects, this step is usually much simpler. The data arrives in an exported format (for example, JSON or CSV). But, sometimes de-normalization must be done as well. The result usually ends up in a bulk storage like Cloud Object Store.","title":"Extract, transform, load (ETL)"},{"location":"methodology/lightweight/#tools-guidance_1","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache Spark SQL IBM Watson Studio \u2013 Data Refinery IBM Information Server \u2013 Data Stage","title":"Tools guidance"},{"location":"methodology/lightweight/#feature-creation","text":"This task transforms input columns of various relations into additional columns to improve model performance. A subset of those features can be created in an initial task (for example, one-hot encoding of categorical variables or normalization of numerical variables). Some others require business understanding or multiple iterations to be considered. This task is one of those benefiting the most from the highly iterative nature of this method.","title":"Feature creation"},{"location":"methodology/lightweight/#tools-guidance_2","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache Spark SQL IBM Information Server \u2013 Data Stage","title":"Tools guidance"},{"location":"methodology/lightweight/#model-definition","text":"This task defines the machine learning or deep learning model. Because this is a highly iterative method, various iterations within this task or including up- and downstream tasks are possible. I recommend starting with simple models first for baseline creation after those models are evaluated.","title":"Model definition"},{"location":"methodology/lightweight/#tools-guidance_3","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone","title":"Tools guidance"},{"location":"methodology/lightweight/#model-training","text":"This task trains the model. The task is set apart from model definition and evaluation for various reasons. First, training is a computationally intense task that might be scaled on computer clusters or GPUs. Therefore, an architectural cut is sometimes unavoidable. (For example, model definition happens in Keras, but training happens on a Keras model export using Apache SystemML on top of Apache Spark running on a GPU cluster.) In hyperparameter tuning and hyperparameter space exploration, the downstream task \u201cModel Evaluation\u201d can be part of this asset.","title":"Model Training"},{"location":"methodology/lightweight/#tools-guidance_4","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone","title":"Tools guidance"},{"location":"methodology/lightweight/#model-evaluation","text":"This task evaluates the model\u2019s performance. Given the nature of the task, different metrics must be applied, for example, categorical-cross entropy for a multi-class classification problem. It\u2019s important to divide the data set into training, test, and validation (if cross-validation isn\u2019t used) and keep track of the performance of different feature engineering, model definition, and training parameters.","title":"Model evaluation"},{"location":"methodology/lightweight/#tools-guidance_5","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone","title":"Tools guidance"},{"location":"methodology/lightweight/#model-deployment","text":"This task deploys the model. The task depends heavily on the use case, especially, on the stakeholder\u2019s expectation on consuming the data product. So, valid ways of deployment include: An interactive Jupyter Notebook An export of an already run, static Jupyter Notebook or some type of report A REST endpoint allowing scoring (and training) of the model (for example, backed by a docker container running on Kubernetes) A full-fledged web or mobile application","title":"Model deployment"},{"location":"methodology/lightweight/#tools-guidance_6","text":"Tools that you can use to perform this task include: IBM Watson Studio Jupyter Notebooks, scikit-learn, pandas IBM Watson Studio Jupyter Notebooks, Apache Spark, Apache SparkML, Apache SystemML IBM Watson Studio \u2013 IBM SPSS Modeler IBM SPSS Modeler stand-alone IBM MAX (Model Asset Exchange) IBM FfDL (Fabric for DeepLearning) IBM Watson Machine Learning IBM Watson DeepLearning as a Service","title":"Tools guidance"},{"location":"methodology/lightweight/#project-asset-naming-convention","text":"Need a structure to name your assets? Here\u2019s our recommended convention. Note that we recommend to always use project_name, while the others are optional. [project_name].data_exp.<technology>.<version>.<extension> [project_name].etl.<technology>.<version>.<extension> [project_name].feature_eng.<technology>.<version>.<extension> [project_name].model_def.<technology>.<version>.<extension> [project_name].model_train.<technology>.<version>.<extension> [project_name].model_evaluate.<technology>.<version>.<extension> [project_name].model_deployment.<technology>.<version>.<extension>","title":"Project Asset Naming Convention"},{"location":"methodology/lightweight/#summary","text":"This article information on the lightweight IBM Cloud Garage Method for data science. It included a process model to map individual technology components to the reference architecture. A separate companion article discusses the architectural decision guidelines .","title":"Summary"},{"location":"model-dev/","text":"Building a machine learning model Integrate ML workbench with data Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning. Real time data sources The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Building AI model"},{"location":"model-dev/#building-a-machine-learning-model","text":"","title":"Building a machine learning model"},{"location":"model-dev/#integrate-ml-workbench-with-data","text":"Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning.","title":"Integrate ML workbench with data"},{"location":"model-dev/#real-time-data-sources","text":"The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Real time data sources"},{"location":"monitoring/","text":"Monitoring","title":"Bias monitoring to ensure equitable"},{"location":"monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"preparation/","text":"Data Preparation The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped. Data Cleansing During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Dealing with missing values Dealing with outliers Correcting typos Grouping sparse classes Dropping duplicates Key considerations Missing Values Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE). Outliers If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups. Grouping Sparse Classes Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used. Data Transformations It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Power functions Key Considerations: Categorical Encoding Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings. Dealing with Skewed Data Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation. Scaling Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization. Bias Mitigation If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover. Feature Engineering Feature engineering is the process of creating new features based upon knowledge about current features and the required task. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships Feature Extraction You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF). Capturing Feature Relationships Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns.","title":"Model selection"},{"location":"preparation/#data-preparation","text":"The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped.","title":"Data Preparation"},{"location":"preparation/#data-cleansing","text":"During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Dealing with missing values Dealing with outliers Correcting typos Grouping sparse classes Dropping duplicates","title":"Data Cleansing"},{"location":"preparation/#key-considerations","text":"","title":"Key considerations"},{"location":"preparation/#missing-values","text":"Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE).","title":"Missing Values"},{"location":"preparation/#outliers","text":"If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups.","title":"Outliers"},{"location":"preparation/#grouping-sparse-classes","text":"Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used.","title":"Grouping Sparse Classes"},{"location":"preparation/#data-transformations","text":"It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Power functions","title":"Data Transformations"},{"location":"preparation/#key-considerations_1","text":"","title":"Key Considerations:"},{"location":"preparation/#categorical-encoding","text":"Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings.","title":"Categorical Encoding"},{"location":"preparation/#dealing-with-skewed-data","text":"Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation.","title":"Dealing with Skewed Data"},{"location":"preparation/#scaling","text":"Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization.","title":"Scaling"},{"location":"preparation/#bias-mitigation","text":"If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover.","title":"Bias Mitigation"},{"location":"preparation/#feature-engineering","text":"Feature engineering is the process of creating new features based upon knowledge about current features and the required task. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships","title":"Feature Engineering"},{"location":"preparation/#feature-extraction","text":"You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF).","title":"Feature Extraction"},{"location":"preparation/#capturing-feature-relationships","text":"Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns.","title":"Capturing Feature Relationships"},{"location":"preparation/data-replication/","text":"Data replication STILL UNDER CONSTRUCTION Abstract In this article, we are dealing with database replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environment or to separate read and write models, which may be a viable solution with microservice, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern. Concepts In the following, we summarize the concepts around data replication. Data Replication is the process of storing data on more than one storage location on same or different data centers. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicate is in a consistently updated state and synchronized with the source Different data replication techniques exist: Transactional Replication : addresses full initial copies of the database and then receive updates as data changes in the same order as they occur with the publisher: transactional consistency is guaranteed. Snapshot Replication : distributes data exactly as it appears at a specific point in time; does not monitor for updates to the data. Used when data change less often. Merge Replication : Data from two or more databases is combined into a single database. It allows both publisher and subscriber to independently make changes to the database. CAP Theorem: Data replication in distributed computing like cloud falls into the problem of the CAP theorem where only two of three properties consisting of Consistency, Availability and Partition Tolerance can be met simultaneously. In our context Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off. Considerations The main advantages for data replication (providing a consistent copy of data across data storage locations) are: increased data availability increased access performance decreased network load Maintaining Data consistency at all different sites involves complex processes, and increase physical resources. Business motivations Among the top three priority of data integration and integrity solution, for 2019 and 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data sync (51%) Two important use cases: Business in Real Time Detect and react to data events as they happen to drive the business, and propagate those changes for others for consumption Optimize decision to sub-second latency level, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data replication deployments Data synchronization for zero down time data migrations and upgrades Classical data replication architecture The DB centric replication mechanism involves different techniques to apply data replication between databases of the same product (DB2 to DB2, Postgresqsl to Postgresql). The approach is to control availability and consistency. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions. For RDBMS, the replication mechanism can use asynchronous, semi-synchronous or synchronous. The classical topology uses a single master, that replicates the changes to all the slaves, this is mostly to avoid concurrent writes between different servers. Most RDBMS uses the master-slave pattern combined with asynchronous replication. All write requests are done on the master, but reads can happen on slaves as well. It helps to scale out the solution and also to support long distance replications. There are different techniques for replication, one of the most common is the file based log shipping, (as used by PostgreSQL), which is triggered once a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data changes in the order in which commits were made on the publisher side so that transactional consistency is guaranteed for the publications within any single subscription. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed (usually mitigated by 3 rd party tooling). Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL products, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities. Data lake technologies There are multiple alternatives build a data lake component. In the field, the following technologies are usually used: Hadoop, Apache Spark and Apache Kafka. Hadoop/HDFS (Hadoop File System) is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Apache Spark is a data processing engine tightly coupled with the Hadoop ecosystem. It is one of the most active open source projects ever. Spark is mainly used as ETL engine capable of integrating data from various SQL and NoSQL data sources and targets including RDBMS, NoSQL DB, Cloud Object Store, HDFS and other cluster file systems COS (Cloud Object Storage) in conjunction with a Query Engine like Apache SparkSQL or IBM Cloud SQLQuery Service is a legit data lake solution, especially because, as a managed service, it provides unlimited capacity and very high scalability and robustness against failures Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Although Kafka supports persisting data in queues for weeks or even months, it's not yet a proved technology for long term storage. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feeding data into Kafka) and data consumers (pulling data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr, etc... Change data capture (CDC) Another important part of the architecture is the change data capture component. For example, IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: Product explanations can be obtained here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. This is known as lambda-architecture . The combined architecture of a deployed solution looks like in the diagram below: With the management console, the developer can define a data replication project that can include one to many subscriptions. Subscriptions define the source database and tables and target kafka cluster and topics. The Kafka cluster can run on Kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running a subscription the first time, Kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more details about this solution see this product tour . Why adopting Kafka for data replication Using Kafka as a integration layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output targets Built in stream processing API on real time feeds with Kafka streams Commit Log Fault tolerance, scalability, multi-tenancy, speed, light-weight, multiple landing-zones. Kafka connect Kafka connect simplifies the integration between Kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed, fault tolerant runtime able to easily scale horizontally. The set of connectors help developers to not re-implement consumers and producers for every type of data source. To get started please read this introduction from the product documentation. The Kafka connect workers are stateless and can run easily on Kubernetes or as standalone docker process. Kafka Connect Source is to get data to Kafka, and Kafka Connect Sink to get data out of Kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements define a task. A connector can have multiple tasks. In distributed deployment, the connector supports scaling by adding new workers and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST API. Debezium Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use Kafka as backbone, and Kafka connect. It uses the approach of replicating one table to one Kafka topic. It can be used for data synchronization between microservices using CDC at a service level and propagate changes via Kafka. The implementation of the CQRS pattern may be simplified with this capability. Requirements We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in a Kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server performance with additional queries Address a writing model, where cloud native apps have to write back changes to the legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build. Note We are providing a special implementation of the container management service using Kafka connect. Recommended Readings IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial","title":"Data replication"},{"location":"preparation/data-replication/#data-replication","text":"STILL UNDER CONSTRUCTION Abstract In this article, we are dealing with database replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environment or to separate read and write models, which may be a viable solution with microservice, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern.","title":"Data replication"},{"location":"preparation/data-replication/#concepts","text":"In the following, we summarize the concepts around data replication. Data Replication is the process of storing data on more than one storage location on same or different data centers. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicate is in a consistently updated state and synchronized with the source Different data replication techniques exist: Transactional Replication : addresses full initial copies of the database and then receive updates as data changes in the same order as they occur with the publisher: transactional consistency is guaranteed. Snapshot Replication : distributes data exactly as it appears at a specific point in time; does not monitor for updates to the data. Used when data change less often. Merge Replication : Data from two or more databases is combined into a single database. It allows both publisher and subscriber to independently make changes to the database. CAP Theorem: Data replication in distributed computing like cloud falls into the problem of the CAP theorem where only two of three properties consisting of Consistency, Availability and Partition Tolerance can be met simultaneously. In our context Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off.","title":"Concepts"},{"location":"preparation/data-replication/#considerations","text":"The main advantages for data replication (providing a consistent copy of data across data storage locations) are: increased data availability increased access performance decreased network load Maintaining Data consistency at all different sites involves complex processes, and increase physical resources.","title":"Considerations"},{"location":"preparation/data-replication/#business-motivations","text":"Among the top three priority of data integration and integrity solution, for 2019 and 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data sync (51%) Two important use cases: Business in Real Time Detect and react to data events as they happen to drive the business, and propagate those changes for others for consumption Optimize decision to sub-second latency level, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data replication deployments Data synchronization for zero down time data migrations and upgrades","title":"Business motivations"},{"location":"preparation/data-replication/#classical-data-replication-architecture","text":"The DB centric replication mechanism involves different techniques to apply data replication between databases of the same product (DB2 to DB2, Postgresqsl to Postgresql). The approach is to control availability and consistency. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions. For RDBMS, the replication mechanism can use asynchronous, semi-synchronous or synchronous. The classical topology uses a single master, that replicates the changes to all the slaves, this is mostly to avoid concurrent writes between different servers. Most RDBMS uses the master-slave pattern combined with asynchronous replication. All write requests are done on the master, but reads can happen on slaves as well. It helps to scale out the solution and also to support long distance replications. There are different techniques for replication, one of the most common is the file based log shipping, (as used by PostgreSQL), which is triggered once a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data changes in the order in which commits were made on the publisher side so that transactional consistency is guaranteed for the publications within any single subscription. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed (usually mitigated by 3 rd party tooling). Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL products, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities.","title":"Classical data replication architecture"},{"location":"preparation/data-replication/#data-lake-technologies","text":"There are multiple alternatives build a data lake component. In the field, the following technologies are usually used: Hadoop, Apache Spark and Apache Kafka. Hadoop/HDFS (Hadoop File System) is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Apache Spark is a data processing engine tightly coupled with the Hadoop ecosystem. It is one of the most active open source projects ever. Spark is mainly used as ETL engine capable of integrating data from various SQL and NoSQL data sources and targets including RDBMS, NoSQL DB, Cloud Object Store, HDFS and other cluster file systems COS (Cloud Object Storage) in conjunction with a Query Engine like Apache SparkSQL or IBM Cloud SQLQuery Service is a legit data lake solution, especially because, as a managed service, it provides unlimited capacity and very high scalability and robustness against failures Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Although Kafka supports persisting data in queues for weeks or even months, it's not yet a proved technology for long term storage. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feeding data into Kafka) and data consumers (pulling data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr, etc...","title":"Data lake technologies"},{"location":"preparation/data-replication/#change-data-capture-cdc","text":"Another important part of the architecture is the change data capture component. For example, IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: Product explanations can be obtained here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. This is known as lambda-architecture . The combined architecture of a deployed solution looks like in the diagram below: With the management console, the developer can define a data replication project that can include one to many subscriptions. Subscriptions define the source database and tables and target kafka cluster and topics. The Kafka cluster can run on Kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running a subscription the first time, Kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more details about this solution see this product tour .","title":"Change data capture (CDC)"},{"location":"preparation/data-replication/#why-adopting-kafka-for-data-replication","text":"Using Kafka as a integration layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output targets Built in stream processing API on real time feeds with Kafka streams Commit Log Fault tolerance, scalability, multi-tenancy, speed, light-weight, multiple landing-zones.","title":"Why adopting Kafka for data replication"},{"location":"preparation/data-replication/#kafka-connect","text":"Kafka connect simplifies the integration between Kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed, fault tolerant runtime able to easily scale horizontally. The set of connectors help developers to not re-implement consumers and producers for every type of data source. To get started please read this introduction from the product documentation. The Kafka connect workers are stateless and can run easily on Kubernetes or as standalone docker process. Kafka Connect Source is to get data to Kafka, and Kafka Connect Sink to get data out of Kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements define a task. A connector can have multiple tasks. In distributed deployment, the connector supports scaling by adding new workers and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST API.","title":"Kafka connect"},{"location":"preparation/data-replication/#debezium","text":"Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use Kafka as backbone, and Kafka connect. It uses the approach of replicating one table to one Kafka topic. It can be used for data synchronization between microservices using CDC at a service level and propagate changes via Kafka. The implementation of the CQRS pattern may be simplified with this capability.","title":"Debezium"},{"location":"preparation/data-replication/#requirements","text":"We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in a Kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server performance with additional queries Address a writing model, where cloud native apps have to write back changes to the legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build. Note We are providing a special implementation of the container management service using Kafka connect.","title":"Requirements"},{"location":"preparation/data-replication/#recommended-readings","text":"IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial","title":"Recommended Readings"},{"location":"preparation/data-understanding/","text":"Data Understanding Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. There are two key stages of Data Understanding: a Data Assessment and Data Exploration . Data Assessment The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today? Key Considerations Collecting Ground Truth Data If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible. Data Relevance Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value. Quantity of Data If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price. Another rule of thumb is that you need ten times as degrees of freedom (or parameters/weights) your model has. In the case of linear regression for example, the number of degrees of freedom equals the number of features. In deep learning, every neuron adds and exponential growth to the degrees of freedom. Ethics It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow user groups, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those individuals in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them. Data Exploration Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they matching the distribution you expect (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others? Key Considerations Missing Values An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data is imputing values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, mode or an interpolation are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely. Outliers An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees. Un-balanced Data A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-understanding","text":"Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. There are two key stages of Data Understanding: a Data Assessment and Data Exploration .","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-assessment","text":"The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today?","title":"Data Assessment"},{"location":"preparation/data-understanding/#key-considerations","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#collecting-ground-truth-data","text":"If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible.","title":"Collecting Ground Truth Data"},{"location":"preparation/data-understanding/#data-relevance","text":"Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value.","title":"Data Relevance"},{"location":"preparation/data-understanding/#quantity-of-data","text":"If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price. Another rule of thumb is that you need ten times as degrees of freedom (or parameters/weights) your model has. In the case of linear regression for example, the number of degrees of freedom equals the number of features. In deep learning, every neuron adds and exponential growth to the degrees of freedom.","title":"Quantity of Data"},{"location":"preparation/data-understanding/#ethics","text":"It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow user groups, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those individuals in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them.","title":"Ethics"},{"location":"preparation/data-understanding/#data-exploration","text":"Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they matching the distribution you expect (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others?","title":"Data Exploration"},{"location":"preparation/data-understanding/#key-considerations_1","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#missing-values","text":"An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data is imputing values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, mode or an interpolation are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely.","title":"Missing Values"},{"location":"preparation/data-understanding/#outliers","text":"An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees.","title":"Outliers"},{"location":"preparation/data-understanding/#un-balanced-data","text":"A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Un-balanced Data"},{"location":"preparation/gov-data-lake/","text":"Building a Governed Data Lake As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party and external data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Data preparation"},{"location":"preparation/gov-data-lake/#building-a-governed-data-lake","text":"As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party and external data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Building a Governed Data Lake"},{"location":"runtimes/","text":"Runtime architectures Intelligent application integrating ML service and bias monitoring From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings. Intelligent application with real time analytics The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Runtime application"},{"location":"runtimes/#runtime-architectures","text":"","title":"Runtime architectures"},{"location":"runtimes/#intelligent-application-integrating-ml-service-and-bias-monitoring","text":"From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings.","title":"Intelligent application integrating ML service and bias monitoring"},{"location":"runtimes/#intelligent-application-with-real-time-analytics","text":"The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Intelligent application with real time analytics"},{"location":"topology/","text":"Data Topology Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics. Core elements of a Data Topology There are three Core elements of a data topology: Zone Map Data Flow Data Layer Zones and Zone Maps Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation. Data Flow The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help to detect circular data flows occurring across zones, to be flagged for investigation as data integrity may potentially be compromised if not well managed (designed and governed). Data Layer The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, edge node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity, here data will likely be highly distributed across an organization and certainly across an enterprise. For example, mixed deployment data layers include: Cloud - Public cloud, unconstrained Fog - Private cloud, constrained by available infrastructure Edge - Smart device, the most constrained by the limitations of compute capabilities, network bandwidth and availability Characteristics to Consider When Designing a Data Topology Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints Keeping an organic data topology A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can be removed after a given period of time (such as removing data from a raw zone after 7 business days) A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added Leaf zone guides A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. the location (virtual or physical) should be singular. A conceptual non-leaf zone can be added, that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations. Simplifying security with leaf zones A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach. Enterprises and Organizations When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly looking \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the system includes data ingestion from a company providing social media data, you can\u2019t complain on data quality from a particular tweet if they spelt your company name wrong. You have to have a data quality assessment process in place correcting spelling mistakes or simply remove those data points.","title":"Principles"},{"location":"topology/#data-topology","text":"Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics.","title":"Data Topology"},{"location":"topology/#core-elements-of-a-data-topology","text":"There are three Core elements of a data topology: Zone Map Data Flow Data Layer","title":"Core elements of a Data Topology"},{"location":"topology/#zones-and-zone-maps","text":"Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation.","title":"Zones and Zone Maps"},{"location":"topology/#data-flow","text":"The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help to detect circular data flows occurring across zones, to be flagged for investigation as data integrity may potentially be compromised if not well managed (designed and governed).","title":"Data Flow"},{"location":"topology/#data-layer","text":"The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, edge node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity, here data will likely be highly distributed across an organization and certainly across an enterprise. For example, mixed deployment data layers include: Cloud - Public cloud, unconstrained Fog - Private cloud, constrained by available infrastructure Edge - Smart device, the most constrained by the limitations of compute capabilities, network bandwidth and availability","title":"Data Layer"},{"location":"topology/#characteristics-to-consider-when-designing-a-data-topology","text":"Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints","title":"Characteristics to Consider When Designing a Data Topology"},{"location":"topology/#keeping-an-organic-data-topology","text":"A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can be removed after a given period of time (such as removing data from a raw zone after 7 business days) A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added","title":"Keeping an organic data topology"},{"location":"topology/#leaf-zone-guides","text":"A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. the location (virtual or physical) should be singular. A conceptual non-leaf zone can be added, that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations.","title":"Leaf zone guides"},{"location":"topology/#simplifying-security-with-leaf-zones","text":"A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach.","title":"Simplifying security with leaf zones"},{"location":"topology/#enterprises-and-organizations","text":"When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly looking \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the system includes data ingestion from a company providing social media data, you can\u2019t complain on data quality from a particular tweet if they spelt your company name wrong. You have to have a data quality assessment process in place correcting spelling mistakes or simply remove those data points.","title":"Enterprises and Organizations"}]}