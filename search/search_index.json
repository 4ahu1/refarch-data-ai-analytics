{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data - AI - Analytics Reference Architecture Abstract In this reference architecture, we are focusing on defining architecture patterns and best practices to build data and AI intensive applications. We are addressing how to integrate data governance, machine learning practices and the full life cycle of a cloud native solution development under the same reference architecture to present a holistic point of view on how to do it. When we consider development of Data and AI intensive applications or Intelligent Applications it is helpful to think of how the combination of three underlying architecture patterns Cloud Native application architecture patterns Data architecture patterns AI architecture patterns provides the right foundation to enable us to develop these Intelligent Applications in a highly agile cloud native way. By considering the nature of joins between the architectures we can also understand how the different roles such as Software Engineer, Data Engineer, and Data Scientist relate and work together in the development of such solutions. From a methodology point of view, a prescriptive approach to help all those project stakeholders to be successful is to adopt a four layers approach based on: COLLECT the data to make them easier to consume and accessible ORGANIZE to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere, available as services INFUSE to operationalize AI with trust and transparency There is no argument about it, AI without Data will not exist. The figure below represents how those layers are related to each other: IBM Data and AI Top Level Conceptual Architecture Based on the above prescriptive approach, a Data centric and AI reference architecture needs to support those layers. The following diagram represents the needed high level capabilities the reference architecture should support to address the Collect, Analyze, Organize and Infuse activities. This architecture diagram illustrates the need for strong data management capabilities inside a 'multi cloud data platform' (dark blue area), on which AI capabilities are plugged in to support analyze done by data scientists (machine learning workbench and business analytics). The data platform addresses the data collection and transformation tasks to move data to a (cloud)local highly scalable data store . Sometimes, data movement can or must be avoided. Common reasons include: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) network partition (data source unreliable e.g. remote IoT Gateway) when there is no need to do transformations or there is no performance impact to the origin data sources by adding readers, so a virtualization capability is necessary to open a view on remote data sources without moving data. On the AI side, data scientists need to perform data analysis , which includes making sense of the data using data visualization . To build a model they need to define features, and the AI environment supports feature engineering . Then to build the model, the development environment helps to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to machine learning cluster . Once the model provides acceptable accuracy level, it can be published as a service. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). When the model is deployed, monitoring capability, ensures the model is still accurate and even not biased. The intelligent application, represented as a combination of capabilities at the top of the diagram: business process, core application, CRM... can run on cloud, fog, or mist. It accesses the deployed model, access Data using APIs, and even consumes pre-built models, congitive services, like a speech to text and text to speech service, an image recognition , a tone analyzer services, the Natural Language Understanding ( NLU ), and chatbot . Data and AI reference architecture capabilities Another way to see this architecture is to zoom one more level to see the expected capabilities: The boundary rectangles are color coded to map the higher level purposes: collect, organize, analyze and infuse. Each icon represents a capability. This diagram is the foundation for the data AI reference architecture and we describe the data preparation , the application runtime and AI model development environment in separate notes. Map to products The following diagrams illustrate the product mapping to capability. Data is fundamental Managing data within the enterprise has proven to be challenging and complex. By itself data doesn't do anything. To make data useful, something other than the data is required \u2013 such as a computer program, a query, or a user (machine or person). Data is inert; it is not self-organizing or even self-understanding. This is what contributes to making data challenging and complex. In the DIKW pyramid , data is the base with the least amount of perceived usefulness. Information has higher value than data, knowledge has higher value than information, and wisdom has the highest perceived value of all. Data requires something else\u2014a program, a machine, or even a person\u2014to move up the value chain and become information. By organizing and classifying information, the value chain can begin and expand from data and information to be regarded as knowledge. Wisdom, the top of the pyramid, is the pinnacle of the data value chain. Wisdom results in a combination of inert data \u2013 a fundamental raw material in the modern digital age \u2013 combined with a series of progressive traits such as: perspective, context, understanding, learning, and the ability to reason. Cognitive computing and artificial intelligence now mean that these traits can be attributed to both a person and a machine. The IBM AI Ladder loosely parallels the DIKW pyramid in that the AI Ladder represents a progressive movement towards value creation within an enterprise. Increased value can be gained from completing activities at each step of the AI Ladder, with the potential to recognize higher levels of value, the higher the ladder is climbed. The AI Ladder contains four discrete levels: collect, organize, analyze, and infuse. Arguably, data lies below the first rung, recognizing the inert nature of data. The first rung is collect, a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. The AI Ladder progresses through the rungs to infuse, a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities. Data as a differentiator Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects . Data platform Principles There exists a spectrum ranging from single source of truth to data hyper personalisation. Fundamentally, we need to embrace the fact that different roles need specialised data stores with redundancy and replication between them, exerising specialisation through connectivity. Different application patterns apply different data specialisation. There is a clear dependency between AI and Data Management, but in an Intelligent Application context there is a Data concern, a AI model management concern, and multi cloud deployment concerns. As you constrain scalability and network connectivity you also constrain data store, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in the time series. Today, our users may have access to terabytes, petabytes, or even exabytes of data. But if that data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed, that data is not just useless, it can become a liability. Collect \u2013 Making Data Simple and Accessible The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above Organize \u2013 Trusted, Governed Analytics The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility Analyze \u2013 Insights On-Demand The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions Infuse \u2013 Operationalize AI with Trust and Transparency The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise Towards Data-Centricity Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods Making data enabled and active There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions.","title":"Introduction"},{"location":"#data-ai-analytics-reference-architecture","text":"Abstract In this reference architecture, we are focusing on defining architecture patterns and best practices to build data and AI intensive applications. We are addressing how to integrate data governance, machine learning practices and the full life cycle of a cloud native solution development under the same reference architecture to present a holistic point of view on how to do it. When we consider development of Data and AI intensive applications or Intelligent Applications it is helpful to think of how the combination of three underlying architecture patterns Cloud Native application architecture patterns Data architecture patterns AI architecture patterns provides the right foundation to enable us to develop these Intelligent Applications in a highly agile cloud native way. By considering the nature of joins between the architectures we can also understand how the different roles such as Software Engineer, Data Engineer, and Data Scientist relate and work together in the development of such solutions. From a methodology point of view, a prescriptive approach to help all those project stakeholders to be successful is to adopt a four layers approach based on: COLLECT the data to make them easier to consume and accessible ORGANIZE to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere, available as services INFUSE to operationalize AI with trust and transparency There is no argument about it, AI without Data will not exist. The figure below represents how those layers are related to each other:","title":"Data - AI - Analytics Reference Architecture"},{"location":"#ibm-data-and-ai-top-level-conceptual-architecture","text":"Based on the above prescriptive approach, a Data centric and AI reference architecture needs to support those layers. The following diagram represents the needed high level capabilities the reference architecture should support to address the Collect, Analyze, Organize and Infuse activities. This architecture diagram illustrates the need for strong data management capabilities inside a 'multi cloud data platform' (dark blue area), on which AI capabilities are plugged in to support analyze done by data scientists (machine learning workbench and business analytics). The data platform addresses the data collection and transformation tasks to move data to a (cloud)local highly scalable data store . Sometimes, data movement can or must be avoided. Common reasons include: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) network partition (data source unreliable e.g. remote IoT Gateway) when there is no need to do transformations or there is no performance impact to the origin data sources by adding readers, so a virtualization capability is necessary to open a view on remote data sources without moving data. On the AI side, data scientists need to perform data analysis , which includes making sense of the data using data visualization . To build a model they need to define features, and the AI environment supports feature engineering . Then to build the model, the development environment helps to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to machine learning cluster . Once the model provides acceptable accuracy level, it can be published as a service. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). When the model is deployed, monitoring capability, ensures the model is still accurate and even not biased. The intelligent application, represented as a combination of capabilities at the top of the diagram: business process, core application, CRM... can run on cloud, fog, or mist. It accesses the deployed model, access Data using APIs, and even consumes pre-built models, congitive services, like a speech to text and text to speech service, an image recognition , a tone analyzer services, the Natural Language Understanding ( NLU ), and chatbot .","title":"IBM Data and AI Top Level Conceptual Architecture"},{"location":"#data-and-ai-reference-architecture-capabilities","text":"Another way to see this architecture is to zoom one more level to see the expected capabilities: The boundary rectangles are color coded to map the higher level purposes: collect, organize, analyze and infuse. Each icon represents a capability. This diagram is the foundation for the data AI reference architecture and we describe the data preparation , the application runtime and AI model development environment in separate notes.","title":"Data and AI reference architecture capabilities"},{"location":"#map-to-products","text":"The following diagrams illustrate the product mapping to capability.","title":"Map to products"},{"location":"#data-is-fundamental","text":"Managing data within the enterprise has proven to be challenging and complex. By itself data doesn't do anything. To make data useful, something other than the data is required \u2013 such as a computer program, a query, or a user (machine or person). Data is inert; it is not self-organizing or even self-understanding. This is what contributes to making data challenging and complex. In the DIKW pyramid , data is the base with the least amount of perceived usefulness. Information has higher value than data, knowledge has higher value than information, and wisdom has the highest perceived value of all. Data requires something else\u2014a program, a machine, or even a person\u2014to move up the value chain and become information. By organizing and classifying information, the value chain can begin and expand from data and information to be regarded as knowledge. Wisdom, the top of the pyramid, is the pinnacle of the data value chain. Wisdom results in a combination of inert data \u2013 a fundamental raw material in the modern digital age \u2013 combined with a series of progressive traits such as: perspective, context, understanding, learning, and the ability to reason. Cognitive computing and artificial intelligence now mean that these traits can be attributed to both a person and a machine. The IBM AI Ladder loosely parallels the DIKW pyramid in that the AI Ladder represents a progressive movement towards value creation within an enterprise. Increased value can be gained from completing activities at each step of the AI Ladder, with the potential to recognize higher levels of value, the higher the ladder is climbed. The AI Ladder contains four discrete levels: collect, organize, analyze, and infuse. Arguably, data lies below the first rung, recognizing the inert nature of data. The first rung is collect, a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. The AI Ladder progresses through the rungs to infuse, a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities.","title":"Data is fundamental"},{"location":"#data-as-a-differentiator","text":"Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects .","title":"Data as a differentiator"},{"location":"#data-platform","text":"","title":"Data platform"},{"location":"#principles","text":"There exists a spectrum ranging from single source of truth to data hyper personalisation. Fundamentally, we need to embrace the fact that different roles need specialised data stores with redundancy and replication between them, exerising specialisation through connectivity. Different application patterns apply different data specialisation. There is a clear dependency between AI and Data Management, but in an Intelligent Application context there is a Data concern, a AI model management concern, and multi cloud deployment concerns. As you constrain scalability and network connectivity you also constrain data store, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in the time series. Today, our users may have access to terabytes, petabytes, or even exabytes of data. But if that data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed, that data is not just useless, it can become a liability.","title":"Principles"},{"location":"#collect-making-data-simple-and-accessible","text":"The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above","title":"Collect \u2013 Making Data Simple and Accessible"},{"location":"#organize-trusted-governed-analytics","text":"The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility","title":"Organize \u2013 Trusted, Governed Analytics"},{"location":"#analyze-insights-on-demand","text":"The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions","title":"Analyze \u2013 Insights On-Demand"},{"location":"#infuse-operationalize-ai-with-trust-and-transparency","text":"The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise","title":"Infuse \u2013 Operationalize AI with Trust and Transparency"},{"location":"#towards-data-centricity","text":"Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"Towards Data-Centricity"},{"location":"#making-data-enabled-and-active","text":"There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions.","title":"Making data enabled and active"},{"location":"deployment/","text":"Deployment","title":"Physical model execution"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"methodology/","text":"Methodology Warning UNDER CONSTRUCTION Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. Integrating data - devops and AI analytics methodologies Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. Personas Before going deeper into the different lifecycles, we need to define the personas involved in those lifecycles. The table below present the icons we are using in subsquent figures, with a short description for the role. Devops lifecycle The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before entering the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. AIOps lifecycle The ai-analytics development lifecycle (ADLC) supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, the ADLC relies on feedback mechanisms to help enhance machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. The ADLC iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. DataOps The data development lifecycle (DDLC) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DDLC is impacted by the SDLC and the ADLC. It iterates on both the incorporation of business requirements and on the manifestation of data. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. Integrating the cycles Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. Data Sciences Introduction The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper management, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories: Descriptive analytics This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years. Predictive analytics This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels. Prescriptive analytics Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis. Concepts Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification) Regression learning problem when the goal is to predict continuous value output Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. A feature is an attribute used as input for the model to train. Other names include dimension or column. Algorithm selection The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem. Another best practice is to use Linear Regression / Logistic Regression as baseline and try out other algorithms to improve on it. Challenges There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets. The Garage Method for Cloud with DataFirst Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem. Personas Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist Differences between analysts and data scientists The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Methodology"},{"location":"methodology/#methodology","text":"Warning UNDER CONSTRUCTION Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices.","title":"Methodology"},{"location":"methodology/#integrating-data-devops-and-ai-analytics-methodologies","text":"Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value.","title":"Integrating data - devops and AI analytics methodologies"},{"location":"methodology/#personas","text":"Before going deeper into the different lifecycles, we need to define the personas involved in those lifecycles. The table below present the icons we are using in subsquent figures, with a short description for the role.","title":"Personas"},{"location":"methodology/#devops-lifecycle","text":"The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before entering the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion.","title":"Devops lifecycle"},{"location":"methodology/#aiops-lifecycle","text":"The ai-analytics development lifecycle (ADLC) supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, the ADLC relies on feedback mechanisms to help enhance machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. The ADLC iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method.","title":"AIOps lifecycle"},{"location":"methodology/#dataops","text":"The data development lifecycle (DDLC) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DDLC is impacted by the SDLC and the ADLC. It iterates on both the incorporation of business requirements and on the manifestation of data. As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work.","title":"DataOps"},{"location":"methodology/#integrating-the-cycles","text":"Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling.","title":"Integrating the cycles"},{"location":"methodology/#data-sciences-introduction","text":"The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper management, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories:","title":"Data Sciences Introduction"},{"location":"methodology/#descriptive-analytics","text":"This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years.","title":"Descriptive analytics"},{"location":"methodology/#predictive-analytics","text":"This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels.","title":"Predictive analytics"},{"location":"methodology/#prescriptive-analytics","text":"Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis.","title":"Prescriptive analytics"},{"location":"methodology/#concepts","text":"Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification) Regression learning problem when the goal is to predict continuous value output Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. A feature is an attribute used as input for the model to train. Other names include dimension or column.","title":"Concepts"},{"location":"methodology/#algorithm-selection","text":"The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem. Another best practice is to use Linear Regression / Logistic Regression as baseline and try out other algorithms to improve on it.","title":"Algorithm selection"},{"location":"methodology/#challenges","text":"There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets.","title":"Challenges"},{"location":"methodology/#the-garage-method-for-cloud-with-datafirst","text":"Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"The Garage Method for Cloud with DataFirst"},{"location":"methodology/#personas_1","text":"Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist","title":"Personas"},{"location":"methodology/#differences-between-analysts-and-data-scientists","text":"The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Differences between analysts and data scientists"},{"location":"model-dev/","text":"Building a machine learning model Integrate ML workbench with data Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning. Real time data sources The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Building AI model"},{"location":"model-dev/#building-a-machine-learning-model","text":"","title":"Building a machine learning model"},{"location":"model-dev/#integrate-ml-workbench-with-data","text":"Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning.","title":"Integrate ML workbench with data"},{"location":"model-dev/#real-time-data-sources","text":"The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Real time data sources"},{"location":"monitoring/","text":"Monitoring","title":"Bias monitoring to ensure equitable"},{"location":"monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"preparation/","text":"Data Preparation The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped. Data Cleansing During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Dealing with missing values Dealing with outliers Correcting typos Grouping sparse classes Dropping duplicates Key considerations Missing Values Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE). Outliers If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups. Grouping Sparse Classes Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used. Data Transformations It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Power functions Key Considerations: Categorical Encoding Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings. Dealing with Skewed Data Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation. Scaling Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization. Bias Mitigation If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover. Feature Engineering Feature engineering is the process of creating new features based upon knowledge about current features and the required task. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships Feature Extraction You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF). Capturing Feature Relationships Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns.","title":"Model selection"},{"location":"preparation/#data-preparation","text":"The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped.","title":"Data Preparation"},{"location":"preparation/#data-cleansing","text":"During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Dealing with missing values Dealing with outliers Correcting typos Grouping sparse classes Dropping duplicates","title":"Data Cleansing"},{"location":"preparation/#key-considerations","text":"","title":"Key considerations"},{"location":"preparation/#missing-values","text":"Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE).","title":"Missing Values"},{"location":"preparation/#outliers","text":"If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups.","title":"Outliers"},{"location":"preparation/#grouping-sparse-classes","text":"Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used.","title":"Grouping Sparse Classes"},{"location":"preparation/#data-transformations","text":"It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Power functions","title":"Data Transformations"},{"location":"preparation/#key-considerations_1","text":"","title":"Key Considerations:"},{"location":"preparation/#categorical-encoding","text":"Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings.","title":"Categorical Encoding"},{"location":"preparation/#dealing-with-skewed-data","text":"Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation.","title":"Dealing with Skewed Data"},{"location":"preparation/#scaling","text":"Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization.","title":"Scaling"},{"location":"preparation/#bias-mitigation","text":"If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover.","title":"Bias Mitigation"},{"location":"preparation/#feature-engineering","text":"Feature engineering is the process of creating new features based upon knowledge about current features and the required task. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships","title":"Feature Engineering"},{"location":"preparation/#feature-extraction","text":"You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF).","title":"Feature Extraction"},{"location":"preparation/#capturing-feature-relationships","text":"Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns.","title":"Capturing Feature Relationships"},{"location":"preparation/data-replication/","text":"Data replication STILL UNDER CONSTRUCTION Abstract In this article, we are dealing with database replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environment or to separate read and write models, which may be a viable solution with microservice, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern. Concepts We just to quickly summarize the concepts around data replication. Data Replication is the process of storing data in more than one storae node on same or different data centers. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicate is in a consistently updated state and synchronized with the source Different data replication techniques: Transactional Replication : addresses full initial copies of the database and then receive updates as data changes in the same order as they occur with the publisher: transactional consistency is guaranteed. Snapshot Replication : distributes data exactly as it appears at a specific moment in time does not monitor for updates to the data. Used when data change less often. Merge Replication : Data from two or more databases is combined into a single database. It allows both publisher and subscriber to independently make changes to the database. CAP Theorem: Data replication in distributed computing like cloud falls into the problem of the CAP theorem where only two of the Consistency, Availability, or Partition tolerance can be met simultaneously. In our context Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off. Considerations The main advantages for data replication are to provide a consistent copy of data across all the database servers, and increase data availability to different client apps. Maintaining Data consistency at all different sites involves complex processes, and increase physical resources. Business motivations Among the top three priority of data integration and integriry solution, for 2019, 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data synch (51%) Two important use cases: Business in Real Time Detect and react to data events as they happen to drive the business, and propagate those changes for others to consume Optimize decision making with up to the second data, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data deployments Data synchronization for zero down time data migrations and upgrades Classical data replication architecture The DB centric replication mechanism involves different techniques to do data replication between databases of the same product (DB2 to DB2, Postgresqsl to postgresql). The approach is to control availability and consistency. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions. For RDBMS, the replication mechanism can use asynchronous, semi-synchronous or synchronous replication. The classical topology is to have a single leader, that replicates the changes to all the followers, this is mostly to avoid concurrent writes between different servers. Most RDBMS uses the master-slave pattern combined with asynchronous replication. All write requests are done on the master, but read could happen on slaves. It helps to scale out the solution and also to support long distance replications. There are different techniques for replication, one of the most common is the file based log shipping, (as used by PostgreSQL), which is triggered once a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data in the order in which commits were made on the publisher so that transactional consistency is guaranteed for the publications within any single subscription. As a result, there is a window for data loss should the primary server suffer a catastrophic failure; transactions not yet shipped will be lost. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed. Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL product, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities. Data lake technologies There are multiple alternatives to support data lake component. In our study, two different approaches are used: Hadoop, or kafka. Hadoop is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feed data into Kafka) and data consumers (pull data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr... Change data capture (CDC) Another important part of the architecture is the change data capture component. IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: And you can get product explanations here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. The combined architecture of a deployed solution looks like in the diagram below: With the management console, developer can define data replication project, that can includes one to many subscriptions. Subscription defines the source database and tables and target kafka cluster and topics. The Kafka cluster is running on kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running subscription the first time, kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more detail about this solution see this product tour . Why adopting kafka for data replication Using Kafka as a mediation layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output Targets Built in stream processing API on real time feeds with kafka streams Commit Log Fault tolerance, scalability, multi-tenant nature, speed, light-weight, multiple landing-zones. Kafka connect Kafka connect simplifies the integration between kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed fault tolerant runtime to be able to easily scale horizontally. The set of connectors help developers to do not reinvent coding consumers and producers. To get started read this introduction from product documentation. The kafka connect workers are stateless and can run easily on kubernetes or as standalone docker process. Kafka Connect Source is to get data to kafka, and Kafka Connect Sink to get data out of kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements are defined a task. A connector can have multiple tasks. With distributed deployment the connector cluster supports easy scaling by adding new worker and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST api. Debezium Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use kafka as backbone, and kafka connect. It uses the approach of one table to one topic. It can be used to do data synchronization between microservices using CDC at one service level and propagate changes via kafka. The implementation of the CQRS pattern may be simplified with this capability. Requirements We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server with a lot of new queries Address writing model, where cloud native apps have to write back to legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data is for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build. Note We are providing a special implementation of the container management service using kafka connect. Recommended Readings IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial","title":"Data replication"},{"location":"preparation/data-replication/#data-replication","text":"STILL UNDER CONSTRUCTION Abstract In this article, we are dealing with database replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environment or to separate read and write models, which may be a viable solution with microservice, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern.","title":"Data replication"},{"location":"preparation/data-replication/#concepts","text":"We just to quickly summarize the concepts around data replication. Data Replication is the process of storing data in more than one storae node on same or different data centers. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicate is in a consistently updated state and synchronized with the source Different data replication techniques: Transactional Replication : addresses full initial copies of the database and then receive updates as data changes in the same order as they occur with the publisher: transactional consistency is guaranteed. Snapshot Replication : distributes data exactly as it appears at a specific moment in time does not monitor for updates to the data. Used when data change less often. Merge Replication : Data from two or more databases is combined into a single database. It allows both publisher and subscriber to independently make changes to the database. CAP Theorem: Data replication in distributed computing like cloud falls into the problem of the CAP theorem where only two of the Consistency, Availability, or Partition tolerance can be met simultaneously. In our context Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off.","title":"Concepts"},{"location":"preparation/data-replication/#considerations","text":"The main advantages for data replication are to provide a consistent copy of data across all the database servers, and increase data availability to different client apps. Maintaining Data consistency at all different sites involves complex processes, and increase physical resources.","title":"Considerations"},{"location":"preparation/data-replication/#business-motivations","text":"Among the top three priority of data integration and integriry solution, for 2019, 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data synch (51%) Two important use cases: Business in Real Time Detect and react to data events as they happen to drive the business, and propagate those changes for others to consume Optimize decision making with up to the second data, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data deployments Data synchronization for zero down time data migrations and upgrades","title":"Business motivations"},{"location":"preparation/data-replication/#classical-data-replication-architecture","text":"The DB centric replication mechanism involves different techniques to do data replication between databases of the same product (DB2 to DB2, Postgresqsl to postgresql). The approach is to control availability and consistency. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions. For RDBMS, the replication mechanism can use asynchronous, semi-synchronous or synchronous replication. The classical topology is to have a single leader, that replicates the changes to all the followers, this is mostly to avoid concurrent writes between different servers. Most RDBMS uses the master-slave pattern combined with asynchronous replication. All write requests are done on the master, but read could happen on slaves. It helps to scale out the solution and also to support long distance replications. There are different techniques for replication, one of the most common is the file based log shipping, (as used by PostgreSQL), which is triggered once a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data in the order in which commits were made on the publisher so that transactional consistency is guaranteed for the publications within any single subscription. As a result, there is a window for data loss should the primary server suffer a catastrophic failure; transactions not yet shipped will be lost. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed. Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL product, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities.","title":"Classical data replication architecture"},{"location":"preparation/data-replication/#data-lake-technologies","text":"There are multiple alternatives to support data lake component. In our study, two different approaches are used: Hadoop, or kafka. Hadoop is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feed data into Kafka) and data consumers (pull data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr...","title":"Data lake technologies"},{"location":"preparation/data-replication/#change-data-capture-cdc","text":"Another important part of the architecture is the change data capture component. IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: And you can get product explanations here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. The combined architecture of a deployed solution looks like in the diagram below: With the management console, developer can define data replication project, that can includes one to many subscriptions. Subscription defines the source database and tables and target kafka cluster and topics. The Kafka cluster is running on kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running subscription the first time, kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more detail about this solution see this product tour .","title":"Change data capture (CDC)"},{"location":"preparation/data-replication/#why-adopting-kafka-for-data-replication","text":"Using Kafka as a mediation layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output Targets Built in stream processing API on real time feeds with kafka streams Commit Log Fault tolerance, scalability, multi-tenant nature, speed, light-weight, multiple landing-zones.","title":"Why adopting kafka for data replication"},{"location":"preparation/data-replication/#kafka-connect","text":"Kafka connect simplifies the integration between kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed fault tolerant runtime to be able to easily scale horizontally. The set of connectors help developers to do not reinvent coding consumers and producers. To get started read this introduction from product documentation. The kafka connect workers are stateless and can run easily on kubernetes or as standalone docker process. Kafka Connect Source is to get data to kafka, and Kafka Connect Sink to get data out of kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements are defined a task. A connector can have multiple tasks. With distributed deployment the connector cluster supports easy scaling by adding new worker and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST api.","title":"Kafka connect"},{"location":"preparation/data-replication/#debezium","text":"Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use kafka as backbone, and kafka connect. It uses the approach of one table to one topic. It can be used to do data synchronization between microservices using CDC at one service level and propagate changes via kafka. The implementation of the CQRS pattern may be simplified with this capability.","title":"Debezium"},{"location":"preparation/data-replication/#requirements","text":"We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server with a lot of new queries Address writing model, where cloud native apps have to write back to legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data is for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build. Note We are providing a special implementation of the container management service using kafka connect.","title":"Requirements"},{"location":"preparation/data-replication/#recommended-readings","text":"IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial","title":"Recommended Readings"},{"location":"preparation/data-understanding/","text":"Data Understanding Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. There are two key stages of Data Understanding: a Data Assessment and Data Exploration . Data Assessment The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today? Key Considerations Collecting Ground Truth Data If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible. Data Relevance Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value. Quantity of Data If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price. Ethics It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow a user group, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them. Data Exploration Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they the distribution you expected (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others? Key Considerations Missing Values An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data, you can impute values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, or mode are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely. Outliers An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees. Un-balanced Data A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-understanding","text":"Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. There are two key stages of Data Understanding: a Data Assessment and Data Exploration .","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-assessment","text":"The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today?","title":"Data Assessment"},{"location":"preparation/data-understanding/#key-considerations","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#collecting-ground-truth-data","text":"If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible.","title":"Collecting Ground Truth Data"},{"location":"preparation/data-understanding/#data-relevance","text":"Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value.","title":"Data Relevance"},{"location":"preparation/data-understanding/#quantity-of-data","text":"If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price.","title":"Quantity of Data"},{"location":"preparation/data-understanding/#ethics","text":"It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow a user group, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them.","title":"Ethics"},{"location":"preparation/data-understanding/#data-exploration","text":"Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they the distribution you expected (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others?","title":"Data Exploration"},{"location":"preparation/data-understanding/#key-considerations_1","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#missing-values","text":"An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data, you can impute values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, or mode are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely.","title":"Missing Values"},{"location":"preparation/data-understanding/#outliers","text":"An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees.","title":"Outliers"},{"location":"preparation/data-understanding/#un-balanced-data","text":"A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Un-balanced Data"},{"location":"preparation/gov-data-lake/","text":"Building a Governed Data Lake As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party and external data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Data preparation"},{"location":"preparation/gov-data-lake/#building-a-governed-data-lake","text":"As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party and external data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Building a Governed Data Lake"},{"location":"runtimes/","text":"Runtime architectures Intelligent application integrating ML service and bias monitoring From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings. Intelligent application with real time analytics The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Runtime application"},{"location":"runtimes/#runtime-architectures","text":"","title":"Runtime architectures"},{"location":"runtimes/#intelligent-application-integrating-ml-service-and-bias-monitoring","text":"From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings.","title":"Intelligent application integrating ML service and bias monitoring"},{"location":"runtimes/#intelligent-application-with-real-time-analytics","text":"The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Intelligent application with real time analytics"},{"location":"topology/","text":"Data Topology Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics. Core elements of a Data Topology There are three Core elements of a data topology: Zone Map Data Flow Data Layer Zones and Zone Maps Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation. Data Flow The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help show when circular data flows occur across the zones, this can become a flag for investigation as data integrity may potentially be compromised if not well managed (designed and governed). Data Layer The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, mist node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity. Cloud - Public cloud, unconstrained or maybe best termed the least constrained Fog - Private cloud, constrained by available infrastructure Mist - Smart device, the most constrained by the limitations of the device platform where data will likely be highly distributed across an organization and certainly across an enterprise. For example mixed deployment data layers cloud include: Characteristics to Consider When Designing a Data Topology Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints Keeping an organic data topology A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can removed after a given period of time (such as removing data from a raw zone after 7 business days) An aging policy can be set to infinity so as not to remove data A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added Leaf zone guides A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. Keep the location (virtual or physical) should also be singular. A conceptual non-leaf zone can be added that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations. Simplifying security with leaf zones A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach. Enterprises and Organizations When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly look \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the ecosystem included aggregation from a company providing social media data, you can\u2019t call \u201cfoul\u201d on the level of data quality from a particular tweet if they spelt your company name wrong. You have to be able to handle the situation without going back to the author and saying, \u201cstop that!\u201d","title":"Data provenance"},{"location":"topology/#data-topology","text":"Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics.","title":"Data Topology"},{"location":"topology/#core-elements-of-a-data-topology","text":"There are three Core elements of a data topology: Zone Map Data Flow Data Layer","title":"Core elements of a Data Topology"},{"location":"topology/#zones-and-zone-maps","text":"Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation.","title":"Zones and Zone Maps"},{"location":"topology/#data-flow","text":"The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help show when circular data flows occur across the zones, this can become a flag for investigation as data integrity may potentially be compromised if not well managed (designed and governed).","title":"Data Flow"},{"location":"topology/#data-layer","text":"The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, mist node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity. Cloud - Public cloud, unconstrained or maybe best termed the least constrained Fog - Private cloud, constrained by available infrastructure Mist - Smart device, the most constrained by the limitations of the device platform where data will likely be highly distributed across an organization and certainly across an enterprise. For example mixed deployment data layers cloud include:","title":"Data Layer"},{"location":"topology/#characteristics-to-consider-when-designing-a-data-topology","text":"Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints","title":"Characteristics to Consider When Designing a Data Topology"},{"location":"topology/#keeping-an-organic-data-topology","text":"A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can removed after a given period of time (such as removing data from a raw zone after 7 business days) An aging policy can be set to infinity so as not to remove data A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added","title":"Keeping an organic data topology"},{"location":"topology/#leaf-zone-guides","text":"A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. Keep the location (virtual or physical) should also be singular. A conceptual non-leaf zone can be added that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations.","title":"Leaf zone guides"},{"location":"topology/#simplifying-security-with-leaf-zones","text":"A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach.","title":"Simplifying security with leaf zones"},{"location":"topology/#enterprises-and-organizations","text":"When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly look \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the ecosystem included aggregation from a company providing social media data, you can\u2019t call \u201cfoul\u201d on the level of data quality from a particular tweet if they spelt your company name wrong. You have to be able to handle the situation without going back to the author and saying, \u201cstop that!\u201d","title":"Enterprises and Organizations"}]}