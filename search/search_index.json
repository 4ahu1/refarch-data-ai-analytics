{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data and AI Reference Architecture Abstract In this reference architecture, we define the architecture patterns and best practices for developing Data and AI intensive applications or Intelligent applications . We consider how data, data governance, analytics and machine learning practices join with the agile life cycle of cloud native application development to enable the development and delivery of Intelligent Applications . When we consider the Data and AI Reference Architecture in terms of developing intelligent applications it becomes apparent that we are looking at bringing together three group of architecture patterns: Cloud Native application architecture patterns Data architecture patterns AI architecture patterns We can think of the Data and AI reference architecture being the sum of these architecture patterns, plus the joins between them. The joins are also key to understanding how Software Engineers, Data Engineers, and Data Scientist, relate and work together in the development of such Intelligent applications . As we look to methodology for developing such solutions we need to consider a prescriptive approach which brings those project stakeholders together to be successful. To do this we adopt a four layers approach: COLLECT data to make them easier to consume and accessible ORGANIZE data to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere INFUSE to operationalize AI with trust and transparency The figure below represents how those layers are related to each other: Figure 1: AI Ladder IBM Data and AI Conceptual Architecture Based on the above prescriptive approach, we cans see that a Data centric and AI reference architecture needs to implement the four layers as shown in the following diagram. Figure 2 : High level view of the Data & amp ; AI reference architecture This architecture diagram illustrates: strong data collection and management capabilities, inside a 'multi cloud data platform' (dark blue area), on which AI capabilities are plugged in to support analysis done by data scientists (machine learning workbench and business analytics) and build models that are consumed by business applications to become more 'intelligent'. Those applications can also consume pre-built model like speech recognition and MLUnderstanding. The data platform addresses the data collection and transformation tasks to move data to a cloud or local highly scalable data store . However we must also recognise that there are cases, where data movement can or must be avoided. For examples where: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) network partition (data source unreliable e.g. remote IoT Gateway) In such cases the data platform provides a virtualization capability which can open a view on remote data sources without moving data. In Analyze data scientists need to perform: data analysis , which includes making sense of the data using data visualization . feature engineering to define the features they need to build an ML model. Then to build the model , the development environment provides the AI frameworks and helps the data scientists to select and combine the different algorithms and to tune the hyper parameters. The model training can be done on local cluster or can be executed, at the big data scale level, to a machine learning cluster. Once the model provides acceptable accuracy level, it can be published so that it can be consumed or infused within an application or exposed as a service or as an autonomous agent. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). Once the model is deployed, monitoring capabilities, ensures the model is still accurate and not biased. The intelligent application , is represented as a combination of capabilities at the top of the diagram, it can be an application we develop, a business process, an ERP or CRM application, etc. running anywhere on cloud, fog, or mist computing. The intelligent application, accesses the deployed model, APIs, and may consumes pre-built models or Cognitive services, such as: speech to text and text to speech services image recognition , a tone analyzer services NLU Natural Language Understanding and chatbot services. Data and AI reference architecture capabilities In the view below of the reference architecture we have zoomed in a level to show the detail of how we realize the required capabilities. Figure 3: High level view of the Data and AI reference architecture This diagram becomes the foundation of the Data and AI reference architecture but can built incrementally as explain in this section . Data collection and organization AI model development Application runtime Mapping to products IBM is offering a comprehensive suite of products to support the above capabilities, the following diagram illustrates the product mapping:","title":"Introduction"},{"location":"#data-and-ai-reference-architecture","text":"Abstract In this reference architecture, we define the architecture patterns and best practices for developing Data and AI intensive applications or Intelligent applications . We consider how data, data governance, analytics and machine learning practices join with the agile life cycle of cloud native application development to enable the development and delivery of Intelligent Applications . When we consider the Data and AI Reference Architecture in terms of developing intelligent applications it becomes apparent that we are looking at bringing together three group of architecture patterns: Cloud Native application architecture patterns Data architecture patterns AI architecture patterns We can think of the Data and AI reference architecture being the sum of these architecture patterns, plus the joins between them. The joins are also key to understanding how Software Engineers, Data Engineers, and Data Scientist, relate and work together in the development of such Intelligent applications . As we look to methodology for developing such solutions we need to consider a prescriptive approach which brings those project stakeholders together to be successful. To do this we adopt a four layers approach: COLLECT data to make them easier to consume and accessible ORGANIZE data to create a trusted analytics foundation on data with business meaning ANALYZE to scale business insight with AI everywhere INFUSE to operationalize AI with trust and transparency The figure below represents how those layers are related to each other: Figure 1: AI Ladder","title":"Data and AI Reference Architecture"},{"location":"#ibm-data-and-ai-conceptual-architecture","text":"Based on the above prescriptive approach, we cans see that a Data centric and AI reference architecture needs to implement the four layers as shown in the following diagram. Figure 2 : High level view of the Data & amp ; AI reference architecture This architecture diagram illustrates: strong data collection and management capabilities, inside a 'multi cloud data platform' (dark blue area), on which AI capabilities are plugged in to support analysis done by data scientists (machine learning workbench and business analytics) and build models that are consumed by business applications to become more 'intelligent'. Those applications can also consume pre-built model like speech recognition and MLUnderstanding. The data platform addresses the data collection and transformation tasks to move data to a cloud or local highly scalable data store . However we must also recognise that there are cases, where data movement can or must be avoided. For examples where: no transformations necessary (e.g. accessing an external data mart via SQL or API) no performance impact (e.g. materialized SQL views served by a parallel database backend) regulatory aspects (each reach access to a data source must be logged to an audit log) real-time aspects (data must be processed immediately, latency of storage too high) size (data movement too expensive from a network bandwith perspective, compute must move toward data source) privacy (data can't be copied, only aggregates as a result of compute can be moved) network partition (data source unreliable e.g. remote IoT Gateway) In such cases the data platform provides a virtualization capability which can open a view on remote data sources without moving data. In Analyze data scientists need to perform: data analysis , which includes making sense of the data using data visualization . feature engineering to define the features they need to build an ML model. Then to build the model , the development environment provides the AI frameworks and helps the data scientists to select and combine the different algorithms and to tune the hyper parameters. The model training can be done on local cluster or can be executed, at the big data scale level, to a machine learning cluster. Once the model provides acceptable accuracy level, it can be published so that it can be consumed or infused within an application or exposed as a service or as an autonomous agent. The model management capability supports the meta-data definition and the life cycle management of the model (data lineage). Once the model is deployed, monitoring capabilities, ensures the model is still accurate and not biased. The intelligent application , is represented as a combination of capabilities at the top of the diagram, it can be an application we develop, a business process, an ERP or CRM application, etc. running anywhere on cloud, fog, or mist computing. The intelligent application, accesses the deployed model, APIs, and may consumes pre-built models or Cognitive services, such as: speech to text and text to speech services image recognition , a tone analyzer services NLU Natural Language Understanding and chatbot services.","title":"IBM Data and AI Conceptual Architecture"},{"location":"#data-and-ai-reference-architecture-capabilities","text":"In the view below of the reference architecture we have zoomed in a level to show the detail of how we realize the required capabilities. Figure 3: High level view of the Data and AI reference architecture This diagram becomes the foundation of the Data and AI reference architecture but can built incrementally as explain in this section . Data collection and organization AI model development Application runtime","title":"Data and AI reference architecture capabilities"},{"location":"#mapping-to-products","text":"IBM is offering a comprehensive suite of products to support the above capabilities, the following diagram illustrates the product mapping:","title":"Mapping to products"},{"location":"architecture/","text":"Data and AI Architecture Principles Context As we look to the Data and AI reference architecture it becomes essential that we have deep understanding of the range of uses, how value is realized from data, the dependencies between data AI and applications, and the constraints which we may face in multi-cloud environments with compute power, data storage and network connectivity/latency. Through consideration of these aspects we have pulled out the following Guiding Principles which help to guide both the architecture patterns and our thinking or methodology for how to approach developing Intelligent applications : There exists a spectrum of concerns ranging from single source of truth to data hyper personalisation. Different roles require specialised data stores with redundancy and replication between them Different application patterns apply different data specialisations. There is a dependency between AI and Data Management. With an Intelligent Application there are a Data concern, an AI model management concern, a Multi cloud deployment concern. As you constrain scalability and network connectivity you also constrain data storage, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in a time series. Looking ahead with the modern digital business we will increasingly see our users have access to terabytes, petabytes, or even exabytes of data. If this data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed , the data is not just useless, it becomes a liability. Embracing this challenge by putting in the right systems, built on a well defined data an AI architecture and following proven methodologies for developing new Intelligent Applications will become an essential step for all enterprises. Architectural Principles Name Simplicity First Statement If a functionality or Technology Component is not needed in the current iteration, it shouldn't be part of the architecture. There must be a need for it in the current iteration. Rationale Distraction is on of our worst enemies. Humans tend to get lost in the abundance of possibilities. By sticking to the Architectural Principle, clear focus on project delivery is ensured. Implications The majority of projects are over-engineered. This adds direct cost for additional unnecessary work and indirect maintenance cost due to architectural complexity. By enforcing the most simple architecture practitioners can focus on solving problems without getting distracted. Name OpenSource First Statement If a functionality or Technology Component is available as Open Source, this should be given preference. Rationale Open Source software tends to be more stable, better documented and better understood which pushed TCO down Implications When using Open Source vendor lock-in is reduced, experts are more abundant and standards are followed more tightly. In addition, independence from a vendor allows for development of extensions to address business needs which might not be available in timely manner from the vendor Name Homogeneity First Statement If a Technology Component is available from the same product suite it should be given preference. Rationale Although standards exist and a lot of products are compatible with each other, compatibility never reaches the level of products coming from the same product suite. Implications If one relies on standards assuming compatibility between Technology Components during Architectural Development and those are broken whole Solution Architectures or Technology Architectures can become completely or partially invalid. Improving homogeneity mitigates that risk. Name Never touch a running system Statement If a Technical Component is needed, it should be provisioned from scratch without reusing existing systems. Rationale When creating \"Systems of Innovation\" speed is on of the key drivers. Therefore, every interaction with existing system introduces complexity, delay and risk. Implications Especially in public, hybrid and private Cloud environments it is more effective to provision a Technical Component as a Service over adjusting an existing Technical Component. Name Data Locality First Statement Allocate compute and storage always as close as possible Rationale Data movement is more expensive than data storage. Bottlenecks slow systems down. Implications Data movement, especially between cloud providers or data centers are usually more expensive than allocating a compute note close to the data. Therefore, cost can be kept low and performance can be kept high Defining the architecture incrementally The IBM Garage methodology guides the development team to adopt an incremental and iterative development practices, lean method. As part of the lean adoption, the implementation of an intelligent application integrates different concerns: microservice, data and AI model developments. But architecture is not forgotten, and it should be done also incrementally. Even if the target end to end solution will look similar to the reference architecture as illustrated in the diagram below, not all components need to be in place at the beginning of a Minimum Viable Product. Figure 1: The Data and AI Reference Architecture The goal of the architects will be, to select product for each of the needed capabilities following the guidances presented in the architectural principles above and build the architecture per increment and reuse previously define capabilities. Because it can be hard to initially define the architecture of a project, our method starts with the reference architecture and supports architectural changes while following the development of the process model. Figure 2: Process Model and Architectural Development Method This way application development and architectural development work hand in hand to produce a production ready and deployable product at the end of each iteration. Data science lightweight architecture As an example, a Data Scientist's environment architecture, in the context of getting started, or for a MVP, will include the minimum components, as illustrated below: Figure 3: The Lightweight Reference Architecture but can evolve over time to address most of the components as described in the model building reference architecture. Microservice integration lightweight architecture For the intelligent application the first architecture may include microservice, serving Web channel, using the Backend for frontend pattern, and doing integration with other microservices. Each service has its own repository that can use different technologies like RDBMS, noSQL, filesystem, cache... A service can be consumer and publisher of events, and can integrate with pre-built AI model. Figure 4: Microservice MVP architecture The scoring service can be deployed as a service or integrated in an agent as part of a real time streaming analytics. The extended runtime architecture is presented in this article . Data lightweight architecture The same apply for the data architecture: the previous diagram illustrates the need to have data ingestion components responsible to extract, transform and load data from data source to storage area. In distributed system the storage need to support high availability and distribution intra and inter datacenters. When scaling the data adoption, architects need to work on the data topology , and extend to the reference architecture as presented in this article .","title":"Architecture Principles"},{"location":"architecture/#data-and-ai-architecture-principles","text":"","title":"Data and AI Architecture Principles"},{"location":"architecture/#context","text":"As we look to the Data and AI reference architecture it becomes essential that we have deep understanding of the range of uses, how value is realized from data, the dependencies between data AI and applications, and the constraints which we may face in multi-cloud environments with compute power, data storage and network connectivity/latency. Through consideration of these aspects we have pulled out the following Guiding Principles which help to guide both the architecture patterns and our thinking or methodology for how to approach developing Intelligent applications : There exists a spectrum of concerns ranging from single source of truth to data hyper personalisation. Different roles require specialised data stores with redundancy and replication between them Different application patterns apply different data specialisations. There is a dependency between AI and Data Management. With an Intelligent Application there are a Data concern, an AI model management concern, a Multi cloud deployment concern. As you constrain scalability and network connectivity you also constrain data storage, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in a time series. Looking ahead with the modern digital business we will increasingly see our users have access to terabytes, petabytes, or even exabytes of data. If this data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed , the data is not just useless, it becomes a liability. Embracing this challenge by putting in the right systems, built on a well defined data an AI architecture and following proven methodologies for developing new Intelligent Applications will become an essential step for all enterprises.","title":"Context"},{"location":"architecture/#architectural-principles","text":"Name Simplicity First Statement If a functionality or Technology Component is not needed in the current iteration, it shouldn't be part of the architecture. There must be a need for it in the current iteration. Rationale Distraction is on of our worst enemies. Humans tend to get lost in the abundance of possibilities. By sticking to the Architectural Principle, clear focus on project delivery is ensured. Implications The majority of projects are over-engineered. This adds direct cost for additional unnecessary work and indirect maintenance cost due to architectural complexity. By enforcing the most simple architecture practitioners can focus on solving problems without getting distracted. Name OpenSource First Statement If a functionality or Technology Component is available as Open Source, this should be given preference. Rationale Open Source software tends to be more stable, better documented and better understood which pushed TCO down Implications When using Open Source vendor lock-in is reduced, experts are more abundant and standards are followed more tightly. In addition, independence from a vendor allows for development of extensions to address business needs which might not be available in timely manner from the vendor Name Homogeneity First Statement If a Technology Component is available from the same product suite it should be given preference. Rationale Although standards exist and a lot of products are compatible with each other, compatibility never reaches the level of products coming from the same product suite. Implications If one relies on standards assuming compatibility between Technology Components during Architectural Development and those are broken whole Solution Architectures or Technology Architectures can become completely or partially invalid. Improving homogeneity mitigates that risk. Name Never touch a running system Statement If a Technical Component is needed, it should be provisioned from scratch without reusing existing systems. Rationale When creating \"Systems of Innovation\" speed is on of the key drivers. Therefore, every interaction with existing system introduces complexity, delay and risk. Implications Especially in public, hybrid and private Cloud environments it is more effective to provision a Technical Component as a Service over adjusting an existing Technical Component. Name Data Locality First Statement Allocate compute and storage always as close as possible Rationale Data movement is more expensive than data storage. Bottlenecks slow systems down. Implications Data movement, especially between cloud providers or data centers are usually more expensive than allocating a compute note close to the data. Therefore, cost can be kept low and performance can be kept high","title":"Architectural Principles"},{"location":"architecture/#defining-the-architecture-incrementally","text":"The IBM Garage methodology guides the development team to adopt an incremental and iterative development practices, lean method. As part of the lean adoption, the implementation of an intelligent application integrates different concerns: microservice, data and AI model developments. But architecture is not forgotten, and it should be done also incrementally. Even if the target end to end solution will look similar to the reference architecture as illustrated in the diagram below, not all components need to be in place at the beginning of a Minimum Viable Product. Figure 1: The Data and AI Reference Architecture The goal of the architects will be, to select product for each of the needed capabilities following the guidances presented in the architectural principles above and build the architecture per increment and reuse previously define capabilities. Because it can be hard to initially define the architecture of a project, our method starts with the reference architecture and supports architectural changes while following the development of the process model. Figure 2: Process Model and Architectural Development Method This way application development and architectural development work hand in hand to produce a production ready and deployable product at the end of each iteration.","title":"Defining the architecture incrementally"},{"location":"architecture/#data-science-lightweight-architecture","text":"As an example, a Data Scientist's environment architecture, in the context of getting started, or for a MVP, will include the minimum components, as illustrated below: Figure 3: The Lightweight Reference Architecture but can evolve over time to address most of the components as described in the model building reference architecture.","title":"Data science lightweight architecture"},{"location":"architecture/#microservice-integration-lightweight-architecture","text":"For the intelligent application the first architecture may include microservice, serving Web channel, using the Backend for frontend pattern, and doing integration with other microservices. Each service has its own repository that can use different technologies like RDBMS, noSQL, filesystem, cache... A service can be consumer and publisher of events, and can integrate with pre-built AI model. Figure 4: Microservice MVP architecture The scoring service can be deployed as a service or integrated in an agent as part of a real time streaming analytics. The extended runtime architecture is presented in this article .","title":"Microservice integration lightweight architecture"},{"location":"architecture/#data-lightweight-architecture","text":"The same apply for the data architecture: the previous diagram illustrates the need to have data ingestion components responsible to extract, transform and load data from data source to storage area. In distributed system the storage need to support high availability and distribution intra and inter datacenters. When scaling the data adoption, architects need to work on the data topology , and extend to the reference architecture as presented in this article .","title":"Data lightweight architecture"},{"location":"architecture/architectural-decisions/","text":"Architectural Decisions Guidelines This way the method is highly iterative, so any findings during this process can result in changes to architectural decisions. However, there are never any wrong architectural decisions because the decisions take into account all the knowledge available at a certain point in time. Therefore, it\u2019s important to document why a decision was made. The following sections provide guidelines on selecting Technology Components for each Application Component. So it is explained what Technology Component should be chosen based on given requirements and if the Application Component needs to be included at all. Application Component: Data Source The data source is a private or public data source that includes relational databases; web pages; CSV, XML, JSON, or text files; and video and audio data. Technology Component mapping guidelines With the data source, there is not much to decide because in most cases, the type and structure of a data source is already defined and controlled by other stakeholders. However, if there is some control over the process, the following Architectural Principles should be considered: How does the delivery point look like? Enterprise data mostly lies in relational databases serving OLTP systems. It\u2019s typically a bad practice to access those systems directly, even in read-only mode because ETL processes are running SQL queries against those systems, which can hinder performance. One exception for example is IBM DB2 Workload Manager because it allows OLAP and ETL workloads to run in parallel with an OLTP workload without performance degradation of OLTP queries using intelligent scheduling and prioritizing mechanisms. Does real-time data need to be considered? Real-time data comes in various shapes and delivery methods. The most prominent include MQTT telemetry and sensor data (for example, data from the IBM Watson IoT Platform), an event stream, a simple REST HTTP endpoint that needs to be polled, or a TCP or UDP socket. If no downstream real-time processing is required, that data can be staged (for example, using Cloud Object Store). If downstream real-time processing is necessary, read the section on Streaming analytics further down in this document. Application Component: Enterprise data Cloud-based solutions tend to extend access of the enterprise data model. Therefore, it might be necessary to continuously transfer subsets of enterprise data to the cloud environment or access those in real time through a VPN API gateway. Technology Component mapping guidelines Moving enterprise data to the cloud can be costly. Therefore, it should be considered only if necessary. For example, if user data is handled in the cloud is it sufficient to store an anonymized primary key. If transfer of enterprise data to the cloud is unavoidable, privacy concerns and regulations must be addressed. Then, there are multiple ways to access it: Batch sync from an enterprise data center to the cloud Real-time access to subsets of data using VPN and an API gateway Expose data via an event backbone Technology Mapping Secure gateway Secure gateway lets cloud applications access specified hosts and ports in a private data center though an outbound connection. Therefore, no external inbound access is required. You can go to the Secure Gateway service on IBM Cloud for more information. Lift Lift allows you to migrate on-premises data to cloud databases in a very efficient manner. Read more about the IBM Lift CLI . Rocket Mainframe Data The Rocket Mainframe Data service uses similar functions for batch-style data integration as Lift, but is dedicated to IBM Mainframes. You can read more information about the service. Apache Kafka Apache Kafka, and IBM Event Streams expose data for other to consume with strong decoupling, and long term persistence. It can scale well for big data needs, and represent a well adopted platform to integrate microservices. See our deep dive into event driven solution here . Application Component: Streaming analytics The current state-of-the-art is batch processing. But, sometimes the value of a data product can be increased tremendously by adding real-time analytics capabilities because most of world\u2019s data loses value within seconds. Think of stock market data or the fact that a vehicle camera captures a pedestrian crossing a street. A streaming analytics system allows for real-time data processing. Think of it like running data against a continuous query instead of running a query against a finite data set. Technology Component mapping guidelines There is a relatively limited set of technologies for real-time stream processing. The most important questions to be asked are: What throughput is required? What latency is accepted? Which data types must be supported? What type of algorithms run on the system? Only relational algebra or advanced modeling? What\u2019s the variance of the workload and what are the elasticity requirements? What type of fault tolerance and delivery guarantees are necessary? Technology Mapping On IBM Cloud, there are many service offerings for real-time data processing that I explain in the following sections along with guidelines for when to use them. Apache Spark and Apache Spark Structured Streaming Apache Spark is often the primary choice when it comes to cluster-grade data processing and machine learning. If you\u2019re already using it for batch processing, Apache Spark Structured Streaming should be the first thing to evaluate. This way, you can have technology homogeneity and batch and streaming jobs can be run together (for example, joining a stream of records against a reference table). What throughput is required? Apache Spark Structured Streaming supports the same throughput as in batch mode. What latency is accepted? In Apache Spark v2.3, the Continuous Processing mode has been introduced, bringing latency down to one millisecond. Which data types must be supported? Apache Spark is strong at structured and semi-structured data. Audio and video data can\u2019t benefit from Apache Spark\u2019s accelerators Tungsten and Catalyst. What type of algorithms run on the system? Only relational algebra or advanced modeling? Apache Spark Structured Streaming supports relational queries as well as machine learning, but machine learning is only supported on sliding and tumbling windows. What\u2019s the variance of the workload and what are the elasticity requirements? Through their fault tolerant nature, Apache Spark clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? Apache Spark Structured Streaming supports exactly once delivery guarantees and depending on the type of data source, complete crash fault tolerance. IBM Streams IBM Streams is a fast streaming engine. Originally designed for low-latency, high throughput network monitoring applications, IBM Streams has its roots in cybersecurity. What throughput is required? IBM Streams can handle high data throughput rates, up to millions of events or messages per second. What latency is accepted? IBM Streams latency goes down to microseconds. Which data types must be supported? Through IBM Streams binary transfer mode, any type of data type can be supported. What type of algorithms run on the system? Only relational algebra or advanced modeling? IBM Streams in its core supports all relational algebra. Also, through toolkits, various machine learning algorithms can be used. Toolkits are an open system, and there are many third-party toolkits. What\u2019s the variance of the workload and what are the elasticity requirements? Through its fault tolerant nature, IBM Streams clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? IBM Streams supports exactly once delivery guarantees and complete crash fault tolerance. Node-RED Node-RED is a lightweight streaming engine. Implemented on top of Node.js in JavaScript, it can even run on a 64 MB memory footprint (for example, running on a Raspberry PI). What throughput is required? Node-RED\u2019s throughput is bound to processing capabilities of a single CPU core, through Node.js\u2019s event processing nature. For increased throughput, multiple instances of Node-RED have been used in parallel. Parallelization is not built in and needs to be provided by the application developer. What latency is accepted? Latency is also dependent on the CPU configuration and on the throughput because high throughput congests the event queue and increases latency. Which data types must be supported? Node-RED best supports JSON streams, although any data type can be nested into JSON. What type of algorithms run on the system? Only relational algebra or advanced modeling? Node-RED has one of the most extensive ecosystems of open source third-party modules. Although advanced machine learning is not supported natively. What\u2019s the variance of the workload and what are the elasticity requirements? Because parallelization is a responsibility of the application developer, for independent computation a round-robin load balancing scheme supports linear scalability and full elasticity. What type of fault tolerance and delivery guarantees are necessary? NodeRED has no built-in fault tolerance and no delivery guarantees. Apache Nifi Apache Nifi is maintained by Hortonworks and is part of the IBM Analytics Engine Service. What throughput is required? Nifi can handle hundreds of MBs on a single node and can be configured to handle multiple GBs in cluster mode. What latency is accepted? Nifi\u2019s latency is in seconds. Through message periodization, the tradeoff between throughput and latency can be tweaked. Which data types must be supported? Nifi best supports structured data streams, although any data type can be nested. What type of algorithms run on the system? Only relational algebra or advanced modeling? Nifi supports relational algebra out of the box, but custom processors can be built. What\u2019s the variance of the workload and what are the elasticity requirements? Nifi can be easily scaled up without restarts, but scaling down requires stopping and starting the Nifi system. What type of fault tolerance and delivery guarantees are necessary? Nifi supports end-to-end guaranteed exactly once delivery. Also, fault tolerance can be configured, but automatic recovery is not possible. Another important feature is backpressure and pressure release, which causes the upstream nodes to stop accepting new data and discarding unprocessed data if an age threshold is exceeded. Apache Kafka Others There are also other technologies like Apache Samza, Apache Flink, Apache Storm, Total.js Flow, Eclipse Kura, and Flogo that might be worth looking at if the ones mentioned don\u2019t meet all of your requirements. Application Component: Data Integration In the data integration stage, data is cleansed, transformed, and if possible, downstream features are added Technology Component mapping guidelines There are numerous technologies for batch data processing, which is the technology used for data integration. The most important questions to be asked are: What throughput is required? Which data types must be supported? What source systems must be supported? What skills are required? Technology Guidelines IBM Cloud has many service offerings for data integration, and the following section explains them and gives guidelines on which one to use. Apache Spark Apache Spark is often the first choice when it comes to cluster-grade data processing and machine learning. Apache Spark is a flexible option that also supports writing integration processes in SQL. But it\u2019s missing a user interface. What throughput is required? Apache Spark scales linearly, so throughput is just a function of the cluster size. Which data types must be supported? Apache Spark works best with structured data, but binary data is supported as well. What source systems must be supported? Apache Spark can access various SQL and NoSQL data as well as file sources. A common data source architecture allows adding capabilities, and it has third-party project functions as well. What skills are required? Advanced SQL skills are required and you should have some familiarity with either Java programming, Scala, or Python. IBM Data Stage on Cloud IBM Data Stage is a sophisticated ETL (Extract Transform Load) tool. Its closed source and supports visual editing. What throughput is required? Data Stage can be used in cluster mode, which supports scale-out. Which data types must be supported? Data Stage has its roots in traditional Data Warehouse ETL and concentrates on structured data. What source systems must be supported? Again, Data Stage concentrates on relational database systems, but files can also be read, even on Object Store. In addition, data sources can be added using plug-ins that are implemented in the Java language. What skills are required? Because Data Stage is a visual editing environment, the learning curve is low. No programming skills are required. Others It\u2019s important to know that data integration is mostly done using ETL tools, plain SQL, or a combination of both. ETL tools are mature technology, and many ETL tools exist. On the other hand, if streaming analytics is part of the project, it\u2019s a good idea to check whether one of those technologies fits your requirements because reuse of such a system reduces technology heterogeneity. Application Component: Data Repository This is the persistent storage for your data. Technology Component mapping guidelines There are lots of technologies for persisting data, and most of them are relational databases. The second largest group are NoSQL databases, with file systems (including Cloud Object Store) forming the last one. The most important questions to be asked are: What is the impact of storage cost? Which data types must be supported? How well must point queries (on fixed or dynamic dimensions) be supported? How well must range queries (on fixed or dynamic dimensions) be supported? How well must full table scans be supported? What skills are required? What\u2019s the requirement for fault tolerance and backup? What are the constant and peak ingestion rates? What amount of storage is needed? What does the growth pattern look like? What are the retention policies? Technology Mapping IBM cloud has numerous service offerings for SQL, NoSQL, and file storage. The following section explains them and provides guidelines on when to use which one. Relational databases Dash DB is the Db2 BLU on the Cloud offering from IBM that features column store, advanced compression, and execution on SIMD instructions sets (that is, vectorized processing). But there are other options in IBM Cloud such as Informix, PostgreSQL, and MySQL. What is the impact of storage cost? Relational databases (RDBMS) have the highest requirements on storage quality. Therefore, the cost of relational storage is always the highest. Which data types must be supported? Relational databases are meant for structured data. Although there are column data types for binary data that can be swapped for cheaper storage, this is just an add-on and not a core function of relational databases. How well must point queries (on fixed or dynamic dimensions) be supported? RDBMS are great for point queries because an index can be created on each column. How well must range queries (on fixed or dynamic dimensions) be supported? RDBMS are great for range queries because an index can be created on each column. How well must full table scans be supported? RDBMS are trying to avoid full table scans in their SQL query optimizers. Therefore, performance is not optimized for full table scans (for example, contaminating page caches). What skills are required? You should have SQL skills, and if a cloud offering isn\u2019t chosen, you should also have database administrator (DBA) skills for the specific database. What\u2019s the requirement for fault tolerance and backup? RDMBS support continuous backup and crash fault tolerance. For recovery, the system might need to go offline. What are the constant and peak ingestion rates? Inserts using SQL are relatively slow, especially if the target table contains many indexes that must be rebalanced and updated. Some RDBMS support bulk inserts from files by bypassing the SQL engine, but then the table usually needs to go offline for that period. What amount of storage is needed? RDMBS perform very well to around 1 TB of data. Going beyond that is complex and needs advanced cluster setups. What does the growth pattern look like? RDBMS support volume management, so continuous growth usually isn\u2019t a problem, even at run time. For shrinking, the system might need to be taken offline. What are the retention policies? RDBMS usually support automated retention mechanisms to delete old data automatically. NoSQL databases The most prominent NoSQL databases like Apache CouchDB, MongoDB, Redis, RethinkDB, ScyllaDB (Cassandra), and InfluxCloud are supported. What is the impact of storage cost? NoSQL databases are usually storage fault-tolerant, by default. Therefore, quality requirements on storage are less, which brings down storage cost. Which data types must be supported? Although NoSQL databases are meant for structured data as well, they usually use JSON as a storage format, which can be enriched with binary data. Although, a lot of binary data attached to a JSON document can bring the performance down as well. How well must point queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves point query performance. How well must range queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves range query performance. How well must full table scans be supported? NoSQL databases perform very well at full table scans. The performance is only limited by the I/O bandwidth to storage. What skills are required? Typically, special query language skills are required for the application developer and if a cloud offering isn\u2019t chosen, database administrator (DBA) skills are needed for the specific database. What\u2019s the requirement for fault tolerance and backup? NoSQL databases support backups in different ways. But some aren\u2019t supporting online backup. NoSQL databases are usually crash fault tolerant, but for recovery, the system might need to go offline. What are the constant and peak ingestion rates? Usually, no indexes need to be updated and data doesn\u2019t need to be mapped to pages. Ingestion rates are usually only bound to I/O performance of the storage system. What amount of storage is needed? RDMBS perform well to approximately 10 \u2013 100 TB of data. Cluster setups on NoSQL databases are much more straightforward than on RDBMS. Successful setups with >100 nodes and > 100.000 database reads and writes per second have been reported. What does the growth pattern look like? The growth of NoSQL databases is not a problem. Volumes can be added at run time. For shrinking, the system might need to be taken offline. What are the retention policies? NoSQL databases don\u2019t support automated retention mechanisms to delete old data automatically. Therefore, this must be implemented manually, resulting in range queries on the data corpus. Object storage Cloud object storage makes it possible to store practically limitless amounts of data. It is commonly used for data archiving and backup, for web and mobile applications, and as scalable, persistent storage for analytics. So, let\u2019s take a look. What is the impact of storage cost? Object storage is the cheapest option for storage. Which data types must be supported? Because object storage resembles a file system, any data type is supported. How well must point queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. How well must range queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. Therefore, range queries on a single defined column (for example, data) can be achieved through hierarchical folder structures. How well must full table scans be supported? Full table scans are bound only by the I/O bandwidth of the object storage. What skills are required? On a file level, working with object storage is much like working with any file system. Through Apache SparkSQL and IBM Cloud SQL Query, data in Object storage can be accessed with SQL. Because object storage is a cloud offering, no administrator skills are required. IBM Object Storage is available for on-premises as well using an appliance box. What\u2019s the requirement for fault tolerance and backup? Fault tolerance and backup is completely handled by the cloud provider. Object storage supports intercontinental data center replication for high-availability out of the box. What are the constant and peak ingestion rates? Ingestion rates to object storage is bound by the uplink speed to the object storage system. What\u2019s the amount of storage needed? Object storage scales to the petabyte range. What does the growth pattern look like? Growth and shrinking on object storage is fully elastic. What are the retention policies? Retention of data residing in object storage must be done manually. Hierarchical file and folder layout that is based on data and time helps here. Some object storage support automatic movement of infrequently accessed files to colder storage (colder means less cost, but also less performance, or even higher cost of accesses to files). Application Component: Discovery and exploration This component allows for visualization and creation of metrics of data. Technology Component mapping guidelines In various process models, data visualization and exploration is one of the first steps. Similar tasks are also applied in traditional data warehousing and business intelligence. So when choosing a technology, ask the following questions: What type of visualizations are needed? Are interactive visualizations needed? Are coding skills available or required? What metrics can be calculated on the data? Do metrics and visualization need to be shared with business stakeholders? Technology Mapping IBM cloud has many service offerings for data exploration. Some of the offerings are open source, and some aren\u2019t. Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust are all open source and supported in IBM Cloud. Some of these components have overlapping features and some of them have complementary features. This can be determined by answering the architectural questions. What type of visualizations are needed? Matplotlib supports the widest range of possible visualizations including run chars, histograms, box-plots, and scatter plots. PixieDust (as of V1.1.11) supports tables, bar charts, line charts, scatter plots, pie charts, histograms, and maps. Are interactive visualizations needed? Matplotlib creates static plots and PixieDust supports interactive ones. Are coding skills available or required? Matplotlib requires coding skills, but PixieDust does not. For computing metrics, some code is necessary. What metrics can be calculated on the data? Using scikit-learn and pandas, all state-of-the-art metrics are supported. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of Jupyter Notebooks, also using a fine-grained user and access management system. SPSS Modeler SPSS Modeler is available in the cloud and also as stand-alone product. What type of visualizations are needed? SPSS Modeler supports the following visualizations out of the box: Bar Pie 3D Bar 3D Pie Line Area 3D Area Path Ribbon Surface Scatter Bubble Histogram Box Map You can get more information in the IBM Knowledge Center . Are interactive visualizations needed? SPSS Modeler Visualizations are not interactive. Are coding skills available or required? SPSS Modeler doesn\u2019t require any coding skills. What metrics can be calculated on the data? All state-of-the-art metrics are supported using the Data Audit node. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of SPSS Modeler Flows, also using a fine-grained user and access management system. However, those might not be suitable to stakeholders. Application Component: Actionable insights This is where most of your work fits in. It\u2019s where you create and evaluate your machine learning and deep learning models. Technology Component mapping guidelines There are numerous technologies for creating and evaluating machine learning and deep learning models. Although different technologies differ in function and performance, those differences are usually miniscule. Therefore, the questions you should ask yourself are: What are the available skills regarding programming languages? What are the costs of skills regarding programming languages? What are the available skills regarding frameworks? What are the costs of skills regarding frameworks? Is model interchange required? Is parallel- or GPU-based training or scoring required? Do algorithms need to be tweaked or new algorithms be developed? Technology Mapping Because there\u2019s an abundance of open and closed source technologies, we are highlighting the most relevant ones in this article. Although it\u2019s the same for the other sections as well, decisions made in this section are very prone to change due to the iterative nature of this process model. Therefore, changing or combining multiple technologies is no problem, although the decisions that led to those changes should be explained and documented. SPSS Modeler This article has already introduced SPSS Modeler. What are the available skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the costs of skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the available skills regarding frameworks? SPSS is an industry leader, so skills are generally available. What are the costs of skills regarding frameworks? Expert costs are usually lower in UI-based tools than in programming frameworks. Is model interchange required? SPSS Modeler supports PMML. Is parallel- or GPU-based training or scoring required? SPSS Modeler supports scaling through IBM Analytics Server or IBM Watson Studio using Apache Spark. Do algorithms need to be tweaked or new algorithms be developed? SPSS Modeler algorithms can\u2019t be changed, but you can add algorithms or customizations using the R language. R/R-Studio R and R-Studio are standards for open source-based data science. They\u2019re supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? R programming skills are usually widely available because it\u2019s a standard programming language in many natural science-based university curriculums. It can be acquired rapidly because it is a procedural language with limited functional programming support. What are the costs of skills regarding programming languages? Costs of R programming are usually low. What are the available skills regarding frameworks? R is not only a programming language but also requires knowledge of tooling (R-Studio), and especially knowledge of the R library (CRAN) with 6000+ packages. What are the costs of skills regarding frameworks? Expert costs are correlated with knowledge of the CRAN library and years of experience and in the range of usual programmer costs. Is model interchange required? Some R libraries support exchange of models, but it is not standardized. Is parallel- or GPU-based training or scoring required? Some R libraries support scaling and GPU acceleration, but it is not standardized. Do algorithms need to be tweaked or new algorithms be developed? R needs algorithms to be implemented in C/C++ to run fast. So, tweaking and custom development usually involves C/C++ coding. Python, pandas and scikit-learn Although R and R-Studio have been the standard for open source-based data science for a while, Python, pandas, and scikit-learn are right behind them. Python is a much cleaner programming language than R and easier to learn. Pandas is the Python equivalent to R data frames, supporting relational access to data. Finally, scikit-learn nicely groups all necessary machine learning algorithms together. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Python skills are very widely available because Python is a clean and easy to learn programming language. What are the costs of skills regarding programming languages? Because of Python\u2019s properties mentioned above, the cost of Python programming skills is very low. What are the available skills regarding frameworks? Pandas and scikit-learn are very clean and easy-to-learn frameworks. Therefore, skills are widely available. What are the costs of skills regarding frameworks? Because of the properties mentioned above, the costs of skills are very low. Is model interchange required? All scikit-learn models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? Neither GPU nor scale-out is supported, although scale-up capabilities can be added individually to make use of multiple cores. Do algorithms need to be tweaked or new algorithms be developed? scikit-learn algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with the numpy library. Therefore, tweaking and adding algorithms is straightforward. Python, Apache Spark and SparkML Although Python, pandas, and scikit-learn are more widely adopted, the Apache Spark ecosystem is catching up, especially because of its scaling capabilities. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Apache Spark supports Python, Java programming, Scala, and R as programming languages. What are the costs of skills regarding programming languages? The costs depend on what programming language is used, with Python typically the cheapest. What are the available skills regarding frameworks? Apache Spark skills are in high demand and usually not available. What are the costs of skills regarding frameworks? Apache Spark skills are in high demand and are usually expensive. Is model interchange required? All SparkML models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. However, GPU\u2019s are only supported through third-party libraries. Do algorithms need to be tweaked or new algorithms be developed? As in scikit-learn, algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with built-in Apache Spark libraries. Therefore, tweaking and adding algorithms is straightforward. Apache SystemML When it comes to relational data processing, SQL is a leader, mainly because an optimizer takes care of optimal query executions. Think of SystemML as an optimizer for linear algebra that\u2019s capable of creating optimal execution plans for jobs running on data parallel frameworks like Apache Spark. What are the available skills regarding programming languages? SystemML has two domain-specific languages (DSL) with R and Python syntax. What are the costs of skills regarding programming languages? Although the DSLs are like R and Python, there is a learning curve involved. What are the available skills regarding frameworks? SystemML skills are very rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? SystemML models can be (de)serialized. PMML is not supported. SystemML can import and run Caffe2 and Keras models. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. SystemML uses this property. In addition, GPU\u2019s are supported as well. Do algorithms need to be tweaked or new algorithms be developed? Although SystemML comes with a large set of pre-implemented algorithms for machine learning and deep learning, its strengths are in tweaking existing algorithms or implementing new ones because the DSL allows for concentrating on the mathematical implementation of the algorithm. The rest is handled by the framework. This makes it an ideal choice for these kind of tasks. Keras and TensorFlow TensorFlow is one of the most widely used deep learning frameworks. At its core, it is a linear algebra library supporting automatic differentiation. TensorFlow\u2019s Python-driven syntax is relatively complex. Therefore, Keras provides an abstraction layer on top of TensorFlow. Both frameworks are seamlessly supported in IBM Cloud through Watson Studio and Watson Machine Learning. What are the available skills regarding programming languages? Python is the core programming language for Keras and TensorFlow. What are the available skills regarding frameworks? Keras and TensorFlow skills are relatively rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? Keras and TensorFlow have their own model exchange formats. There are converters from and to ONNX. Is parallel- or GPU-based training or sc oring required? Running TensorFlow on top of ApacheSpark is supported through TensorFrames and TensorSpark. Keras models can be run on ApacheSpark using DeepLearning4J and SystemML. Both of the latter frameworks also support GPUs. TensorFlow (and therefore, Keras) support GPU natively as well. Do algorithms need to be tweaked or new algorithms be developed? TensorFlow is a linear algebra execution engine. Therefore, it\u2019s optimally suited for tweaking and creating new algorithms. Keras is a very flexible deep learning library that supports many neural network layouts. Application Component: Applications and Data Products Models are fine, but their value rises when they can be consumed by the ordinary business user. Therefore, you must create a data product. Data products don\u2019t necessarily need to stay on the cloud. They can be pushed to mobile or enterprise applications. In contrast to machine learning and deep learning frameworks, the space of frameworks to create data product is tiny. This might reflect what the current state-of-the-art technology in data science concentrates on. Depending on the requirements, data products are relatively visualization-centric after a lot of user input data has been gathered. They also might involve asynchronous workflows as batch data integration and model training and scoring is performed within the workflow. Questions to ask about the technology are: What skills are present for developing a data product? What skills are necessary for developing a data product? Is instant feedback required or is batch processing accepted? What\u2019s the degree of customization needed? What\u2019s the target audience? Is cloud scale deployment for a public use base required? Technology Mapping Currently, only a limited set of frameworks and technologies is available in different categories. In the following section, I\u2019ve explained the most prominent examples. R-Shiny R-Shiny is a great framework for building data products. Closely tied to the R language, it enables data scientists to rapidly create a UI on top of existing R-scripts. What skills are present for developing a data product? R-Shiny requires R development skills. So, it best fits into an R development ecosystem. What skills are necessary for developing a data product? Although based on R, R-Shiny needs additional skills. For experienced R developers, the learning curve is steep and additional knowledge to acquire is minimal. Is instant feedback required or is batch processing accepted? The messaging model of R-Shiny supports instant UI feedback when server-side data structures are updated. Therefore, the response time is independent of the framework and should be considered and resolved programmatically on the server side. What\u2019s the degree of customization needed? Although R-Shiny is an extensible framework, extending it requires a deep understanding of the framework and R-Technology. Out of the box, there is a large set of UI widgets end elements supported, allowing for very customizable applications. If requirements go beyond those capabilities, costly extensions are required. What\u2019s the target audience? Is cloud scale deployment for a public use base required? R-Shiny applications look very professional, although quite distinguishable. Therefore, the target audience must accept the UI design limitations. R-Shiny is best dynamically scaled horizontally in a container environment like Kubernetes. Ideally, every user session runs in its own container because R and R-Shiny are very sensitive to main memory shortages Node-RED Although Node-RED is a no-code/low-code data flow/data integration environment, because of its modular nature it supports various extensions including the dash boarding extension. This extension allows for fast creation of user interfaces including advanced visualizations that are updated in real-time. What skills are present for developing a data product? Due to the completely graphical user interface-based software development approach, only basic programming skills are required to build data products with Node-RED. What skills are necessary for developing a data product? Any resource familiar with flow-based programming as used in many state-of-the-art ETL and data mining tools will have a fast start with Node-RED. Basic JavaScript knowledge is required for creating advanced flows and for extending the framework. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? Node-RED is a Node.js/JavaScript-based framework. Custom UI widgets require advanced Node.js development skills. What\u2019s the target audience? Is cloud scale deployment for a public use base required? The Node-RED dashboard can be deployed for a pubic user base as long as the limitations regarding UI customization are acceptable. Because Node.js runs on a single threaded event loop, scaling must be done horizontally, preferably using a containerized environment. Note: The Internet of Things Starter kit in IBM Cloud supports horizontal scaling out of the box. D3 When it comes to custom application development, D3 is one of the most prominent and most widely used visualization widget frameworks with a large open source ecosystem contributing a lot of widgets for every desirable use case. There\u2019s a good chance that you can find a D3 widget to fit your use case. What skills are present for developing a data product? D3 fits best into an existing, preferably JavaScript-based developer ecosystem, although JavaScript is only required on the client side. Therefore, on the server side, any REST-based endpoints in any programming language are supported. One example is REST endpoints accessed by a D3 UI provided by Apache Livy that encapsulates Apache Spark jobs. What skills are necessary for developing a data product? D3 requires sophisticated D3 skills and at least client-side JavaScript skills. Skills in a JavaScript AJAX framework like AngularJS are highly recommended. On the server side, capabilities of providing REST endpoints to the D3 applications are required. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? D3 applications usually are implemented from scratch. Therefore, this solution provides the most flexibility to the end user. What\u2019s the target audience? Is cloud scale deployment for a public use base required? As a cloud native application, a D3-based data product can provide all capabilities for horizontal and vertical scaling and full adoption to user requirements. Application Component: Security, information governance and systems management This important step can be easily forgotten. It\u2019s important to control who has access to which information for many compliance regulations. In addition, modern data science architectures involve many components that require operational aspects as well. Data privacy is a major challenge in many data science projects. Questions that you should ask are: What granularity is required for managing user access to data assets? Are existing user registries required to be integrated? Who is taking care of operational aspects? What are the requirements for data retention? What level of security against attacks from hackers is required? Technology Mapping Again, there\u2019s a lot of software for this as well as ways to solve the requirements involved in this topic. We have chosen representative examples for this article. Deploying a productive client-facing web application brings with it serious risks. IBM Cloud Internet Services provides global points of presence (PoPs). It includes domain name service (DNS), global load balancer (GLB), distributed denial of service (DDoS) protection, web application firewall (WAF), transport layer security (TLS), and caching. What level of security against attacks from hackers is required? Internet Services is using services from CloudFlare, the world leader in this space. IBM App ID Identity Management allows for cloud-based user and identity management for web and mobile applications, APIs, and back-end systems. Cloud users can sign up and sign in with App ID\u2019s scalable user registry or social login with Google or Facebook. Enterprise users can be integrated using SAML 2.0 federation. What granularity is required for managing user access to data assets? IBM App ID supports user management but no group/roles. Therefore, fine-grained access must be managed within the application. Are existing user registries required to be integrated? IBM App ID supports registry integration using SAML 2.0 federation. Object Storage Object Storage is a standard when it comes to modern, cost-effective cloud storage. What granularity is required for managing user access to data assets? IBM Identity and Access Management (IAM) integration allows for granular access control at the bucket-level using role-based policies. What are the requirements for data retention? Object storage supports different storage classes for frequently accessed data, occasionally accessed data, and long-term data retention with standard, vault, and cold vault. The Flex class allows for dynamic data access and automates this process. Physical deletion of data still must be triggered externally. Who is taking care of operational aspects? Regional and cross-region resiliency options allow for increased data reliability. What level of security against attacks from hackers is required? All data is encrypted at rest and in flight by default. Keys are automatically managed by default, but optionally can be self-managed or managed using IBM Key Protect. IBM Cloud PaaS/SaaS IBM Cloud PaaS/SaaS eliminates operational aspects from data science projects because all components involved are managed by IBM. Who is taking care of operational aspects? In PaaS/SaaS, cloud operational aspects are being taken care of by the cloud provider.","title":"Architectural Decision guidelines"},{"location":"architecture/architectural-decisions/#architectural-decisions-guidelines","text":"This way the method is highly iterative, so any findings during this process can result in changes to architectural decisions. However, there are never any wrong architectural decisions because the decisions take into account all the knowledge available at a certain point in time. Therefore, it\u2019s important to document why a decision was made. The following sections provide guidelines on selecting Technology Components for each Application Component. So it is explained what Technology Component should be chosen based on given requirements and if the Application Component needs to be included at all.","title":"Architectural Decisions Guidelines"},{"location":"architecture/architectural-decisions/#application-component-data-source","text":"The data source is a private or public data source that includes relational databases; web pages; CSV, XML, JSON, or text files; and video and audio data.","title":"Application Component: Data Source"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines","text":"With the data source, there is not much to decide because in most cases, the type and structure of a data source is already defined and controlled by other stakeholders. However, if there is some control over the process, the following Architectural Principles should be considered: How does the delivery point look like? Enterprise data mostly lies in relational databases serving OLTP systems. It\u2019s typically a bad practice to access those systems directly, even in read-only mode because ETL processes are running SQL queries against those systems, which can hinder performance. One exception for example is IBM DB2 Workload Manager because it allows OLAP and ETL workloads to run in parallel with an OLTP workload without performance degradation of OLTP queries using intelligent scheduling and prioritizing mechanisms. Does real-time data need to be considered? Real-time data comes in various shapes and delivery methods. The most prominent include MQTT telemetry and sensor data (for example, data from the IBM Watson IoT Platform), an event stream, a simple REST HTTP endpoint that needs to be polled, or a TCP or UDP socket. If no downstream real-time processing is required, that data can be staged (for example, using Cloud Object Store). If downstream real-time processing is necessary, read the section on Streaming analytics further down in this document.","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#application-component-enterprise-data","text":"Cloud-based solutions tend to extend access of the enterprise data model. Therefore, it might be necessary to continuously transfer subsets of enterprise data to the cloud environment or access those in real time through a VPN API gateway.","title":"Application Component: Enterprise data"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_1","text":"Moving enterprise data to the cloud can be costly. Therefore, it should be considered only if necessary. For example, if user data is handled in the cloud is it sufficient to store an anonymized primary key. If transfer of enterprise data to the cloud is unavoidable, privacy concerns and regulations must be addressed. Then, there are multiple ways to access it: Batch sync from an enterprise data center to the cloud Real-time access to subsets of data using VPN and an API gateway Expose data via an event backbone","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-mapping","text":"","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#secure-gateway","text":"Secure gateway lets cloud applications access specified hosts and ports in a private data center though an outbound connection. Therefore, no external inbound access is required. You can go to the Secure Gateway service on IBM Cloud for more information.","title":"Secure gateway"},{"location":"architecture/architectural-decisions/#lift","text":"Lift allows you to migrate on-premises data to cloud databases in a very efficient manner. Read more about the IBM Lift CLI .","title":"Lift"},{"location":"architecture/architectural-decisions/#rocket-mainframe-data","text":"The Rocket Mainframe Data service uses similar functions for batch-style data integration as Lift, but is dedicated to IBM Mainframes. You can read more information about the service.","title":"Rocket Mainframe Data"},{"location":"architecture/architectural-decisions/#apache-kafka","text":"Apache Kafka, and IBM Event Streams expose data for other to consume with strong decoupling, and long term persistence. It can scale well for big data needs, and represent a well adopted platform to integrate microservices. See our deep dive into event driven solution here .","title":"Apache Kafka"},{"location":"architecture/architectural-decisions/#application-component-streaming-analytics","text":"The current state-of-the-art is batch processing. But, sometimes the value of a data product can be increased tremendously by adding real-time analytics capabilities because most of world\u2019s data loses value within seconds. Think of stock market data or the fact that a vehicle camera captures a pedestrian crossing a street. A streaming analytics system allows for real-time data processing. Think of it like running data against a continuous query instead of running a query against a finite data set.","title":"Application Component: Streaming analytics"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_2","text":"There is a relatively limited set of technologies for real-time stream processing. The most important questions to be asked are: What throughput is required? What latency is accepted? Which data types must be supported? What type of algorithms run on the system? Only relational algebra or advanced modeling? What\u2019s the variance of the workload and what are the elasticity requirements? What type of fault tolerance and delivery guarantees are necessary?","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-mapping_1","text":"On IBM Cloud, there are many service offerings for real-time data processing that I explain in the following sections along with guidelines for when to use them.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#apache-spark-and-apache-spark-structured-streaming","text":"Apache Spark is often the primary choice when it comes to cluster-grade data processing and machine learning. If you\u2019re already using it for batch processing, Apache Spark Structured Streaming should be the first thing to evaluate. This way, you can have technology homogeneity and batch and streaming jobs can be run together (for example, joining a stream of records against a reference table). What throughput is required? Apache Spark Structured Streaming supports the same throughput as in batch mode. What latency is accepted? In Apache Spark v2.3, the Continuous Processing mode has been introduced, bringing latency down to one millisecond. Which data types must be supported? Apache Spark is strong at structured and semi-structured data. Audio and video data can\u2019t benefit from Apache Spark\u2019s accelerators Tungsten and Catalyst. What type of algorithms run on the system? Only relational algebra or advanced modeling? Apache Spark Structured Streaming supports relational queries as well as machine learning, but machine learning is only supported on sliding and tumbling windows. What\u2019s the variance of the workload and what are the elasticity requirements? Through their fault tolerant nature, Apache Spark clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? Apache Spark Structured Streaming supports exactly once delivery guarantees and depending on the type of data source, complete crash fault tolerance.","title":"Apache Spark and Apache Spark Structured Streaming"},{"location":"architecture/architectural-decisions/#ibm-streams","text":"IBM Streams is a fast streaming engine. Originally designed for low-latency, high throughput network monitoring applications, IBM Streams has its roots in cybersecurity. What throughput is required? IBM Streams can handle high data throughput rates, up to millions of events or messages per second. What latency is accepted? IBM Streams latency goes down to microseconds. Which data types must be supported? Through IBM Streams binary transfer mode, any type of data type can be supported. What type of algorithms run on the system? Only relational algebra or advanced modeling? IBM Streams in its core supports all relational algebra. Also, through toolkits, various machine learning algorithms can be used. Toolkits are an open system, and there are many third-party toolkits. What\u2019s the variance of the workload and what are the elasticity requirements? Through its fault tolerant nature, IBM Streams clusters can be grown and shrunk dynamically. What type of fault tolerance and delivery guarantees are necessary? IBM Streams supports exactly once delivery guarantees and complete crash fault tolerance.","title":"IBM Streams"},{"location":"architecture/architectural-decisions/#node-red","text":"Node-RED is a lightweight streaming engine. Implemented on top of Node.js in JavaScript, it can even run on a 64 MB memory footprint (for example, running on a Raspberry PI). What throughput is required? Node-RED\u2019s throughput is bound to processing capabilities of a single CPU core, through Node.js\u2019s event processing nature. For increased throughput, multiple instances of Node-RED have been used in parallel. Parallelization is not built in and needs to be provided by the application developer. What latency is accepted? Latency is also dependent on the CPU configuration and on the throughput because high throughput congests the event queue and increases latency. Which data types must be supported? Node-RED best supports JSON streams, although any data type can be nested into JSON. What type of algorithms run on the system? Only relational algebra or advanced modeling? Node-RED has one of the most extensive ecosystems of open source third-party modules. Although advanced machine learning is not supported natively. What\u2019s the variance of the workload and what are the elasticity requirements? Because parallelization is a responsibility of the application developer, for independent computation a round-robin load balancing scheme supports linear scalability and full elasticity. What type of fault tolerance and delivery guarantees are necessary? NodeRED has no built-in fault tolerance and no delivery guarantees.","title":"Node-RED"},{"location":"architecture/architectural-decisions/#apache-nifi","text":"Apache Nifi is maintained by Hortonworks and is part of the IBM Analytics Engine Service. What throughput is required? Nifi can handle hundreds of MBs on a single node and can be configured to handle multiple GBs in cluster mode. What latency is accepted? Nifi\u2019s latency is in seconds. Through message periodization, the tradeoff between throughput and latency can be tweaked. Which data types must be supported? Nifi best supports structured data streams, although any data type can be nested. What type of algorithms run on the system? Only relational algebra or advanced modeling? Nifi supports relational algebra out of the box, but custom processors can be built. What\u2019s the variance of the workload and what are the elasticity requirements? Nifi can be easily scaled up without restarts, but scaling down requires stopping and starting the Nifi system. What type of fault tolerance and delivery guarantees are necessary? Nifi supports end-to-end guaranteed exactly once delivery. Also, fault tolerance can be configured, but automatic recovery is not possible. Another important feature is backpressure and pressure release, which causes the upstream nodes to stop accepting new data and discarding unprocessed data if an age threshold is exceeded.","title":"Apache Nifi"},{"location":"architecture/architectural-decisions/#apache-kafka_1","text":"","title":"Apache Kafka"},{"location":"architecture/architectural-decisions/#others","text":"There are also other technologies like Apache Samza, Apache Flink, Apache Storm, Total.js Flow, Eclipse Kura, and Flogo that might be worth looking at if the ones mentioned don\u2019t meet all of your requirements.","title":"Others"},{"location":"architecture/architectural-decisions/#application-component-data-integration","text":"In the data integration stage, data is cleansed, transformed, and if possible, downstream features are added","title":"Application Component: Data Integration"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_3","text":"There are numerous technologies for batch data processing, which is the technology used for data integration. The most important questions to be asked are: What throughput is required? Which data types must be supported? What source systems must be supported? What skills are required?","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-guidelines","text":"IBM Cloud has many service offerings for data integration, and the following section explains them and gives guidelines on which one to use.","title":"Technology Guidelines"},{"location":"architecture/architectural-decisions/#apache-spark","text":"Apache Spark is often the first choice when it comes to cluster-grade data processing and machine learning. Apache Spark is a flexible option that also supports writing integration processes in SQL. But it\u2019s missing a user interface. What throughput is required? Apache Spark scales linearly, so throughput is just a function of the cluster size. Which data types must be supported? Apache Spark works best with structured data, but binary data is supported as well. What source systems must be supported? Apache Spark can access various SQL and NoSQL data as well as file sources. A common data source architecture allows adding capabilities, and it has third-party project functions as well. What skills are required? Advanced SQL skills are required and you should have some familiarity with either Java programming, Scala, or Python.","title":"Apache Spark"},{"location":"architecture/architectural-decisions/#ibm-data-stage-on-cloud","text":"IBM Data Stage is a sophisticated ETL (Extract Transform Load) tool. Its closed source and supports visual editing. What throughput is required? Data Stage can be used in cluster mode, which supports scale-out. Which data types must be supported? Data Stage has its roots in traditional Data Warehouse ETL and concentrates on structured data. What source systems must be supported? Again, Data Stage concentrates on relational database systems, but files can also be read, even on Object Store. In addition, data sources can be added using plug-ins that are implemented in the Java language. What skills are required? Because Data Stage is a visual editing environment, the learning curve is low. No programming skills are required.","title":"IBM Data Stage on Cloud"},{"location":"architecture/architectural-decisions/#others_1","text":"It\u2019s important to know that data integration is mostly done using ETL tools, plain SQL, or a combination of both. ETL tools are mature technology, and many ETL tools exist. On the other hand, if streaming analytics is part of the project, it\u2019s a good idea to check whether one of those technologies fits your requirements because reuse of such a system reduces technology heterogeneity.","title":"Others"},{"location":"architecture/architectural-decisions/#application-component-data-repository","text":"This is the persistent storage for your data.","title":"Application Component: Data Repository"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_4","text":"There are lots of technologies for persisting data, and most of them are relational databases. The second largest group are NoSQL databases, with file systems (including Cloud Object Store) forming the last one. The most important questions to be asked are: What is the impact of storage cost? Which data types must be supported? How well must point queries (on fixed or dynamic dimensions) be supported? How well must range queries (on fixed or dynamic dimensions) be supported? How well must full table scans be supported? What skills are required? What\u2019s the requirement for fault tolerance and backup? What are the constant and peak ingestion rates? What amount of storage is needed? What does the growth pattern look like? What are the retention policies?","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-mapping_2","text":"IBM cloud has numerous service offerings for SQL, NoSQL, and file storage. The following section explains them and provides guidelines on when to use which one.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#relational-databases","text":"Dash DB is the Db2 BLU on the Cloud offering from IBM that features column store, advanced compression, and execution on SIMD instructions sets (that is, vectorized processing). But there are other options in IBM Cloud such as Informix, PostgreSQL, and MySQL. What is the impact of storage cost? Relational databases (RDBMS) have the highest requirements on storage quality. Therefore, the cost of relational storage is always the highest. Which data types must be supported? Relational databases are meant for structured data. Although there are column data types for binary data that can be swapped for cheaper storage, this is just an add-on and not a core function of relational databases. How well must point queries (on fixed or dynamic dimensions) be supported? RDBMS are great for point queries because an index can be created on each column. How well must range queries (on fixed or dynamic dimensions) be supported? RDBMS are great for range queries because an index can be created on each column. How well must full table scans be supported? RDBMS are trying to avoid full table scans in their SQL query optimizers. Therefore, performance is not optimized for full table scans (for example, contaminating page caches). What skills are required? You should have SQL skills, and if a cloud offering isn\u2019t chosen, you should also have database administrator (DBA) skills for the specific database. What\u2019s the requirement for fault tolerance and backup? RDMBS support continuous backup and crash fault tolerance. For recovery, the system might need to go offline. What are the constant and peak ingestion rates? Inserts using SQL are relatively slow, especially if the target table contains many indexes that must be rebalanced and updated. Some RDBMS support bulk inserts from files by bypassing the SQL engine, but then the table usually needs to go offline for that period. What amount of storage is needed? RDMBS perform very well to around 1 TB of data. Going beyond that is complex and needs advanced cluster setups. What does the growth pattern look like? RDBMS support volume management, so continuous growth usually isn\u2019t a problem, even at run time. For shrinking, the system might need to be taken offline. What are the retention policies? RDBMS usually support automated retention mechanisms to delete old data automatically.","title":"Relational databases"},{"location":"architecture/architectural-decisions/#nosql-databases","text":"The most prominent NoSQL databases like Apache CouchDB, MongoDB, Redis, RethinkDB, ScyllaDB (Cassandra), and InfluxCloud are supported. What is the impact of storage cost? NoSQL databases are usually storage fault-tolerant, by default. Therefore, quality requirements on storage are less, which brings down storage cost. Which data types must be supported? Although NoSQL databases are meant for structured data as well, they usually use JSON as a storage format, which can be enriched with binary data. Although, a lot of binary data attached to a JSON document can bring the performance down as well. How well must point queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves point query performance. How well must range queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves range query performance. How well must full table scans be supported? NoSQL databases perform very well at full table scans. The performance is only limited by the I/O bandwidth to storage. What skills are required? Typically, special query language skills are required for the application developer and if a cloud offering isn\u2019t chosen, database administrator (DBA) skills are needed for the specific database. What\u2019s the requirement for fault tolerance and backup? NoSQL databases support backups in different ways. But some aren\u2019t supporting online backup. NoSQL databases are usually crash fault tolerant, but for recovery, the system might need to go offline. What are the constant and peak ingestion rates? Usually, no indexes need to be updated and data doesn\u2019t need to be mapped to pages. Ingestion rates are usually only bound to I/O performance of the storage system. What amount of storage is needed? RDMBS perform well to approximately 10 \u2013 100 TB of data. Cluster setups on NoSQL databases are much more straightforward than on RDBMS. Successful setups with >100 nodes and > 100.000 database reads and writes per second have been reported. What does the growth pattern look like? The growth of NoSQL databases is not a problem. Volumes can be added at run time. For shrinking, the system might need to be taken offline. What are the retention policies? NoSQL databases don\u2019t support automated retention mechanisms to delete old data automatically. Therefore, this must be implemented manually, resulting in range queries on the data corpus.","title":"NoSQL databases"},{"location":"architecture/architectural-decisions/#object-storage","text":"Cloud object storage makes it possible to store practically limitless amounts of data. It is commonly used for data archiving and backup, for web and mobile applications, and as scalable, persistent storage for analytics. So, let\u2019s take a look. What is the impact of storage cost? Object storage is the cheapest option for storage. Which data types must be supported? Because object storage resembles a file system, any data type is supported. How well must point queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. How well must range queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. Therefore, range queries on a single defined column (for example, data) can be achieved through hierarchical folder structures. How well must full table scans be supported? Full table scans are bound only by the I/O bandwidth of the object storage. What skills are required? On a file level, working with object storage is much like working with any file system. Through Apache SparkSQL and IBM Cloud SQL Query, data in Object storage can be accessed with SQL. Because object storage is a cloud offering, no administrator skills are required. IBM Object Storage is available for on-premises as well using an appliance box. What\u2019s the requirement for fault tolerance and backup? Fault tolerance and backup is completely handled by the cloud provider. Object storage supports intercontinental data center replication for high-availability out of the box. What are the constant and peak ingestion rates? Ingestion rates to object storage is bound by the uplink speed to the object storage system. What\u2019s the amount of storage needed? Object storage scales to the petabyte range. What does the growth pattern look like? Growth and shrinking on object storage is fully elastic. What are the retention policies? Retention of data residing in object storage must be done manually. Hierarchical file and folder layout that is based on data and time helps here. Some object storage support automatic movement of infrequently accessed files to colder storage (colder means less cost, but also less performance, or even higher cost of accesses to files).","title":"Object storage"},{"location":"architecture/architectural-decisions/#application-component-discovery-and-exploration","text":"This component allows for visualization and creation of metrics of data.","title":"Application Component: Discovery and exploration"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_5","text":"In various process models, data visualization and exploration is one of the first steps. Similar tasks are also applied in traditional data warehousing and business intelligence. So when choosing a technology, ask the following questions: What type of visualizations are needed? Are interactive visualizations needed? Are coding skills available or required? What metrics can be calculated on the data? Do metrics and visualization need to be shared with business stakeholders?","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-mapping_3","text":"IBM cloud has many service offerings for data exploration. Some of the offerings are open source, and some aren\u2019t.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#jupyter-python-pyspark-scikit-learn-pandas-matplotlib-pixiedust","text":"Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust are all open source and supported in IBM Cloud. Some of these components have overlapping features and some of them have complementary features. This can be determined by answering the architectural questions. What type of visualizations are needed? Matplotlib supports the widest range of possible visualizations including run chars, histograms, box-plots, and scatter plots. PixieDust (as of V1.1.11) supports tables, bar charts, line charts, scatter plots, pie charts, histograms, and maps. Are interactive visualizations needed? Matplotlib creates static plots and PixieDust supports interactive ones. Are coding skills available or required? Matplotlib requires coding skills, but PixieDust does not. For computing metrics, some code is necessary. What metrics can be calculated on the data? Using scikit-learn and pandas, all state-of-the-art metrics are supported. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of Jupyter Notebooks, also using a fine-grained user and access management system.","title":"Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust"},{"location":"architecture/architectural-decisions/#spss-modeler","text":"SPSS Modeler is available in the cloud and also as stand-alone product. What type of visualizations are needed? SPSS Modeler supports the following visualizations out of the box: Bar Pie 3D Bar 3D Pie Line Area 3D Area Path Ribbon Surface Scatter Bubble Histogram Box Map You can get more information in the IBM Knowledge Center . Are interactive visualizations needed? SPSS Modeler Visualizations are not interactive. Are coding skills available or required? SPSS Modeler doesn\u2019t require any coding skills. What metrics can be calculated on the data? All state-of-the-art metrics are supported using the Data Audit node. Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of SPSS Modeler Flows, also using a fine-grained user and access management system. However, those might not be suitable to stakeholders.","title":"SPSS Modeler"},{"location":"architecture/architectural-decisions/#application-component-actionable-insights","text":"This is where most of your work fits in. It\u2019s where you create and evaluate your machine learning and deep learning models.","title":"Application Component:\u00a0Actionable insights"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_6","text":"There are numerous technologies for creating and evaluating machine learning and deep learning models. Although different technologies differ in function and performance, those differences are usually miniscule. Therefore, the questions you should ask yourself are: What are the available skills regarding programming languages? What are the costs of skills regarding programming languages? What are the available skills regarding frameworks? What are the costs of skills regarding frameworks? Is model interchange required? Is parallel- or GPU-based training or scoring required? Do algorithms need to be tweaked or new algorithms be developed?","title":"Technology Component mapping guidelines"},{"location":"architecture/architectural-decisions/#technology-mapping_4","text":"Because there\u2019s an abundance of open and closed source technologies, we are highlighting the most relevant ones in this article. Although it\u2019s the same for the other sections as well, decisions made in this section are very prone to change due to the iterative nature of this process model. Therefore, changing or combining multiple technologies is no problem, although the decisions that led to those changes should be explained and documented.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#spss-modeler_1","text":"This article has already introduced SPSS Modeler. What are the available skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the costs of skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts. What are the available skills regarding frameworks? SPSS is an industry leader, so skills are generally available. What are the costs of skills regarding frameworks? Expert costs are usually lower in UI-based tools than in programming frameworks. Is model interchange required? SPSS Modeler supports PMML. Is parallel- or GPU-based training or scoring required? SPSS Modeler supports scaling through IBM Analytics Server or IBM Watson Studio using Apache Spark. Do algorithms need to be tweaked or new algorithms be developed? SPSS Modeler algorithms can\u2019t be changed, but you can add algorithms or customizations using the R language.","title":"SPSS Modeler"},{"location":"architecture/architectural-decisions/#rr-studio","text":"R and R-Studio are standards for open source-based data science. They\u2019re supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? R programming skills are usually widely available because it\u2019s a standard programming language in many natural science-based university curriculums. It can be acquired rapidly because it is a procedural language with limited functional programming support. What are the costs of skills regarding programming languages? Costs of R programming are usually low. What are the available skills regarding frameworks? R is not only a programming language but also requires knowledge of tooling (R-Studio), and especially knowledge of the R library (CRAN) with 6000+ packages. What are the costs of skills regarding frameworks? Expert costs are correlated with knowledge of the CRAN library and years of experience and in the range of usual programmer costs. Is model interchange required? Some R libraries support exchange of models, but it is not standardized. Is parallel- or GPU-based training or scoring required? Some R libraries support scaling and GPU acceleration, but it is not standardized. Do algorithms need to be tweaked or new algorithms be developed? R needs algorithms to be implemented in C/C++ to run fast. So, tweaking and custom development usually involves C/C++ coding.","title":"R/R-Studio"},{"location":"architecture/architectural-decisions/#python-pandas-and-scikit-learn","text":"Although R and R-Studio have been the standard for open source-based data science for a while, Python, pandas, and scikit-learn are right behind them. Python is a much cleaner programming language than R and easier to learn. Pandas is the Python equivalent to R data frames, supporting relational access to data. Finally, scikit-learn nicely groups all necessary machine learning algorithms together. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Python skills are very widely available because Python is a clean and easy to learn programming language. What are the costs of skills regarding programming languages? Because of Python\u2019s properties mentioned above, the cost of Python programming skills is very low. What are the available skills regarding frameworks? Pandas and scikit-learn are very clean and easy-to-learn frameworks. Therefore, skills are widely available. What are the costs of skills regarding frameworks? Because of the properties mentioned above, the costs of skills are very low. Is model interchange required? All scikit-learn models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? Neither GPU nor scale-out is supported, although scale-up capabilities can be added individually to make use of multiple cores. Do algorithms need to be tweaked or new algorithms be developed? scikit-learn algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with the numpy library. Therefore, tweaking and adding algorithms is straightforward.","title":"Python, pandas and scikit-learn"},{"location":"architecture/architectural-decisions/#python-apache-spark-and-sparkml","text":"Although Python, pandas, and scikit-learn are more widely adopted, the Apache Spark ecosystem is catching up, especially because of its scaling capabilities. It\u2019s supported in IBM Cloud through IBM Watson Studio as well. What are the available skills regarding programming languages? Apache Spark supports Python, Java programming, Scala, and R as programming languages. What are the costs of skills regarding programming languages? The costs depend on what programming language is used, with Python typically the cheapest. What are the available skills regarding frameworks? Apache Spark skills are in high demand and usually not available. What are the costs of skills regarding frameworks? Apache Spark skills are in high demand and are usually expensive. Is model interchange required? All SparkML models can be (de)serialized. PMML is supported through third-party libraries. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. However, GPU\u2019s are only supported through third-party libraries. Do algorithms need to be tweaked or new algorithms be developed? As in scikit-learn, algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with built-in Apache Spark libraries. Therefore, tweaking and adding algorithms is straightforward.","title":"Python, Apache Spark and SparkML"},{"location":"architecture/architectural-decisions/#apache-systemml","text":"When it comes to relational data processing, SQL is a leader, mainly because an optimizer takes care of optimal query executions. Think of SystemML as an optimizer for linear algebra that\u2019s capable of creating optimal execution plans for jobs running on data parallel frameworks like Apache Spark. What are the available skills regarding programming languages? SystemML has two domain-specific languages (DSL) with R and Python syntax. What are the costs of skills regarding programming languages? Although the DSLs are like R and Python, there is a learning curve involved. What are the available skills regarding frameworks? SystemML skills are very rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? SystemML models can be (de)serialized. PMML is not supported. SystemML can import and run Caffe2 and Keras models. Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. SystemML uses this property. In addition, GPU\u2019s are supported as well. Do algorithms need to be tweaked or new algorithms be developed? Although SystemML comes with a large set of pre-implemented algorithms for machine learning and deep learning, its strengths are in tweaking existing algorithms or implementing new ones because the DSL allows for concentrating on the mathematical implementation of the algorithm. The rest is handled by the framework. This makes it an ideal choice for these kind of tasks.","title":"Apache SystemML"},{"location":"architecture/architectural-decisions/#keras-and-tensorflow","text":"TensorFlow is one of the most widely used deep learning frameworks. At its core, it is a linear algebra library supporting automatic differentiation. TensorFlow\u2019s Python-driven syntax is relatively complex. Therefore, Keras provides an abstraction layer on top of TensorFlow. Both frameworks are seamlessly supported in IBM Cloud through Watson Studio and Watson Machine Learning. What are the available skills regarding programming languages? Python is the core programming language for Keras and TensorFlow. What are the available skills regarding frameworks? Keras and TensorFlow skills are relatively rare. What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high. Is model interchange required? Keras and TensorFlow have their own model exchange formats. There are converters from and to ONNX. Is parallel- or GPU-based training or sc oring required? Running TensorFlow on top of ApacheSpark is supported through TensorFrames and TensorSpark. Keras models can be run on ApacheSpark using DeepLearning4J and SystemML. Both of the latter frameworks also support GPUs. TensorFlow (and therefore, Keras) support GPU natively as well. Do algorithms need to be tweaked or new algorithms be developed? TensorFlow is a linear algebra execution engine. Therefore, it\u2019s optimally suited for tweaking and creating new algorithms. Keras is a very flexible deep learning library that supports many neural network layouts.","title":"Keras and TensorFlow"},{"location":"architecture/architectural-decisions/#application-component-applications-and-data-products","text":"Models are fine, but their value rises when they can be consumed by the ordinary business user. Therefore, you must create a data product. Data products don\u2019t necessarily need to stay on the cloud. They can be pushed to mobile or enterprise applications. In contrast to machine learning and deep learning frameworks, the space of frameworks to create data product is tiny. This might reflect what the current state-of-the-art technology in data science concentrates on. Depending on the requirements, data products are relatively visualization-centric after a lot of user input data has been gathered. They also might involve asynchronous workflows as batch data integration and model training and scoring is performed within the workflow. Questions to ask about the technology are: What skills are present for developing a data product? What skills are necessary for developing a data product? Is instant feedback required or is batch processing accepted? What\u2019s the degree of customization needed? What\u2019s the target audience? Is cloud scale deployment for a public use base required?","title":"Application Component:\u00a0Applications and Data Products"},{"location":"architecture/architectural-decisions/#technology-mapping_5","text":"Currently, only a limited set of frameworks and technologies is available in different categories. In the following section, I\u2019ve explained the most prominent examples.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#r-shiny","text":"R-Shiny is a great framework for building data products. Closely tied to the R language, it enables data scientists to rapidly create a UI on top of existing R-scripts. What skills are present for developing a data product? R-Shiny requires R development skills. So, it best fits into an R development ecosystem. What skills are necessary for developing a data product? Although based on R, R-Shiny needs additional skills. For experienced R developers, the learning curve is steep and additional knowledge to acquire is minimal. Is instant feedback required or is batch processing accepted? The messaging model of R-Shiny supports instant UI feedback when server-side data structures are updated. Therefore, the response time is independent of the framework and should be considered and resolved programmatically on the server side. What\u2019s the degree of customization needed? Although R-Shiny is an extensible framework, extending it requires a deep understanding of the framework and R-Technology. Out of the box, there is a large set of UI widgets end elements supported, allowing for very customizable applications. If requirements go beyond those capabilities, costly extensions are required. What\u2019s the target audience? Is cloud scale deployment for a public use base required? R-Shiny applications look very professional, although quite distinguishable. Therefore, the target audience must accept the UI design limitations. R-Shiny is best dynamically scaled horizontally in a container environment like Kubernetes. Ideally, every user session runs in its own container because R and R-Shiny are very sensitive to main memory shortages","title":"R-Shiny"},{"location":"architecture/architectural-decisions/#node-red_1","text":"Although Node-RED is a no-code/low-code data flow/data integration environment, because of its modular nature it supports various extensions including the dash boarding extension. This extension allows for fast creation of user interfaces including advanced visualizations that are updated in real-time. What skills are present for developing a data product? Due to the completely graphical user interface-based software development approach, only basic programming skills are required to build data products with Node-RED. What skills are necessary for developing a data product? Any resource familiar with flow-based programming as used in many state-of-the-art ETL and data mining tools will have a fast start with Node-RED. Basic JavaScript knowledge is required for creating advanced flows and for extending the framework. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? Node-RED is a Node.js/JavaScript-based framework. Custom UI widgets require advanced Node.js development skills. What\u2019s the target audience? Is cloud scale deployment for a public use base required? The Node-RED dashboard can be deployed for a pubic user base as long as the limitations regarding UI customization are acceptable. Because Node.js runs on a single threaded event loop, scaling must be done horizontally, preferably using a containerized environment. Note: The Internet of Things Starter kit in IBM Cloud supports horizontal scaling out of the box.","title":"Node-RED"},{"location":"architecture/architectural-decisions/#d3","text":"When it comes to custom application development, D3 is one of the most prominent and most widely used visualization widget frameworks with a large open source ecosystem contributing a lot of widgets for every desirable use case. There\u2019s a good chance that you can find a D3 widget to fit your use case. What skills are present for developing a data product? D3 fits best into an existing, preferably JavaScript-based developer ecosystem, although JavaScript is only required on the client side. Therefore, on the server side, any REST-based endpoints in any programming language are supported. One example is REST endpoints accessed by a D3 UI provided by Apache Livy that encapsulates Apache Spark jobs. What skills are necessary for developing a data product? D3 requires sophisticated D3 skills and at least client-side JavaScript skills. Skills in a JavaScript AJAX framework like AngularJS are highly recommended. On the server side, capabilities of providing REST endpoints to the D3 applications are required. Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services. What\u2019s the degree of customization needed? D3 applications usually are implemented from scratch. Therefore, this solution provides the most flexibility to the end user. What\u2019s the target audience? Is cloud scale deployment for a public use base required? As a cloud native application, a D3-based data product can provide all capabilities for horizontal and vertical scaling and full adoption to user requirements.","title":"D3"},{"location":"architecture/architectural-decisions/#application-component-security-information-governance-and-systems-management","text":"This important step can be easily forgotten. It\u2019s important to control who has access to which information for many compliance regulations. In addition, modern data science architectures involve many components that require operational aspects as well. Data privacy is a major challenge in many data science projects. Questions that you should ask are: What granularity is required for managing user access to data assets? Are existing user registries required to be integrated? Who is taking care of operational aspects? What are the requirements for data retention? What level of security against attacks from hackers is required?","title":"Application Component:\u00a0Security, information governance and systems management"},{"location":"architecture/architectural-decisions/#technology-mapping_6","text":"Again, there\u2019s a lot of software for this as well as ways to solve the requirements involved in this topic. We have chosen representative examples for this article. Deploying a productive client-facing web application brings with it serious risks. IBM Cloud Internet Services provides global points of presence (PoPs). It includes domain name service (DNS), global load balancer (GLB), distributed denial of service (DDoS) protection, web application firewall (WAF), transport layer security (TLS), and caching. What level of security against attacks from hackers is required? Internet Services is using services from CloudFlare, the world leader in this space.","title":"Technology Mapping"},{"location":"architecture/architectural-decisions/#ibm-app-id","text":"Identity Management allows for cloud-based user and identity management for web and mobile applications, APIs, and back-end systems. Cloud users can sign up and sign in with App ID\u2019s scalable user registry or social login with Google or Facebook. Enterprise users can be integrated using SAML 2.0 federation. What granularity is required for managing user access to data assets? IBM App ID supports user management but no group/roles. Therefore, fine-grained access must be managed within the application. Are existing user registries required to be integrated? IBM App ID supports registry integration using SAML 2.0 federation.","title":"IBM App ID"},{"location":"architecture/architectural-decisions/#object-storage_1","text":"Object Storage is a standard when it comes to modern, cost-effective cloud storage. What granularity is required for managing user access to data assets? IBM Identity and Access Management (IAM) integration allows for granular access control at the bucket-level using role-based policies. What are the requirements for data retention? Object storage supports different storage classes for frequently accessed data, occasionally accessed data, and long-term data retention with standard, vault, and cold vault. The Flex class allows for dynamic data access and automates this process. Physical deletion of data still must be triggered externally. Who is taking care of operational aspects? Regional and cross-region resiliency options allow for increased data reliability. What level of security against attacks from hackers is required? All data is encrypted at rest and in flight by default. Keys are automatically managed by default, but optionally can be self-managed or managed using IBM Key Protect.","title":"Object Storage"},{"location":"architecture/architectural-decisions/#ibm-cloud-paassaas","text":"IBM Cloud PaaS/SaaS eliminates operational aspects from data science projects because all components involved are managed by IBM. Who is taking care of operational aspects? In PaaS/SaaS, cloud operational aspects are being taken care of by the cloud provider.","title":"IBM Cloud PaaS/SaaS"},{"location":"architecture/build-model/","text":"Building a machine learning model Integrate ML workbench with data Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning. Real time data sources The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Analyze & build model"},{"location":"architecture/build-model/#building-a-machine-learning-model","text":"","title":"Building a machine learning model"},{"location":"architecture/build-model/#integrate-ml-workbench-with-data","text":"Reusing our reference architecture, we are now defining the flow of activities happening to develop the model and to continuously improve it overtime while the intelligent application is running in production: The business has identified a desired outcome that can be realized by building a predictive model e.g. customer churn and next best action prodiction. To build the predictive model; the Business Analyst, Data Steward, Data Scientist and Cognitive Architect collaborate using Watson Studio. They look at existing catalogs and data collections (such as in an enterprise data lake or data warehouse) to identify the optimal informing features towards building an effective predictive model. Understanding the needs and goals of the new models will provide the metrics required to determine if existing data structures may be used, or if new environments will need to be created. At this time, the overall design goals will be defined i.e. should it be a data lake or warehouse, time sensitivity of the data, where should it be deployed etc. They discuss some signals could be extracted out of third party and social data that is available on a different cloud. In today\u2019s hybrid world, this information may be pulled in via tools like ICP for Data and choice can be made around what use as is, what to virtualize and what to federate. Using a cloud native data service, these additional signals are pulled into the data collections. Depending on the choices made at step 3, tools are available to include the new data sources in the environment. Various portions of the informing data collections are refined to engineer features suitable for the predictive model using Data Refinery. Often data requires some curation, or possibly new data is created (i.e. aggregate totals may be created) or some other modification/enhancement will be needed. Since some of the information is unstructured, a number of Watson NLU APIs are used to extract entities and taxonomies to enrich the unstructured information with relevant meta-data to extract entities and taxonomies to enrich the unstructured information with relevant meta-data. With data like this, we can choose the optimal database technology to store and to access the data. As these enrichments are deemed to be impacting many business use cases, these are methodically introduced as part of the ongoing data enrichment. Given the iterative nature of building models, it\u2019s important to note that a key feature is the ability to react to change quickly. With multiple batch experiments and tuned hyperparameters, a performant model is available for use. If the user has chosen an on-premise deployment, they may choose to take advantage of the fact that our appliance has the Watson Studio built directly onto the warehousing system. The model is deployed using Watson Machine Learning, providing a scalable model with continuous learning.","title":"Integrate ML workbench with data"},{"location":"architecture/build-model/#real-time-data-sources","text":"The following diagram presents another view for data injection and transformation from real time events streams persisted to data repositories or warehouse. When using event backbone combined with event store, data persisted in this area can be combined with warehouse data, as data sources for the Data Scientist to build his model. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Real time data sources"},{"location":"architecture/collect-org-data/","text":"Collect and Organize Data As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party, external data, social media, sensors and weather data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Collect - organize data"},{"location":"architecture/collect-org-data/#collect-and-organize-data","text":"As part of the collect and analyze activities, is a focus to govern and manage the data for building the data lake. Using the reference architecture, the following capabilities are used to address this goal: Data sources are more than just databases, these include 3 rd party, external data, social media, sensors and weather data. Connectors provide specialized client code to access the function/data of an asset as well as any metadata it handles. Typically, the metadata identifies the type of asset, how to access it and possibly the schema of the data if the data is structured. Capturing metadata from processes that copy data from one location to another enables the assembly of lineage. The data catalog provides a search interface to make it easy to query details of the data landscape. Metadata discovery engines access the data through the connectors and add additional insight about the data content into the data catalog. Knowlege catalog: Glossaries define the meaning of the terminology used by the people working in the organization when they interact with each other, external stakeholders and digital systems. The Asset Owners link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance team add definitions for how the assets of the organization should be governed and link them to classifications. The Subject Area Owners attach the governance classifications to their glossary terms. This identifies the governance requirements that apply to assets linked to these terms. The Asset Owners still link their assets to the relevant glossary terms, making it easier to find relevant data in the data catalog. The governance classifications attached to the glossary terms then apply to the assets. They may add additional classifications to the assets if not covered by the classifications attached to the glossary terms. Classified glossary terms flow to the data catalog. Replication of governance definitions into the data catalog allows the catalog users to understand the governance requirements behind the classifications. Users of the search tool can drill down to understand the classification of assets and the governance requirements behind them. The IT Team encode the governance definitions into the technologies so that enforcement and auditing of governance requirements is automated.The encoding uses placeholders for metadata values that are accessed at runtime. Details of the governance requirements, classification and asset details are sent to an operational metadata store. This store serves up metadata that drives the governance functions embedded in operational systems. Updates to classifications attached to glossary terms made by the subject area owners flow through the data catalog to the operational metadata store. Large population of users accessing the data catalog. Need to consider security of metadata and scaling the deployment of the data catalog. Two way synchronization needed with business tools that work with data to ensure all users see consistent governed assets, and feedback is consolidated to be passed to asset owners. Asset owners need to be part of the collaboration and feedback cycle. Subject area owners receive direct feedback and questions from data catalog users about the glossary term definitions. They use the interaction to correct and improve the glossary terms as well as crowd-source new terms.","title":"Collect and Organize Data"},{"location":"architecture/intelligent-app/","text":"Designing Data intensive applications In this article we are highlighting some practice to design data intensive application and microservice solution. This is strongly linked to the adoption of event-driven microservices, but address the data consistency and eventual data consitency discussions, as well as the different strategies for implementation. Started 10/30/2019, still under heavy work. Context A typical modern business solution will include a set of microservices working together in choreography to exchange data. The adoption of event-driven microservice with all related design pattern is described in separate articles that you can read here . When zooming to a particular data intensive microservice we will find a set of important data centric features that may look like in the diagram below, which presents one instance of a distributed system. The components involved include: Databases to store data for long term Caches to speed up retrieving data from expensive operation Search indexes to support search on a corpus Stream processing to pub/sub messages with other processes, which are now also considered as long duration datastores (Kafka). Batch processing to move large data between data sources When designing such application we need to address a set of important subjects: How to ensure data correctness and completeness? How to address good performance when exposing data, even when app is running slowly? How to scale and address increase in transaction volume and data size increase? What data to expose to other services via messaging ? What data to expose to other services via APIs ? How to support application reliability when some components are not performing within their SLA? How to be fault-tolerant? How to test fault-tolerance? How adding horizontal compute power impact the data access? In modern big data applications, hardware redundancy is not suffisant, the design needs to support unexpected faults, to avoid cascading failures and to support new version deployment with rolling-upgrade capability. When addressing scalability and load growth, we need to define the load parameters: number of transactions per second, or number of read and write operations, number of active sessions, ... on average and at peak. Each microservice in a solution will have its own load parameters. From there we need to design to address the following issues: How does load growth impact performance while keeping existing compute resources? What is the increase of compute resource needed to support same performance while load growth? The solution problem is a combination of different characteristics to address: read volume, write volume, data store volume, data complexity and size, response time, access logic... For batch processing the measurement is the throughput: number of records per second or time to process n records. For real time processing the response time measures the time to get a response from a client's point of view after sending a request. When defining service level agreement, it is important to use the median response time and a percentile of outliers. An example the median could be at 300ms at P99 (99/100) under 1s. Tail latencies, or high percentils of response time, impact directly user experience and cost money. Distributed data Adopting microservice architecture, means distributed systems and distributed data. The main motivations for that are scalability (load data operations could not be supported by one server), high availability (by sharing the same processing between multiple machines), and reducing latency to distribute data close to the end users. Vertical scaling is still bounded by hardware resources, so at higher load we need to support horizontal scaling by adding more machine in cluster or in multiple clusters. When adding machines, we may want to adopt different techniques for data sharing: shared memory shared storage shared nothing: cpu, memory and disk are per node. Cluster manages node orchestration over network. This architecture brings new challenges. Compendium Designing data intensive application - Martin Kleppmann Cassandra","title":"Designing intelligent application"},{"location":"architecture/intelligent-app/#designing-data-intensive-applications","text":"In this article we are highlighting some practice to design data intensive application and microservice solution. This is strongly linked to the adoption of event-driven microservices, but address the data consistency and eventual data consitency discussions, as well as the different strategies for implementation. Started 10/30/2019, still under heavy work.","title":"Designing Data intensive applications"},{"location":"architecture/intelligent-app/#context","text":"A typical modern business solution will include a set of microservices working together in choreography to exchange data. The adoption of event-driven microservice with all related design pattern is described in separate articles that you can read here . When zooming to a particular data intensive microservice we will find a set of important data centric features that may look like in the diagram below, which presents one instance of a distributed system. The components involved include: Databases to store data for long term Caches to speed up retrieving data from expensive operation Search indexes to support search on a corpus Stream processing to pub/sub messages with other processes, which are now also considered as long duration datastores (Kafka). Batch processing to move large data between data sources When designing such application we need to address a set of important subjects: How to ensure data correctness and completeness? How to address good performance when exposing data, even when app is running slowly? How to scale and address increase in transaction volume and data size increase? What data to expose to other services via messaging ? What data to expose to other services via APIs ? How to support application reliability when some components are not performing within their SLA? How to be fault-tolerant? How to test fault-tolerance? How adding horizontal compute power impact the data access? In modern big data applications, hardware redundancy is not suffisant, the design needs to support unexpected faults, to avoid cascading failures and to support new version deployment with rolling-upgrade capability. When addressing scalability and load growth, we need to define the load parameters: number of transactions per second, or number of read and write operations, number of active sessions, ... on average and at peak. Each microservice in a solution will have its own load parameters. From there we need to design to address the following issues: How does load growth impact performance while keeping existing compute resources? What is the increase of compute resource needed to support same performance while load growth? The solution problem is a combination of different characteristics to address: read volume, write volume, data store volume, data complexity and size, response time, access logic... For batch processing the measurement is the throughput: number of records per second or time to process n records. For real time processing the response time measures the time to get a response from a client's point of view after sending a request. When defining service level agreement, it is important to use the median response time and a percentile of outliers. An example the median could be at 300ms at P99 (99/100) under 1s. Tail latencies, or high percentils of response time, impact directly user experience and cost money.","title":"Context"},{"location":"architecture/intelligent-app/#distributed-data","text":"Adopting microservice architecture, means distributed systems and distributed data. The main motivations for that are scalability (load data operations could not be supported by one server), high availability (by sharing the same processing between multiple machines), and reducing latency to distribute data close to the end users. Vertical scaling is still bounded by hardware resources, so at higher load we need to support horizontal scaling by adding more machine in cluster or in multiple clusters. When adding machines, we may want to adopt different techniques for data sharing: shared memory shared storage shared nothing: cpu, memory and disk are per node. Cluster manages node orchestration over network. This architecture brings new challenges.","title":"Distributed data"},{"location":"architecture/intelligent-app/#compendium","text":"Designing data intensive application - Martin Kleppmann Cassandra","title":"Compendium"},{"location":"architecture/lightweight-architectural-decisions-document-template/","text":"The Lightweight Method Architectural Decisions Document Template Application Components Overview Application Component: Data Source Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Enterprise Data Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Streaming analytics Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Data Integration Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Data Repository Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Discovery and Exploration Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Actionable Insights Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Applications / Data Products Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here. Application Component: Security, Information Governance and Systems Management Technology Component mapping Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below. Justification Please justify your Technology Component mappings here.","title":"The Lightweight Method Architectural Decisions Document Template"},{"location":"architecture/lightweight-architectural-decisions-document-template/#the-lightweight-method-architectural-decisions-document-template","text":"","title":"The Lightweight Method Architectural Decisions Document Template"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-components-overview","text":"","title":"Application Components Overview"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-data-source","text":"","title":"Application Component: Data Source"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-enterprise-data","text":"","title":"Application Component: Enterprise Data"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_1","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_1","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-streaming-analytics","text":"","title":"Application Component: Streaming analytics"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_2","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_2","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-data-integration","text":"","title":"Application Component: Data Integration"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_3","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_3","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-data-repository","text":"","title":"Application Component: Data Repository"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_4","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_4","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-discovery-and-exploration","text":"","title":"Application Component: Discovery and Exploration"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_5","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_5","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-actionable-insights","text":"","title":"Application Component: Actionable Insights"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_6","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_6","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-applications-data-products","text":"","title":"Application Component: Applications / Data Products"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_7","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_7","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/lightweight-architectural-decisions-document-template/#application-component-security-information-governance-and-systems-management","text":"","title":"Application Component: Security, Information Governance and Systems Management"},{"location":"architecture/lightweight-architectural-decisions-document-template/#technology-component-mapping_8","text":"Please describe what Technology Component you have defined here. Please justify below, why. In case this component is not needed justify below.","title":"Technology Component mapping"},{"location":"architecture/lightweight-architectural-decisions-document-template/#justification_8","text":"Please justify your Technology Component mappings here.","title":"Justification"},{"location":"architecture/runtime-flow/","text":"Runtime architectures Intelligent application integrating ML service and bias monitoring From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings. Intelligent application with real time analytics The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Infuse in application"},{"location":"architecture/runtime-flow/#runtime-architectures","text":"","title":"Runtime architectures"},{"location":"architecture/runtime-flow/#intelligent-application-integrating-ml-service-and-bias-monitoring","text":"From the main reference architecture, the intelligent application is composed of multiple components working together to bring business agility and values: The AI model is deployed as a service or running within a real time analytics processing application or embedded device. An AI Orchestrator (e.g.Watson OpenScale) is used to monitor runtime performance and fairness over the customer population. The predictions are interpreted by an AI Insights Orchestrator (e.g. deployed on IBM Cloud). The orchestrator consults Decision Management or Optimization components for the next best action or offer to be provided to the customer. This offer is communicated within the customer\u2019s workflow by infusing it into the active customer care and support interactions, such as a personal assistant (e.g. Watson Assistant) implementation. Over some time period, the AI Orchestrator detects model skew and needs to be retrained. This triggers deeper analysis using, ML Workbench (e.g. Watson Studio), of the divergence from the original model and similar steps to the original model design are taken to retrain the model with optimal features and new training data. An updated, better performing model is deployed to the model serving component (e.g. Watson Machine Learning), as another turn of the iterative end to end model lifecycle governance. Customers react favorably to the offers and the business starts to see the desired outcomes of lower customer churn and higher satisfaction ratings.","title":"Intelligent application integrating ML service and bias monitoring"},{"location":"architecture/runtime-flow/#intelligent-application-with-real-time-analytics","text":"The generic reference architecture can be extended with a real time analytics view. The application solution includes components integrated with event streams to do real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in this note . The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsible to have its own data source. Either a scoring service (e.g. function as service or model serving component) runs the model or the model is embedded directly into the streaming engine (e.g. same process space) The action to perform once the scoring is returned, is business rules intensive, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: Typical event sources include: IoT devices Trade / Financial Data Weather Data In an Edge/Fog computing screnario first aggregations and transformations can be done on the edge A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics engine training/applying machine learning models on the event stream. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is no strong deviation on the accuracy and responses are not biased. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. One specific architecture pattern used in the Reefer shipment reference implementation looks like this: The Reefer container, acting as IoT device, emits container metrics every minute via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the messages to kafka events. Kafka is used as the event backbone and event source so microservices, deployed on a container orchestrator (e.g. RedHat OpenShift), can consume and publish messages. For persistence reasons, we may leverage big data types of storage like Cassandra to persist the container metrics over a longer time period. This datasource is used for the Data Scientists to do its data preparation and build training and test sets. Data scientists can run a ML Worbench (e.g. Watson Studio on IBM Cloud or Jupyter Lab on OpenShift) and build a model to be deployed as python microservice, to consume kafka events. A potential action is triggering a notification to put the Reefer container into maintenance.","title":"Intelligent application with real time analytics"},{"location":"data/","text":"Data is a fundamental element of every business and therefore fundamental to our architecture. Data is our record of current state of the business, the history of what has happened, and is the base which enables us to predict what may happen in the future. However, on its own data doesn't do anything and to realise value from data we have to do something with it, we have to understand it, and act on it. Typically this requires something other than the data such as a computer program, a query, or a user (machine or person). Perhaps one of the biggest and most complex challenges comes with managing data. Data is inert; it is not self-organizing or even self-understanding. So how do we manage the data , how do we organize an attach meaning so that the data can easily be used bu the business, or a computer program etc. The DIKW pyramid , provides a simple visualisation of the value chain growing from data to Wisdom : Data is the base with the least amount of perceived usefulness. Information has higher value than data. Knowledge has higher value than information. Wisdom has the highest perceived value of all. To move up the value chain data requires something else such as a program, a machine, or even a person\u2014to add to understanding to it so that it becomes Information . By organizing and classifying information , the value chain expands from data and information to be regarded as knowledge . At the top of the data value chain is Wisdom . Wisdom comes from a combination of inert data, which is the fundamental raw material in the modern digital age, combined with a series of progressive traits such as: perspective context understanding learning * the ability to reason. With the advent of Cognitive computing and artificial intelligence these traits can now be attributed to both a person and a machine. The IBM AI Ladder loosely parallels the DIKW pyramid in that the AI Ladder represents a progressive movement towards value creation within an enterprise. Increased value can be gained from completing activities at each step of the AI Ladder, with the potential to recognize higher levels of value, the higher the ladder is climbed. The AI Ladder contains four discrete levels: Collect Organize Analyze Infuse. The first rung is Collect , a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. In linking to the DIKW pyramid we could say that, data lies below the first rung, recognizing the inert nature of data. The AI Ladder progresses through the rungs to infuse , a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities. Collect \u2013 Making Data Simple and Accessible The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above Organize \u2013 Trusted, Governed Analytics The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility Analyze \u2013 Insights On-Demand The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities, from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions Infuse \u2013 Operationalize AI with Trust and Transparency The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise Data as a differentiator Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects . Making data enabled and active There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions. Towards Data-Centricity Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"AI Ladder"},{"location":"data/#collect-making-data-simple-and-accessible","text":"The first rung of the AI Ladder is Collect and is how an enterprise can formally incorporate data into any analytic process. Properties of data include: Structured, semi-structured, unstructured Proprietary or open In the cloud or on-premise Any combination above","title":"Collect \u2013 Making Data Simple and Accessible"},{"location":"data/#organize-trusted-governed-analytics","text":"The second rung of the AI Ladder is Organize and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of Organize is a body of data that is appropriately curated and offers the highest value to an enterprise. Organize allows data to be: Discoverable Cataloged Profiled Categorized Classified Secured (e.g. through policy-based enforcement) A source of truth and utility","title":"Organize \u2013 Trusted, Governed Analytics"},{"location":"data/#analyze-insights-on-demand","text":"The third rung of the AI Ladder is Analyze and is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities, from basic reporting and business intelligence to deep learning. Analyze, through data, allows to: Determine what has happened Determine what is happening Determine what might happen Compare against expectations Automate and optimize decisions","title":"Analyze \u2013 Insights On-Demand"},{"location":"data/#infuse-operationalize-ai-with-trust-and-transparency","text":"The fourth rung of the AI Ladder is Infuse and is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. Infuse allows data to be: Used for automation and optimization Part of a causal loop of action and feedback Exercised in a deployed model Used for developing insights and decision-making Beneficial to the data-driven organization Applied by the data-centric enterprise","title":"Infuse \u2013 Operationalize AI with Trust and Transparency"},{"location":"data/#data-as-a-differentiator","text":"Data needs to become treated as a corporate asset. Data has the power to transform any organization, add monetary value, and enable the workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. While a dog house can be built without much planning, you cannot build a modern skyscraper with the same approach. The scale of preserved data across a complex hybrid cloud or multi-cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, organizations can become data-centric and data-driven in a short time. More so than ever, businesses need to move rapidly. Organizations must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, irrespective of size. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects .","title":"Data as a differentiator"},{"location":"data/#making-data-enabled-and-active","text":"There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions.","title":"Making data enabled and active"},{"location":"data/#towards-data-centricity","text":"Drivers for what causes change within a business can be regarded as being stochastic. Whether foreseen or randomly determined, each change is likely to require new data \u2013 data that an organization has not previously anticipated. Increased data volumes, increases in the number of data sources, increases to the rates of data ingestion, and increases in the variety of the types of data are nothing more than de facto a prioris. While users are likely to have access to terabytes, petabytes, or even exabytes of data from data streams, IOT-sensors, transactional systems, and so on, if the data is not properly incorporated, managed, controlled, enriched, governed, measured, and deployed then the data may not only become useless, the data may become a liability. The activities to properly handle data and to pursue the AI Ladder, can be shown in the three solution areas of IBM Data and AI offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a common application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"Towards Data-Centricity"},{"location":"data/data-integration/","text":"Data integration There is no universal database solution to address the variety of data requirements business applications need. And it is not wishable to have such monster product. So the need for data integration should come by considering the dataflows across a business process at stake or across an entire business organization and even sometime in a network of business partners (a blockchain network as an example). Dataflows When we need to separate the write and read model, like by adapting CQRS, we need to pay attention on where the data are create first, where they should be persisted, and which representations are derived from which sources. For each target reads, what are the expected formats, and data locations? The classical approach to keep different systems data consistent, is to use two phase commit transaction, and the orchestrator to keep the write operation in expected sequence. Alternate model is to use event sourcing to keep write order in a log.","title":"Data Integration"},{"location":"data/data-integration/#data-integration","text":"There is no universal database solution to address the variety of data requirements business applications need. And it is not wishable to have such monster product. So the need for data integration should come by considering the dataflows across a business process at stake or across an entire business organization and even sometime in a network of business partners (a blockchain network as an example).","title":"Data integration"},{"location":"data/data-integration/#dataflows","text":"When we need to separate the write and read model, like by adapting CQRS, we need to pay attention on where the data are create first, where they should be persisted, and which representations are derived from which sources. For each target reads, what are the expected formats, and data locations? The classical approach to keep different systems data consistent, is to use two phase commit transaction, and the orchestrator to keep the write operation in expected sequence. Alternate model is to use event sourcing to keep write order in a log.","title":"Dataflows"},{"location":"data/data-partitioning/","text":"Data partitioning The goal of partitioning is to spread the data and query load evenly across multiple machines, avoiding nodes with disproportionately load.","title":"Data Partitioning"},{"location":"data/data-partitioning/#data-partitioning","text":"The goal of partitioning is to spread the data and query load evenly across multiple machines, avoiding nodes with disproportionately load.","title":"Data partitioning"},{"location":"data/data-replication/","text":"Data replication Abstract In this article, we are dealing with data replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environments or to separate read and write models, which may be a viable solution with microservices, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern. We recommend reading the 'Ensure data resilience' article from Neal Fishman to understand the problems related to data resilience and how they fit into data topology and governance broader discussions. Concepts Data Replication is the process of storing data on more than one storage location on same or different data centers. It fits well, when dataset is small enough to be persisted in one machine, and when data do not change over time. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicates are in a consistently updated state and synchronized with the source. The database centric replication mechanism involves different techniques to apply data replication between databases of the same product (DB2 to DB2, Postgresqsl to Postgresql). The approach is to control availability and consistency. Replication implementations are most of the time, black box for the business application developers, but we want to present three types of replication architecture: single-leader : one of the replica is the leader, and receive write operations from client services. Other replicas are followers, listening to data updates from a replication log and modify their own data store. Read operations can happen on any node, and followers are specifically read-only. This is a pattern used by Kafka, Postgresql, RabbitMQ... multi-leader : multiple nodes accept write operations (they are leaders), and act as follower for other leaders. leaderless : each node accept write operations. Client sends write to multiple replicas, accept p acknowledge (p <= number of nodes), but also performs n reads to assess eventual stale data. (This type is used in Cassandra or DynamoDB) Single leader All write requests are done on the leader, but reads can happen on followers as well. It helps to scale out the solution and also to support long distance replications. Replication can be done synchronously or asynchronously. With synchronous the leader waits until at least one follower has confirmed replication before reporting to the client that the write operation is successful. From the client point of view, it is a synchronous call. With asynchronous the leader does not wait for followers to replicate. With synchronous, we are sure the data are not lost as at least one follower responded, but if the follower node has failure then the write operation is not consider completed, even if leader has the data. The client can do a retry. But the leader may need to block other write requests until it sees a follower alive. With synch, the data is also consistent with the leader. The figure above illustrates a synchronous mechanism for client - leader and first follower to respond, and asynchronous for the other followers. It is important to note that asynch replica may be the only possible solution when the number of followers is important or when they are separated by long distance with high latency networking communication. Because a full synchronous mechanism will block the write operations in case of a follower failure and will not be reliable. With asynch, two reads at the same time on the leader and one of the follower will not get the same results. This inconsistency is temporal: when there is no more write then the followers will become eventually consistent. This elapse time is called replication lag. This lag can have some interesting derived issues, like seeing data in one query (done on a follower with small lag) and then not seeing the data from the same query done on a bigger lagged follower. To avoid that, the pratice is to use a monolitic read: user makes several reads in sequence, they will not see time go backward. Which can be achieved by ensuring the reads for a given user are done on the same follower. This is what kafka does by assigning a consumer to partition. With asynch replica, if the leader fails, data not yet replicated to the followers, is lost until a new leader starts to accept new writes. Also adding a new follower brings other challenges: as the data are continuously being written to the leader database, copying the database files to the new follower will not be consistent. One adopted solution is to snapshot the database, without locking write operations, and copy the data to the follower, then from this snapshot, consumes the update log. To work the snapshot position needs to be known in the replication log. To support follower failure, the log needs to keep position of the last commited read, so when the follower restarts, it can load data from this position in the log. When the leader fails, a follower needs to take the leadership, and then other followers need to get data from the new leader. Kafka, combined with zookeeper, uses those last two mechanisms to support replication. Selecting the new leader is based on consensus algorithm, to ensure minimum data lost. When using single leader, it is possible to reach a split brain state, when the old leader comes back to live, and thinks it is still a leader: both leaders accept writes which leads to data corruption. Most RDBMS uses the master-slave pattern combined with asynchronous replication. Postgresql uses a file based log shipped to the followers when a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data changes in the order in which commits were made on the publisher side so that transactional consistency is guaranteed for the publications within any single subscription. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed (usually mitigated by 3 rd party tooling). Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly. Multi leaders With multi-leader configuration each leader gets write operations, and propagates data update notifications to all nodes. This is an active-active replication. And it is the practice when dealing with multiple datacenters. The write operations are propagated asynchronously which is more permissive to network failure. It is important to note that the following problems need to be addressed with this topology: Update of the same data at the same time, leading to write conflict Using automatic primary key generation by the database may create duplicate keys Triggers that work on conditions on the data to do something could never be triggered due to data conditions that will never happen in this active - active configuration. Integrity constraints can be violated, as one records may be processed without the others being yet present. For the write conflict resolution, there are different strategies, one uses timestamps so the last write wins the record update, but this could lead to data lost. In fact conflict resolution will be dependant of the business logic and data knowledge. So custom code needs to be done to apply this logic to write conflicts. When there is more than two leaders, the replicas topology can be use a ring, star or all-to-all model, as illustrated in the figure below: With Ring , a leader forwards and propagates its own writes to one neighbor. With Star one designated root node forwards writes to all of the other nodes. Finally with All-to-all , every leader sends its writes to every other leader. With Ring and Star, a node failure impacts the data replication to any node, and with all-to-all we need to address looping on the same write. The mitigation is to use a unique identifier for each node, so a write event coming with the same node_id at the current node id is discarded. Leaderless In this last replication technique, the client application is doing write operation on any replicas. There is no leader. This is the approach used by Cassandra and Dynamo system. The write operations order is not maintained. With leaderless failover does not exist, in case of node failure the client has to accept n missing acknowledges. If the client read back data from a previously failed node, that just restarted, it may read old / stale data. To mitigate this problem, reads are done to multiple nodes in parallel, and the client needs to consolidate the returned value (may be using timestamp or version number). A node that is restarting can catch up on the replicated data by doing read repair or by using an anti-entropy process. Read repair, is done by the client seeing old value from one of the replica, and push back the new value to it. The anti-entropy process is in the database mechanism, with a daemon process running to replicate missing data to any replicas. CAP Theorem Data replication in distributed computing, like the cloud, falls into the problem of the CAP theorem where only two of three properties consisting of Consistency, Availability and Partition Tolerance can be met simultaneously. In most microservice implementation context, Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions. Considerations The main advantages for data replication (providing a consistent copy of data across data storage locations) are: increased data availability increased access performance by moving data close to users decreased network load Maintaining Data consistency at all different sites involves complex processes, and increase physical resources. Business motivations Among the top three priority of data integration and integrity solution, for 2019 and 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data sync (51%) Two important derived use cases: Business in Real Time : Detect and react to data events as they happen to drive the business, and propagate those changes for others for consumption Optimize decision to sub-second latency level, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data replication deployments Data synchronization for zero down time data migrations and upgrades Technology overview Data lake technologies There are multiple alternatives to build a data lake solution. In the field, the following technologies are usually used: Hadoop, Apache Spark, cloud object storage, and Apache Kafka. Hadoop/HDFS (Hadoop File System) is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Apache Spark is a data processing engine tightly coupled with the Hadoop ecosystem. It is one of the most active open source projects ever. Spark is mainly used as ETL engine capable of integrating data from various SQL and NoSQL data sources and targets including RDBMS, NoSQL DB, Cloud Object Store, HDFS and other cluster file systems COS (Cloud Object Storage) in conjunction with a Query Engine like Apache SparkSQL or IBM Cloud SQLQuery Service is a legit data lake solution, especially because, as a managed service, it provides unlimited capacity and very high scalability and robustness against failures Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Although Kafka supports persisting data in queues for weeks or even months, it's not yet a proved technology for long term storage, even if companies are already adopting it for event sourcing. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feeding data into Kafka) and data consumers (pulling data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr, etc... Change data capture (CDC) Another important part of the architecture is the change data capture component. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL products, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities. For example, IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: Product explanations can be obtained here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. This is known as lambda-architecture . The combined architecture of a deployed solution looks like in the diagram below: With the management console, the developer can define a data replication project that can include one to many subscriptions. Subscriptions define the source database and tables and target kafka cluster and topics. The Kafka cluster can run on Kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running a subscription the first time, Kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more details about this solution see this product tour . Debezium Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use Kafka as backbone, and Kafka connect. It uses the approach of replicating one table to one Kafka topic. It can be used for data synchronization between microservices using CDC at a service level and propagate changes via Kafka. The implementation of the CQRS pattern may be simplified with this capability. Why adopting Kafka for data replication Using Kafka as a integration layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output targets Built in stream processing API on real time feeds with Kafka streams Commit Log Fault tolerance, scalability, multi-tenancy, speed, light-weight, multiple landing-zones. Kafka connect Kafka connect simplifies the integration between Kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed, fault tolerant runtime able to easily scale horizontally. The set of connectors help developers to not re-implement consumers and producers for every type of data source. To get started please read this introduction from the product documentation. The Kafka connect workers are stateless and can run easily on Kubernetes or as standalone docker process. Kafka Connect Source is to get data to Kafka, and Kafka Connect Sink to get data out of Kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements define a task. A connector can have multiple tasks. In distributed deployment, the connector supports scaling by adding new workers and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST API. Recommended Readings IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial Ensure data resilience - author: Neal Fishman","title":"Data Replication"},{"location":"data/data-replication/#data-replication","text":"Abstract In this article, we are dealing with data replication from traditional Database, like DB2, to a microservice environment with document oriented or relational data. Database replications can be used to move data between environments or to separate read and write models, which may be a viable solution with microservices, but we need to assess how to support coexistence where older systems run in parallel to microservices and use eventual data consistency pattern. We recommend reading the 'Ensure data resilience' article from Neal Fishman to understand the problems related to data resilience and how they fit into data topology and governance broader discussions.","title":"Data replication"},{"location":"data/data-replication/#concepts","text":"Data Replication is the process of storing data on more than one storage location on same or different data centers. It fits well, when dataset is small enough to be persisted in one machine, and when data do not change over time. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicates are in a consistently updated state and synchronized with the source. The database centric replication mechanism involves different techniques to apply data replication between databases of the same product (DB2 to DB2, Postgresqsl to Postgresql). The approach is to control availability and consistency. Replication implementations are most of the time, black box for the business application developers, but we want to present three types of replication architecture: single-leader : one of the replica is the leader, and receive write operations from client services. Other replicas are followers, listening to data updates from a replication log and modify their own data store. Read operations can happen on any node, and followers are specifically read-only. This is a pattern used by Kafka, Postgresql, RabbitMQ... multi-leader : multiple nodes accept write operations (they are leaders), and act as follower for other leaders. leaderless : each node accept write operations. Client sends write to multiple replicas, accept p acknowledge (p <= number of nodes), but also performs n reads to assess eventual stale data. (This type is used in Cassandra or DynamoDB)","title":"Concepts"},{"location":"data/data-replication/#single-leader","text":"All write requests are done on the leader, but reads can happen on followers as well. It helps to scale out the solution and also to support long distance replications. Replication can be done synchronously or asynchronously. With synchronous the leader waits until at least one follower has confirmed replication before reporting to the client that the write operation is successful. From the client point of view, it is a synchronous call. With asynchronous the leader does not wait for followers to replicate. With synchronous, we are sure the data are not lost as at least one follower responded, but if the follower node has failure then the write operation is not consider completed, even if leader has the data. The client can do a retry. But the leader may need to block other write requests until it sees a follower alive. With synch, the data is also consistent with the leader. The figure above illustrates a synchronous mechanism for client - leader and first follower to respond, and asynchronous for the other followers. It is important to note that asynch replica may be the only possible solution when the number of followers is important or when they are separated by long distance with high latency networking communication. Because a full synchronous mechanism will block the write operations in case of a follower failure and will not be reliable. With asynch, two reads at the same time on the leader and one of the follower will not get the same results. This inconsistency is temporal: when there is no more write then the followers will become eventually consistent. This elapse time is called replication lag. This lag can have some interesting derived issues, like seeing data in one query (done on a follower with small lag) and then not seeing the data from the same query done on a bigger lagged follower. To avoid that, the pratice is to use a monolitic read: user makes several reads in sequence, they will not see time go backward. Which can be achieved by ensuring the reads for a given user are done on the same follower. This is what kafka does by assigning a consumer to partition. With asynch replica, if the leader fails, data not yet replicated to the followers, is lost until a new leader starts to accept new writes. Also adding a new follower brings other challenges: as the data are continuously being written to the leader database, copying the database files to the new follower will not be consistent. One adopted solution is to snapshot the database, without locking write operations, and copy the data to the follower, then from this snapshot, consumes the update log. To work the snapshot position needs to be known in the replication log. To support follower failure, the log needs to keep position of the last commited read, so when the follower restarts, it can load data from this position in the log. When the leader fails, a follower needs to take the leadership, and then other followers need to get data from the new leader. Kafka, combined with zookeeper, uses those last two mechanisms to support replication. Selecting the new leader is based on consensus algorithm, to ensure minimum data lost. When using single leader, it is possible to reach a split brain state, when the old leader comes back to live, and thinks it is still a leader: both leaders accept writes which leads to data corruption. Most RDBMS uses the master-slave pattern combined with asynchronous replication. Postgresql uses a file based log shipped to the followers when a transaction is committed. Logical replication starts by copying a snapshot of the data on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data changes in the order in which commits were made on the publisher side so that transactional consistency is guaranteed for the publications within any single subscription. DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note that source and target data structures have to be the same, and change to the table structure is a challenge but can be addressed (usually mitigated by 3 rd party tooling). Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable solution for microservice coexistance as there is a need to do data transformation on the fly.","title":"Single leader"},{"location":"data/data-replication/#multi-leaders","text":"With multi-leader configuration each leader gets write operations, and propagates data update notifications to all nodes. This is an active-active replication. And it is the practice when dealing with multiple datacenters. The write operations are propagated asynchronously which is more permissive to network failure. It is important to note that the following problems need to be addressed with this topology: Update of the same data at the same time, leading to write conflict Using automatic primary key generation by the database may create duplicate keys Triggers that work on conditions on the data to do something could never be triggered due to data conditions that will never happen in this active - active configuration. Integrity constraints can be violated, as one records may be processed without the others being yet present. For the write conflict resolution, there are different strategies, one uses timestamps so the last write wins the record update, but this could lead to data lost. In fact conflict resolution will be dependant of the business logic and data knowledge. So custom code needs to be done to apply this logic to write conflicts. When there is more than two leaders, the replicas topology can be use a ring, star or all-to-all model, as illustrated in the figure below: With Ring , a leader forwards and propagates its own writes to one neighbor. With Star one designated root node forwards writes to all of the other nodes. Finally with All-to-all , every leader sends its writes to every other leader. With Ring and Star, a node failure impacts the data replication to any node, and with all-to-all we need to address looping on the same write. The mitigation is to use a unique identifier for each node, so a write event coming with the same node_id at the current node id is discarded.","title":"Multi leaders"},{"location":"data/data-replication/#leaderless","text":"In this last replication technique, the client application is doing write operation on any replicas. There is no leader. This is the approach used by Cassandra and Dynamo system. The write operations order is not maintained. With leaderless failover does not exist, in case of node failure the client has to accept n missing acknowledges. If the client read back data from a previously failed node, that just restarted, it may read old / stale data. To mitigate this problem, reads are done to multiple nodes in parallel, and the client needs to consolidate the returned value (may be using timestamp or version number). A node that is restarting can catch up on the replicated data by doing read repair or by using an anti-entropy process. Read repair, is done by the client seeing old value from one of the replica, and push back the new value to it. The anti-entropy process is in the database mechanism, with a daemon process running to replicate missing data to any replicas.","title":"Leaderless"},{"location":"data/data-replication/#cap-theorem","text":"Data replication in distributed computing, like the cloud, falls into the problem of the CAP theorem where only two of three properties consisting of Consistency, Availability and Partition Tolerance can be met simultaneously. In most microservice implementation context, Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off. The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions.","title":"CAP Theorem"},{"location":"data/data-replication/#considerations","text":"The main advantages for data replication (providing a consistent copy of data across data storage locations) are: increased data availability increased access performance by moving data close to users decreased network load Maintaining Data consistency at all different sites involves complex processes, and increase physical resources.","title":"Considerations"},{"location":"data/data-replication/#business-motivations","text":"Among the top three priority of data integration and integrity solution, for 2019 and 2020, are ( source IDC ): Data intelligence (57%) Data replication (50%) Application data sync (51%) Two important derived use cases: Business in Real Time : Detect and react to data events as they happen to drive the business, and propagate those changes for others for consumption Optimize decision to sub-second latency level, i.e. real time analytics Always On Information High availability with Active-Standby and Active-Active data replication deployments Data synchronization for zero down time data migrations and upgrades","title":"Business motivations"},{"location":"data/data-replication/#technology-overview","text":"","title":"Technology overview"},{"location":"data/data-replication/#data-lake-technologies","text":"There are multiple alternatives to build a data lake solution. In the field, the following technologies are usually used: Hadoop, Apache Spark, cloud object storage, and Apache Kafka. Hadoop/HDFS (Hadoop File System) is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that is changing at the file level. With Hadoop, you don\u2019t change a record in a file. Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively scalable processing. Apache Spark is a data processing engine tightly coupled with the Hadoop ecosystem. It is one of the most active open source projects ever. Spark is mainly used as ETL engine capable of integrating data from various SQL and NoSQL data sources and targets including RDBMS, NoSQL DB, Cloud Object Store, HDFS and other cluster file systems COS (Cloud Object Storage) in conjunction with a Query Engine like Apache SparkSQL or IBM Cloud SQLQuery Service is a legit data lake solution, especially because, as a managed service, it provides unlimited capacity and very high scalability and robustness against failures Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data storage may be self described JSON document wrapped in Apache Avro binary format. Kafka exploits the scalability and availability of inexpensive commodity hardware. Although Kafka supports persisting data in queues for weeks or even months, it's not yet a proved technology for long term storage, even if companies are already adopting it for event sourcing. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both Hadoop and Kafka. There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics. If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files. Data stored in Kafka can be consumed by real time microservices and real time analytics engines. Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feeding data into Kafka) and data consumers (pulling data from Kafka), all with self managed storage. Other use cases are related to auditing and historical query on what happened on specific records. Using event sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in Elasticsearch, Apache Solr, etc...","title":"Data lake technologies"},{"location":"data/data-replication/#change-data-capture-cdc","text":"Another important part of the architecture is the change data capture component. The following diagram presents a generic architecture for real time data replication, using transaction logs as source for data update, a change data capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to persist to the target destination. The data replication between databases by continuously propagate changes in real time instead of doing it by batch with traditional ETL products, brings data availability and consistency cross systems. It can be used to feed analytics system and data warehouses, for business intelligence activities. For example, IBM's InfoSphere Data Replication (IIDR) captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and synchronization between sources and targets. The architecture diagram below presents the components involved in CDC replication: Product explanations can be obtained here. We can combine Kafka and IIDR to support a flexible pub sub architecture for data replication where databases are replicated but event streams about those data can be processed in real time by any applications and microservices. This is known as lambda-architecture . The combined architecture of a deployed solution looks like in the diagram below: With the management console, the developer can define a data replication project that can include one to many subscriptions. Subscriptions define the source database and tables and target kafka cluster and topics. The Kafka cluster can run on Kubernetes. The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. When running a subscription the first time, Kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. For more details about this solution see this product tour .","title":"Change data capture (CDC)"},{"location":"data/data-replication/#debezium","text":"Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use Kafka as backbone, and Kafka connect. It uses the approach of replicating one table to one Kafka topic. It can be used for data synchronization between microservices using CDC at a service level and propagate changes via Kafka. The implementation of the CQRS pattern may be simplified with this capability.","title":"Debezium"},{"location":"data/data-replication/#why-adopting-kafka-for-data-replication","text":"Using Kafka as a integration layer brings the following advantages: Offload processing Data aggregation from multiple sources Deliver a common platform for staging to other data consumers Provide a storage system for duplicating data Buffer unprocessed messages Offers throughput and low end-to-end Latency Offers real time processing and retrospective analysis Can correlate streaming feeds of disparate formats Flexibility of input source and output targets Built in stream processing API on real time feeds with Kafka streams Commit Log Fault tolerance, scalability, multi-tenancy, speed, light-weight, multiple landing-zones.","title":"Why adopting Kafka for data replication"},{"location":"data/data-replication/#kafka-connect","text":"Kafka connect simplifies the integration between Kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed, fault tolerant runtime able to easily scale horizontally. The set of connectors help developers to not re-implement consumers and producers for every type of data source. To get started please read this introduction from the product documentation. The Kafka connect workers are stateless and can run easily on Kubernetes or as standalone docker process. Kafka Connect Source is to get data to Kafka, and Kafka Connect Sink to get data out of Kafka. A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements define a task. A connector can have multiple tasks. In distributed deployment, the connector supports scaling by adding new workers and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST API.","title":"Kafka connect"},{"location":"data/data-replication/#recommended-readings","text":"IBM InfoSphere Data Replication Product Tour Kafka connect hands-on learning from St\u00e9phane Maarek Integrating IBM CDC Replication Engine with kafka Very good article from Brian Storti on data replication and consistency PostgreSQL warm standby replication mechanism Change Data Capture in PostgreSQL eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition Weighted quorum mechanism for cluster to select primary node Using Kafka Connect as a CDC solution Debezium tutorial Ensure data resilience - author: Neal Fishman","title":"Recommended Readings"},{"location":"data/example/","text":"Requirements We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in a Kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server performance with additional queries Address a writing model, where cloud native apps have to write back changes to the legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build.","title":"Example"},{"location":"data/example/#requirements","text":"We want to support the following requirements: Keep a RDBMS database like DB2 on the mainframe where transactions are supported Add cloud native applications in a Kubernetes environment, with a need to read data coming from the legacy DB, without impacting the DB server performance with additional queries Address a writing model, where cloud native apps have to write back changes to the legacy DB Replicate data in real time to data lake or cloud based data store. Replicated data for multiple consumers There are a lot of products which are addressing those requirements, but here we address the integration with Kafka for a pub/sub and event store need. The previous diagram may illustrate what we want to build.","title":"Requirements"},{"location":"data/cassandra/readme/","text":"Cassandra Summary In this article we are a quick summar of Cassandra capabilities in the context of data persistence and how to deploy it on Openshift. Cassandra addresses linear scalability and high availability to persist a huge data set. It uses replication to multiple nodes managed in cluster, even deployed cross data centers. Concepts Cassandra uses a ring based Distributed Hash Table servers but without finger or routing tables. Keys are stored as in DHT to the next server with key > key id, replicated on the 2 next servers too. There is one ring per data center. The coordinator forward the query to a subset of replicas for a particular key. Every server that could be a coordinator needs to know the 'key to server' assignment. Two data placement strategies: simple strategy: use two kinds of partitioners: random, which does chord like hashing byte ordered, which assigns range of keys to servers: easier for range queries network topology strategy for multiple data centers: it supports different configuration, like 2 replicas of each key per decision center. First replica is placed according to the Partitioner logic, and make sure to store the other replica to different rack, to avoid a rack failure will make all copies of key not available. Go around the ring clockwise until encountering a server in a different rack. Snitches: is a mechanism to map ip addresses to racks in DC. Cassandra support such configuration. Client sends write to one coordinator node in Cassandra cluster. Coordinator may per key or per client or per query. It uses partitioner to send query to all replica nodes responsible for key. The process to write should be fast and not involving lock on resource. It should not involve read and disk seeks. Write operations are always successful, even in case of failure: the coordinator uses the Hinted Handoff mechanism (it assumes ownership of the key until being sure it is supported by the replica), as it writes to other replicas and keeps the write locally until the down replica comes back up. If all the replicas are done, the Coordinator buffers writes for few hours. Here are some key concepts of Cassandra to keep in mind for this implementation: Cluster : the set of nodes potentially deployed cross data centers, organized as a 'ring'. Keyspace : like a schema in SQL DB. It is the higher abstraction object to contain data. The important keyspace attributes are the Replication Factor, the Replica Placement Strategy and the Column Families. Column Family : they are like tables in Relational Databases. Each Column Family contains a collection of rows which are represented by a Map >. The key gives the ability to access related data together Column \u2013 A column is a data structure which contains a column name, a value and a timestamp. The columns and the number of columns in each row may vary in contrast with a relational database where data are well structured. Cassandra deployment Deploying on \"Docker Edge for desktop\" kubernetes For development work we use the Docker Edge version of Docker and enable Kubernetes (see this note from docker web site ). Then you can use our script deployCassandra.sh under the scripts folder BUT be sure to adapt the yaml settings in the folder deployments/cassandra to limit the replicas for what you need. The steps are the same as for ICP deployment so see next section. Deployment on Kubernetes Deploying stateful distributed applications like Cassandra is not easy. You will leverage the kubernetes cluster to support high availability and deploy c7a to the worker nodes. We also recommend to be familiar with this kubernetes tutorial on how to deploy Cassandra with Stateful Sets . Performance considerations The resource requirements for higher performance c7a node are: A minimum of 32GB RAM (JVM + Linux memory buffers to cache SSTable) + memory.available defined in Kubernetes Eviction policy + Resources needed by k8s components that run on every worker node (e.g. proxy) A minimum of 8 processor cores per Cassandra Node with 2 CPUs per core (16 vCPU in a VM) 4-8 GB JVM heap, recommend trying to keep heaps limited to 4 GB to minimize garbage collection pauses caused by large heaps. Cassandra needs local storage to get best performance. Avoid to use distributed storage, and prefer hostPath or localstorage. With distributed storage like a Glusterfs cluster you may have 9 replicas (3x Cassandra replica factor which is usually 3) Cassandra nodes tend to be IO bound rather than CPU bound: Upper limit of data per node <= 1.5 TB for spinning disk and <= 4 TB for SSD Increase the number of nodes to keep the data per node at or below the recommended capacity Actual data per node determined by data throughput, for high throughput need to limit the data per node. The use of Vnodes is generally considered to be a good practice as they eliminate the need to perform manual token assignment, distribute workload across all nodes in a cluster when nodes are added or removed. It helps rebuilding dead nodes faster. Vnode reduces the size of SSTables which can improve read performance. Cassandra best practices set the number of tokens per Cassandra node to 256. Avoid getting multiple node instances on the same physical host, so use podAntiAffinity in the StatefulSet spec. spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : topologyKey : \"kubernetes.io/hostname\" Using our yaml configurations You can reuse the yaml config files under deployment/cassandra folder to configure a Service to expose Cassandra externally, create static persistence volumes, and use the StatefulSet to deploy Cassandra image. The steps to deploy to ICP are: Connect to ICP. You may want to get the admin security token using the Admin console and the script: scripts/connectToCluster.sh . We are using one namespace called 'greencompute'. You can also use our script deployCassandra.sh under the ../scripts folder to automate this deployment. Create Cassandra headless service, so application accesses it via KubeDNS. If you do wish to connect an application to cassandra, use the KubeDNS value of cassandra-svc.greencompute.svc.cluster.local , or cassandra-svc or cassandra-0 . The alternate solution is to use Ingress rule and set a hostname as cassandra.green.case. The casssandara-ingress.yml file defines such Ingress. $ kubectl apply -f deployment/cassandra/cassandra-service.yaml --namespace greencompute $ kubectl get svc cassandra-svc -n greencompute NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE cassandra-svc ClusterIP None <none> 9042 /TCP 12h Create static persistence volumes to keep data for cassandra: you need the same number of PV as there are cassandra nodes (here 3 nodes) $ kubectl apply -f deployment/cassandra/cassandra-volumes.yaml $ kubectl get pv -n greencompute | grep cassandra cassandra-data-1 1Gi RWO Recycle Bound greencompute/cassandra-data-cassandra-0 12h cassandra-data-2 1Gi RWO Recycle Available 12h cassandra-data-3 1Gi RWO Recycle Available Create the StatefulSet, which defines a cassandra ring of 3 nodes. The cassandra image used is coming from dockerhub public repository. If you are using your own namespace name or you change the service name, modify the service name and namespace used in the yaml : env : - name : CASSANDRA_SEEDS value : cassandra-0.cassandra-svc.greencompute.svc.cluster.local Cassandra seed is used for two purposes: Node discovery: when a new cassandra node is added (which means when deployed on k8s, a new pod instance added by increasing the replica), it needs to find the cluster, so here it is set the svc Assist on gossip convergence: by having all of the nodes in the cluster gossip regularly with the same set of seeds. It ensures changes are propagated regularly. Here it needs to reference the headless service we defined for Cassandra deployment. $ kubectl apply -f deployment/cassandra/cassandra-statefulset.yaml -n greencompute $ kubectl get statefulset -n greencompute NAME DESIRED CURRENT AGE cassandra 1 1 12h Connect to the pod to assess the configuration is as expected. $ kubectl get pods -o wide -n greencompute NAME READY STATUS RESTARTS AGE IP NODE cassandra-0 0/1 Running 0 2m 192.168.35.93 169.61.151.164 $ kubectl exec -tin greencompute cassandra-0 -- nodetool status Datacenter: DC1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 192.168.212.174 257.29 KiB 256 100.0% ea8acc49-1336-4941-b122-a4ef711ca0e6 Rack1 The string \"UN\", means for Up and Normal state. Removing cassandra cluster We are providing a script for that ./scripts/deleteCassandra.sh which remove the stateful, the pv, pvc and service grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\ && kubectl delete statefulset -l app=cassandra -n greencompute \\ && echo \"Sleeping $grace\" \\ && sleep $grace \\ && kubectl delete pvc,pv,svc -l app=cassandra High availability Within a cluster the number of replicas in the statefulset is at least 3 but can be increased to 5 when code maintenance is needed. The choice for persistence storage is important, and the backup and restore strategy of the storage area network used. When creating connection to persist data into a keyspace, you specify the persistence strategy and number of replicas at the client code level. Mostly using properties file. p.setProperty(CASSANDRA_STRATEGY, \"SimpleStrategy\"); p.setProperty(CASSANDRA_REPLICAS, \"1\"); When deploying on Staging or test cluster, ring topology, SimpleStrategy is enough: additional replicas are placed on the next nodes in the ring moving clockwise without considering topology, such as rack or datacenter location. For HA and production deployment NetworkTopologyStrategy is needed: replicas are placed in the same datacenter but on distinct racks. For the number of replicas, it is recommended to use 3 per datacenter. The spec.env parameters in the statefulset defines the datacenter name and rack name too. Code We have done two implementations for persisting asset data into Cassandra, one using Cassandra client API and one with SpringBoot cassandra repository API. Cassandra client API The code is under refarch-reefer-ml project and can be loaded into Eclipse. This component is deployed as container inside a kubernetes cluster like Openshift. See code explanation, how to build and run in this note In the pom.xml we added the following dependencies to get access to the core driver API: <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> 3.1.4 </version> </dependency> The DAO code is CassandraRepo.java and it basically connects to the Cassandra cluster when the DAO class is created... Builder b = Cluster.builder().addContactPoints(endpoints); cluster = b.build(); session = cluster.connect(); The trick is in the endpoints name. We externalize this setting in a configuration properties and use the cassandra-svc name when deploy in ICP. It is possible also to create keyspace and tables by API if they do not exist by building CQL query string and use the session.execute(aquery) method. See this section below Define Assets Table Structure with CQL Using the csql tool we can create space and table. To use cqlsh connect to cassandra pod: $ kubectl exec -tin greencompute cassandra-0 cqlsh You are now in cqlsh shell and you can define assets table under keyspace assetmonitoring : sqlsh > create keyspace assetmonitoring with replication = { 'class':'SimpleStrategy', 'replication_factor':1 } ; sqlsh > use assetmonitoring ; sqlsh : assetmonitoring > create TABLE assets ( id text PRIMARY KEY , os text , type text , ipaddress text , version text , antivirus text , current double , rotation int , pressure int , temperature int , latitude double , longitude double ); Add an index on the asset operating system field and one on type. CREATE INDEX ON assetmonitoring.assets (os); CREATE INDEX ON assetmonitoring.assets (type); If you reconnect to the pod using cqlsh you can assess the table using describe tables describe assets Some useful CQL commands # See the table schema cqlsh> describe table assets; # modify a table structure adding a column cqlsh> alter table assets add flowRate bigint; # change column type. example the name column: cqlsh> alter table assets alter name type text; # list content of a table cqlsh> select id,ipaddress,latitude,longitude from assets; # delete a table cqlsh> drop table if exists assets; Use Cassandra Java API to create objects Create keyspace: StringBuilder sb = new StringBuilder ( \"CREATE KEYSPACE IF NOT EXISTS \" ) . append ( keyspaceName ). append ( \" WITH replication = {\" ) . append ( \"'class':'\" ). append ( replicationStrategy ) . append ( \"','replication_factor':\" ). append ( replicationFactor ) . append ( \"};\" ); String query = sb . toString (); session . execute ( query ); Create table StringBuilder sb = new StringBuilder ( \"CREATE TABLE IF NOT EXISTS \" ) . append ( TABLE_NAME ). append ( \"(\" ) . append ( \"id uuid PRIMARY KEY, \" ) . append ( \"temperature text,\" ) . append ( \"latitude text,\" ) . append ( \"longitude text);\" ); String query = sb . toString (); session . execute ( query ); insert data: there is no update so if you want a strict insert you need to add \"IF NOT EXISTS\" condition in the query. Issues When having the cassandra replica set to more than two, the cassandra operations are not happenning parallely on both the pods at a time. Some operations are happenning on one node and some others on the other node. Due to this, inconsistent data is retrieved since both the tables doesnot have the same data. Future Readings 10 steps to set up a multi-data center Cassandra cluster on a Kubernetes platform IBM Article: Scalable multi-node Cassandra deployment on Kubernetes Cluster Running Cassandra on Kubernetes","title":"Cassandra practices"},{"location":"data/cassandra/readme/#cassandra-summary","text":"In this article we are a quick summar of Cassandra capabilities in the context of data persistence and how to deploy it on Openshift. Cassandra addresses linear scalability and high availability to persist a huge data set. It uses replication to multiple nodes managed in cluster, even deployed cross data centers.","title":"Cassandra Summary"},{"location":"data/cassandra/readme/#concepts","text":"Cassandra uses a ring based Distributed Hash Table servers but without finger or routing tables. Keys are stored as in DHT to the next server with key > key id, replicated on the 2 next servers too. There is one ring per data center. The coordinator forward the query to a subset of replicas for a particular key. Every server that could be a coordinator needs to know the 'key to server' assignment. Two data placement strategies: simple strategy: use two kinds of partitioners: random, which does chord like hashing byte ordered, which assigns range of keys to servers: easier for range queries network topology strategy for multiple data centers: it supports different configuration, like 2 replicas of each key per decision center. First replica is placed according to the Partitioner logic, and make sure to store the other replica to different rack, to avoid a rack failure will make all copies of key not available. Go around the ring clockwise until encountering a server in a different rack. Snitches: is a mechanism to map ip addresses to racks in DC. Cassandra support such configuration. Client sends write to one coordinator node in Cassandra cluster. Coordinator may per key or per client or per query. It uses partitioner to send query to all replica nodes responsible for key. The process to write should be fast and not involving lock on resource. It should not involve read and disk seeks. Write operations are always successful, even in case of failure: the coordinator uses the Hinted Handoff mechanism (it assumes ownership of the key until being sure it is supported by the replica), as it writes to other replicas and keeps the write locally until the down replica comes back up. If all the replicas are done, the Coordinator buffers writes for few hours. Here are some key concepts of Cassandra to keep in mind for this implementation: Cluster : the set of nodes potentially deployed cross data centers, organized as a 'ring'. Keyspace : like a schema in SQL DB. It is the higher abstraction object to contain data. The important keyspace attributes are the Replication Factor, the Replica Placement Strategy and the Column Families. Column Family : they are like tables in Relational Databases. Each Column Family contains a collection of rows which are represented by a Map >. The key gives the ability to access related data together Column \u2013 A column is a data structure which contains a column name, a value and a timestamp. The columns and the number of columns in each row may vary in contrast with a relational database where data are well structured.","title":"Concepts"},{"location":"data/cassandra/readme/#cassandra-deployment","text":"","title":"Cassandra deployment"},{"location":"data/cassandra/readme/#deploying-on-docker-edge-for-desktop-kubernetes","text":"For development work we use the Docker Edge version of Docker and enable Kubernetes (see this note from docker web site ). Then you can use our script deployCassandra.sh under the scripts folder BUT be sure to adapt the yaml settings in the folder deployments/cassandra to limit the replicas for what you need. The steps are the same as for ICP deployment so see next section.","title":"Deploying on \"Docker Edge for desktop\" kubernetes"},{"location":"data/cassandra/readme/#deployment-on-kubernetes","text":"Deploying stateful distributed applications like Cassandra is not easy. You will leverage the kubernetes cluster to support high availability and deploy c7a to the worker nodes. We also recommend to be familiar with this kubernetes tutorial on how to deploy Cassandra with Stateful Sets .","title":"Deployment on Kubernetes"},{"location":"data/cassandra/readme/#performance-considerations","text":"The resource requirements for higher performance c7a node are: A minimum of 32GB RAM (JVM + Linux memory buffers to cache SSTable) + memory.available defined in Kubernetes Eviction policy + Resources needed by k8s components that run on every worker node (e.g. proxy) A minimum of 8 processor cores per Cassandra Node with 2 CPUs per core (16 vCPU in a VM) 4-8 GB JVM heap, recommend trying to keep heaps limited to 4 GB to minimize garbage collection pauses caused by large heaps. Cassandra needs local storage to get best performance. Avoid to use distributed storage, and prefer hostPath or localstorage. With distributed storage like a Glusterfs cluster you may have 9 replicas (3x Cassandra replica factor which is usually 3) Cassandra nodes tend to be IO bound rather than CPU bound: Upper limit of data per node <= 1.5 TB for spinning disk and <= 4 TB for SSD Increase the number of nodes to keep the data per node at or below the recommended capacity Actual data per node determined by data throughput, for high throughput need to limit the data per node. The use of Vnodes is generally considered to be a good practice as they eliminate the need to perform manual token assignment, distribute workload across all nodes in a cluster when nodes are added or removed. It helps rebuilding dead nodes faster. Vnode reduces the size of SSTables which can improve read performance. Cassandra best practices set the number of tokens per Cassandra node to 256. Avoid getting multiple node instances on the same physical host, so use podAntiAffinity in the StatefulSet spec. spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : topologyKey : \"kubernetes.io/hostname\"","title":"Performance considerations"},{"location":"data/cassandra/readme/#using-our-yaml-configurations","text":"You can reuse the yaml config files under deployment/cassandra folder to configure a Service to expose Cassandra externally, create static persistence volumes, and use the StatefulSet to deploy Cassandra image. The steps to deploy to ICP are: Connect to ICP. You may want to get the admin security token using the Admin console and the script: scripts/connectToCluster.sh . We are using one namespace called 'greencompute'. You can also use our script deployCassandra.sh under the ../scripts folder to automate this deployment. Create Cassandra headless service, so application accesses it via KubeDNS. If you do wish to connect an application to cassandra, use the KubeDNS value of cassandra-svc.greencompute.svc.cluster.local , or cassandra-svc or cassandra-0 . The alternate solution is to use Ingress rule and set a hostname as cassandra.green.case. The casssandara-ingress.yml file defines such Ingress. $ kubectl apply -f deployment/cassandra/cassandra-service.yaml --namespace greencompute $ kubectl get svc cassandra-svc -n greencompute NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE cassandra-svc ClusterIP None <none> 9042 /TCP 12h Create static persistence volumes to keep data for cassandra: you need the same number of PV as there are cassandra nodes (here 3 nodes) $ kubectl apply -f deployment/cassandra/cassandra-volumes.yaml $ kubectl get pv -n greencompute | grep cassandra cassandra-data-1 1Gi RWO Recycle Bound greencompute/cassandra-data-cassandra-0 12h cassandra-data-2 1Gi RWO Recycle Available 12h cassandra-data-3 1Gi RWO Recycle Available Create the StatefulSet, which defines a cassandra ring of 3 nodes. The cassandra image used is coming from dockerhub public repository. If you are using your own namespace name or you change the service name, modify the service name and namespace used in the yaml : env : - name : CASSANDRA_SEEDS value : cassandra-0.cassandra-svc.greencompute.svc.cluster.local Cassandra seed is used for two purposes: Node discovery: when a new cassandra node is added (which means when deployed on k8s, a new pod instance added by increasing the replica), it needs to find the cluster, so here it is set the svc Assist on gossip convergence: by having all of the nodes in the cluster gossip regularly with the same set of seeds. It ensures changes are propagated regularly. Here it needs to reference the headless service we defined for Cassandra deployment. $ kubectl apply -f deployment/cassandra/cassandra-statefulset.yaml -n greencompute $ kubectl get statefulset -n greencompute NAME DESIRED CURRENT AGE cassandra 1 1 12h Connect to the pod to assess the configuration is as expected. $ kubectl get pods -o wide -n greencompute NAME READY STATUS RESTARTS AGE IP NODE cassandra-0 0/1 Running 0 2m 192.168.35.93 169.61.151.164 $ kubectl exec -tin greencompute cassandra-0 -- nodetool status Datacenter: DC1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 192.168.212.174 257.29 KiB 256 100.0% ea8acc49-1336-4941-b122-a4ef711ca0e6 Rack1 The string \"UN\", means for Up and Normal state.","title":"Using our yaml configurations"},{"location":"data/cassandra/readme/#removing-cassandra-cluster","text":"We are providing a script for that ./scripts/deleteCassandra.sh which remove the stateful, the pv, pvc and service grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\ && kubectl delete statefulset -l app=cassandra -n greencompute \\ && echo \"Sleeping $grace\" \\ && sleep $grace \\ && kubectl delete pvc,pv,svc -l app=cassandra","title":"Removing cassandra cluster"},{"location":"data/cassandra/readme/#high-availability","text":"Within a cluster the number of replicas in the statefulset is at least 3 but can be increased to 5 when code maintenance is needed. The choice for persistence storage is important, and the backup and restore strategy of the storage area network used. When creating connection to persist data into a keyspace, you specify the persistence strategy and number of replicas at the client code level. Mostly using properties file. p.setProperty(CASSANDRA_STRATEGY, \"SimpleStrategy\"); p.setProperty(CASSANDRA_REPLICAS, \"1\"); When deploying on Staging or test cluster, ring topology, SimpleStrategy is enough: additional replicas are placed on the next nodes in the ring moving clockwise without considering topology, such as rack or datacenter location. For HA and production deployment NetworkTopologyStrategy is needed: replicas are placed in the same datacenter but on distinct racks. For the number of replicas, it is recommended to use 3 per datacenter. The spec.env parameters in the statefulset defines the datacenter name and rack name too.","title":"High availability"},{"location":"data/cassandra/readme/#code","text":"We have done two implementations for persisting asset data into Cassandra, one using Cassandra client API and one with SpringBoot cassandra repository API.","title":"Code"},{"location":"data/cassandra/readme/#cassandra-client-api","text":"The code is under refarch-reefer-ml project and can be loaded into Eclipse. This component is deployed as container inside a kubernetes cluster like Openshift. See code explanation, how to build and run in this note In the pom.xml we added the following dependencies to get access to the core driver API: <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> 3.1.4 </version> </dependency> The DAO code is CassandraRepo.java and it basically connects to the Cassandra cluster when the DAO class is created... Builder b = Cluster.builder().addContactPoints(endpoints); cluster = b.build(); session = cluster.connect(); The trick is in the endpoints name. We externalize this setting in a configuration properties and use the cassandra-svc name when deploy in ICP. It is possible also to create keyspace and tables by API if they do not exist by building CQL query string and use the session.execute(aquery) method. See this section below","title":"Cassandra client API"},{"location":"data/cassandra/readme/#define-assets-table-structure-with-cql","text":"Using the csql tool we can create space and table. To use cqlsh connect to cassandra pod: $ kubectl exec -tin greencompute cassandra-0 cqlsh You are now in cqlsh shell and you can define assets table under keyspace assetmonitoring : sqlsh > create keyspace assetmonitoring with replication = { 'class':'SimpleStrategy', 'replication_factor':1 } ; sqlsh > use assetmonitoring ; sqlsh : assetmonitoring > create TABLE assets ( id text PRIMARY KEY , os text , type text , ipaddress text , version text , antivirus text , current double , rotation int , pressure int , temperature int , latitude double , longitude double ); Add an index on the asset operating system field and one on type. CREATE INDEX ON assetmonitoring.assets (os); CREATE INDEX ON assetmonitoring.assets (type); If you reconnect to the pod using cqlsh you can assess the table using describe tables describe assets","title":"Define Assets Table Structure with CQL"},{"location":"data/cassandra/readme/#some-useful-cql-commands","text":"# See the table schema cqlsh> describe table assets; # modify a table structure adding a column cqlsh> alter table assets add flowRate bigint; # change column type. example the name column: cqlsh> alter table assets alter name type text; # list content of a table cqlsh> select id,ipaddress,latitude,longitude from assets; # delete a table cqlsh> drop table if exists assets;","title":"Some useful CQL commands"},{"location":"data/cassandra/readme/#use-cassandra-java-api-to-create-objects","text":"Create keyspace: StringBuilder sb = new StringBuilder ( \"CREATE KEYSPACE IF NOT EXISTS \" ) . append ( keyspaceName ). append ( \" WITH replication = {\" ) . append ( \"'class':'\" ). append ( replicationStrategy ) . append ( \"','replication_factor':\" ). append ( replicationFactor ) . append ( \"};\" ); String query = sb . toString (); session . execute ( query ); Create table StringBuilder sb = new StringBuilder ( \"CREATE TABLE IF NOT EXISTS \" ) . append ( TABLE_NAME ). append ( \"(\" ) . append ( \"id uuid PRIMARY KEY, \" ) . append ( \"temperature text,\" ) . append ( \"latitude text,\" ) . append ( \"longitude text);\" ); String query = sb . toString (); session . execute ( query ); insert data: there is no update so if you want a strict insert you need to add \"IF NOT EXISTS\" condition in the query.","title":"Use Cassandra Java API to create objects"},{"location":"data/cassandra/readme/#issues","text":"When having the cassandra replica set to more than two, the cassandra operations are not happenning parallely on both the pods at a time. Some operations are happenning on one node and some others on the other node. Due to this, inconsistent data is retrieved since both the tables doesnot have the same data.","title":"Issues"},{"location":"data/cassandra/readme/#future-readings","text":"10 steps to set up a multi-data center Cassandra cluster on a Kubernetes platform IBM Article: Scalable multi-node Cassandra deployment on Kubernetes Cluster Running Cassandra on Kubernetes","title":"Future Readings"},{"location":"deployment/","text":"Deployment Model deployment comes in many shapes. The key to everything is that the business insights that result from the model are made available to stakeholders. This can happen in various ways. At the simplest level a PDF report is generated (e.g. using a jupyter notebook in Watson Studio) and handed over to business stakeholders. Alternatively, the model is encapsulated behind a REST API and made either available to be consumed by a microservice or a data integration component. Model are exposed via API (e.g. by using IBM Watson Machine Learning or Fabric for DeepLearning) or can be embbeded in agent consuming real time event as demonstrated in this project . Depending on your use case, please choose and implement an appropriate model deployment option and justify your decisions in the architecture decision document.","title":"Physical model execution"},{"location":"deployment/#deployment","text":"Model deployment comes in many shapes. The key to everything is that the business insights that result from the model are made available to stakeholders. This can happen in various ways. At the simplest level a PDF report is generated (e.g. using a jupyter notebook in Watson Studio) and handed over to business stakeholders. Alternatively, the model is encapsulated behind a REST API and made either available to be consumed by a microservice or a data integration component. Model are exposed via API (e.g. by using IBM Watson Machine Learning or Fabric for DeepLearning) or can be embbeded in agent consuming real time event as demonstrated in this project . Depending on your use case, please choose and implement an appropriate model deployment option and justify your decisions in the architecture decision document.","title":"Deployment"},{"location":"deployment/model-evaluation/","text":"Model evaluation is a critical task in data science. This is one of the few measures business stakeholders are interested in. Model performance heavily influences business impact of a data science project. Therefore, it is important take some time apart in an independent task in the process model. So how are models evaluated? In supervised machine learning this is relatively straightforward since you can always create a ground truth and compare your results against ground truth. So, we are either splitting data into training-, test- and validation-sets to assess model performance on the test set or we use cross validation. This all is explained in the following coursera course https://www.coursera.org/learn/advanced-machine-learning-signal-processing/ Week 2. In case we know what data set we can use as ground truth in supervised learning (classification and regression) we need to define a different measure for evaluation than in unsupervised learning (clustering). Since it depends on the type of model we create, the following none exhaustive lists can be used as a starting point for further research: Classification: Confusion Matrix Accuracy Precision Recall Specificity True positive rate True negative rate False positive rate False negative rate F1-score Gain and Lift Kolomogorov Smirnov Area Under ROC Gini Coefficient Concordant \u2013 Discordant ratio Regression: Root Mean Squared Error (RMSE) Mean Squared Error Mean Absolute Error (MAE) R-Squared Relative Squared Error Relative Absolute Error Sum of Differences ACF plot of residuals Histogram of residuals Residual plots against predictors Residual plots against fitted values Clustering: Adjusted Rand index Mutual Information Homogeneity completeness V-measure Fowlkes-Mallows Silhouette Coefficient Calinski-Harabaz\u00b6 References: http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation Please choose at least one appropriate model performance measure, justify why you\u2019ve used it and document how iterative changes in the feature creation task influence it.","title":"Model evaluation"},{"location":"methodology/","text":"Develop Intelligent Application Methodology Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. The method supports lightweight development practices, to start the implementation of a MVP (Minimum Viable Product) , to support scaling-up the architecture and the complexity of an end-to-end integrated solution. This method always guarantees to have deployable components at any point in time, a concept known in CI/CD (Continuous Integration, Continuous Delivery) aapplied to microservices, integration, data and machine learning models. Integrating data - devops and AI-Analytics development practices Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application AI-Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. Personas We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role. Differences between analysts and data scientists The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89% DevOps lifecycle The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. \"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts. The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. We recommend reading this chapter on designing intelligent application to assess what needs to be done, and some best practices. Among the tasks described in the release and development iteration loops, we do need to cover each of them, but two specifics tasks are interesting to consider in this integrated methodology: the integrate services and the build components tasks. Integration service task has a blue border to demonstrate integration activities between the different lifecycles, like ML model integration. DataOps The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and this article , \"The evaluation of data\" , introduces to the concepts to recognize the value of data. The discover business objectives activity task, in fact group a set of different subjects depending of the context: data, integration, machine learning model. The goal is to highlight the measurable outcome expected by business stakeholders. This article ) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And this practice of translating a business problem into an AI and data science solution helps the analysis team to As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. As part of the gather data requirements , it is important to review the dimensions of value as introduced in the previous article and the formalize the value chain of the data in the scope of the project and address if the data contains the correct information to answer the business challenges and support the business process. Transform data for AI and Deploy data integration flow tasks have different border colors to demonstrate integration activities between the different lifecycles. MLOps lifecycle The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. Understanding the data and analytics goals task is explained in this note with the business analytic patterns. Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs. Further readings Integrating the cycles Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. Challenges There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets. The Garage Method for Cloud with DataFirst Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem. Compendium Assemble the team to support a data-driven project - author: Stacey Ronaghan The valuation of data - author: Neal Fishman Define business objectives - author: Neal Fishman Recognize the value of data - author: Neal Fishman Translate a business problem into an AI and Data Science solution - authors: Tommy Eunice, Edd Biddle, Paul Christensen Prepare your data for AI and data science - authors: Edd Biddle, Paul Christensen Define your data strategy -authors: Beth Ackerman, Paul Christensen Normalize data to its atomic level - author: Neal Fishman Understand data needs to support AI and Data Science solutions - authors: Tommy Eunice, Edd Biddle, Paul Christensen Run thought experiments by using hypothesis-driven analysis - author: Edd Biddle, Paul Christensen Deliver a singular data function - author: Neal Fishman Construct your data topology - authors: Neal Fishman, Paul Christensen Build your data lake design - author: Paul Christensen Put AI and data science to work in your organization - authors: Edd Biddle, Paul Christensen Look behind the curtain of AI - author: Edd Biddle Select and develop an AI and data science model - author: Edd Biddle Enhance and optimize your AI and data science models - author: Edd Biddle Establish data governance - author: Neal Fishman Deploy an AI model - author: Sujatha Perepa","title":"Integrated iterative approach"},{"location":"methodology/#develop-intelligent-application-methodology","text":"Abstract In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices. The method supports lightweight development practices, to start the implementation of a MVP (Minimum Viable Product) , to support scaling-up the architecture and the complexity of an end-to-end integrated solution. This method always guarantees to have deployable components at any point in time, a concept known in CI/CD (Continuous Integration, Continuous Delivery) aapplied to microservices, integration, data and machine learning models.","title":"Develop Intelligent Application Methodology"},{"location":"methodology/#integrating-data-devops-and-ai-analytics-development-practices","text":"Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary. To help achieve digital transformation, your organization should integrate three development lifecycles: Software/Application AI-Analytics Data Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value.","title":"Integrating data - devops and AI-Analytics development practices"},{"location":"methodology/#personas","text":"We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role.","title":"Personas"},{"location":"methodology/#differences-between-analysts-and-data-scientists","text":"The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects 54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects 89%","title":"Differences between analysts and data scientists"},{"location":"methodology/#devops-lifecycle","text":"The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop to continuously deliver application features to production (bigger loop). Notes Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion. \"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts. The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. We recommend reading this chapter on designing intelligent application to assess what needs to be done, and some best practices. Among the tasks described in the release and development iteration loops, we do need to cover each of them, but two specifics tasks are interesting to consider in this integrated methodology: the integrate services and the build components tasks. Integration service task has a blue border to demonstrate integration activities between the different lifecycles, like ML model integration.","title":"DevOps lifecycle"},{"location":"methodology/#dataops","text":"The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and this article , \"The evaluation of data\" , introduces to the concepts to recognize the value of data. The discover business objectives activity task, in fact group a set of different subjects depending of the context: data, integration, machine learning model. The goal is to highlight the measurable outcome expected by business stakeholders. This article ) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And this practice of translating a business problem into an AI and data science solution helps the analysis team to As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work. As part of the gather data requirements , it is important to review the dimensions of value as introduced in the previous article and the formalize the value chain of the data in the scope of the project and address if the data contains the correct information to answer the business challenges and support the business process. Transform data for AI and Deploy data integration flow tasks have different border colors to demonstrate integration activities between the different lifecycles.","title":"DataOps"},{"location":"methodology/#mlops-lifecycle","text":"The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. Notes The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method. Understanding the data and analytics goals task is explained in this note with the business analytic patterns. Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs. Further readings","title":"MLOps lifecycle"},{"location":"methodology/#integrating-the-cycles","text":"Although the three lifecycles are independent, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment. The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data highlights integration and access paths to information. The intersection between data and analytics highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements. Another interesting view is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project manage efficiently to avoid teams waiting for others. Interface definitions and data schema are important elements to focus on as early as possible. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture. The data store integration addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench. Note The AI model as a service can be mocked-up behind Facade interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iteration and different focuses over time. Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc... The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different operate tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application. Notes The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling.","title":"Integrating the cycles"},{"location":"methodology/#challenges","text":"There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. How will the model be made available to developers? Is it a batch process updating/appending static records or real time processing on a data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model, data and code: version management / data lineage How to assess the features needed for the training sets.","title":"Challenges"},{"location":"methodology/#the-garage-method-for-cloud-with-datafirst","text":"Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"The Garage Method for Cloud with DataFirst"},{"location":"methodology/#compendium","text":"Assemble the team to support a data-driven project - author: Stacey Ronaghan The valuation of data - author: Neal Fishman Define business objectives - author: Neal Fishman Recognize the value of data - author: Neal Fishman Translate a business problem into an AI and Data Science solution - authors: Tommy Eunice, Edd Biddle, Paul Christensen Prepare your data for AI and data science - authors: Edd Biddle, Paul Christensen Define your data strategy -authors: Beth Ackerman, Paul Christensen Normalize data to its atomic level - author: Neal Fishman Understand data needs to support AI and Data Science solutions - authors: Tommy Eunice, Edd Biddle, Paul Christensen Run thought experiments by using hypothesis-driven analysis - author: Edd Biddle, Paul Christensen Deliver a singular data function - author: Neal Fishman Construct your data topology - authors: Neal Fishman, Paul Christensen Build your data lake design - author: Paul Christensen Put AI and data science to work in your organization - authors: Edd Biddle, Paul Christensen Look behind the curtain of AI - author: Edd Biddle Select and develop an AI and data science model - author: Edd Biddle Enhance and optimize your AI and data science models - author: Edd Biddle Establish data governance - author: Neal Fishman Deploy an AI model - author: Sujatha Perepa","title":"Compendium"},{"location":"methodology/MLops/","text":"The Machine Learning Development and Operations This part of the integrated method is aligned to the Pace-Layered Application Strategy and IT Organizational Design published by Garter in 2016. The MLOps method is tailored for the development of \"Systems of Innovation\" and composed of a process model to develop individual technology components based on a reference architecture and a architecture development framework for extending the Solution Architecture beyond the reference architecture. This way \"Systems of Innovation\" can be moved into \"Systems of Differentiation\" and \"Systems of Record\" in the long run. This method on purpose does not include any requirement engineering or design thinking tasks since they are covered in the IBM Cloud Garage Method. If at a later point in time we think we need to adopt Design Thinking to the Domain of Data Science we will do it. The Lightweight Method for Data Science Process Model This section introduces this lightweight process model. The first thing that you should notice is the bidirectional communication between \"Business Needs\" and \"Initial Data Exploration\". This is a data driven method and data is crucial on what can be achieved, therefore \"Business Needs\" may be adjusted based on the possibilities provided by given data. Second, with this model, there is no communication between \"Business Needs\" and other steps because an iteration of this process is very short (in the range of hours and days). The following sections explain the individual tasks. Understand business needs The task is a generic taks to group a set of iterative tasks to define the business objectives and to translate a business problem into an AI and data science solution . Initial Data Exploration This task is crucial for understanding your data. Data quality is the most important driver for success in any data science project. So, this task lets you address data quality from the beginning. This includes going back to the data owners and asking them for better quality data, if applicable. Best practices . Technology Component Suggested technology components to perform this task: Jupyter Lab Python Seaborn Data preparation This task is an important step in transforming the data from the source system into data suitable for analytics. In traditional data warehousing, this process includes accessing the online transaction processing (OLTP) system\u2019s databases, transforming the data from a highly normalized data model into a Star or Snowflake Schema, and storing the data to a data warehouse. In data science projects, this step is usually much simpler. The data arrives in an exported format (for example, JSON or CSV). But, sometimes de-normalization must be done as well. The result usually ends up in a bulk storage like Cloud Object Store. Data preparation further guidelines Technology Component Suggested technology components to perform this task: Apache SparkSQL COS (Cloud Object Store) Feature creation This task transforms input columns of various relations into additional columns to improve model performance. A subset of those features can be created in an initial task (for example, one-hot encoding of categorical variables or normalization of numerical variables). Some others require business understanding or multiple iterations to be considered. This task is one of those benefiting the most from the highly iterative nature of this method. Further Guidelines Technology Component Suggested technology components to perform this task: scikit-learn, pandas Apache Spark Model definition This task defines the machine learning or deep learning model. Because this is a highly iterative method, various iterations within this task or including up- and downstream tasks are possible. I recommend starting with simple models first for baseline creation after those models are evaluated. Further Guidelines Technology Component Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow Model Training This task trains the model. The task is set apart from model definition and evaluation for various reasons. First, training is a computationally intense task that might be scaled on computer clusters or GPUs. This can happen on a single thread or on a parallel framework like Watson Machine Learning or Apache Spark. In the most simple case Model Definition and Model Training is just a couple of lines of code away. Therefore, an architectural cut is sometimes unavoidable. (For example, model definition happens in Keras, but training happens on a Keras model export using Apache SystemML on top of Apache Spark running on a GPU cluster.) In hyperparameter tuning and hyperparameter space exploration, the downstream task \u201cModel Evaluation\u201d can be part of this asset. Once you think you have achieved a descent model performance save the notebook according to the process model\u2019s naming convention and proceed to the model evaluation task. Technology Component Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow Model evaluation This task evaluates the model\u2019s performance. Given the nature of the task, different metrics must be applied, for example, categorical-cross entropy for a multi-class classification problem. It\u2019s important to divide the data set into training, test, and validation (if cross-validation isn\u2019t used) and keep track of the performance of different feature engineering, model definition, and training parameters. In addition to model performance, Trusted AI (Adversarial Robustness, Model Bias)parameter need to be evaluated as well. Further Guidelines Technology Component Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow AI Fairness 360 Adversarial Robustness Toolbox Model deployment This task deploys the model. The task depends heavily on the use case, especially, on the stakeholder\u2019s expectation on consuming the data product. So, valid ways of deployment include: An interactive Jupyter Notebook An export of an already run, static Jupyter Notebook resembling into a report A REST endpoint allowing scoring (and training) of the model (for example, backed by a docker container running on Kubernetes) A full-fledged web or mobile application Further Guidelines Technology Component Suggested technology components to perform this task: Jupyter Seldon Project Asset Naming Convention Need a structure to name your assets? Here\u2019s our recommended convention. Note that we recommend to always use project_name, while the others are optional. [project_name].data_exp.<technology>.<version>.<extension> [project_name].etl.<technology>.<version>.<extension> [project_name].feature_eng.<technology>.<version>.<extension> [project_name].model_def.<technology>.<version>.<extension> [project_name].model_train.<technology>.<version>.<extension> [project_name].model_evaluate.<technology>.<version>.<extension> [project_name].model_deployment.<technology>.<version>.<extension>","title":"MLOps"},{"location":"methodology/MLops/#the-machine-learning-development-and-operations","text":"This part of the integrated method is aligned to the Pace-Layered Application Strategy and IT Organizational Design published by Garter in 2016. The MLOps method is tailored for the development of \"Systems of Innovation\" and composed of a process model to develop individual technology components based on a reference architecture and a architecture development framework for extending the Solution Architecture beyond the reference architecture. This way \"Systems of Innovation\" can be moved into \"Systems of Differentiation\" and \"Systems of Record\" in the long run. This method on purpose does not include any requirement engineering or design thinking tasks since they are covered in the IBM Cloud Garage Method. If at a later point in time we think we need to adopt Design Thinking to the Domain of Data Science we will do it.","title":"The Machine Learning Development and Operations"},{"location":"methodology/MLops/#the-lightweight-method-for-data-science-process-model","text":"This section introduces this lightweight process model. The first thing that you should notice is the bidirectional communication between \"Business Needs\" and \"Initial Data Exploration\". This is a data driven method and data is crucial on what can be achieved, therefore \"Business Needs\" may be adjusted based on the possibilities provided by given data. Second, with this model, there is no communication between \"Business Needs\" and other steps because an iteration of this process is very short (in the range of hours and days). The following sections explain the individual tasks.","title":"The Lightweight Method for Data Science Process Model"},{"location":"methodology/MLops/#understand-business-needs","text":"The task is a generic taks to group a set of iterative tasks to define the business objectives and to translate a business problem into an AI and data science solution .","title":"Understand business needs"},{"location":"methodology/MLops/#initial-data-exploration","text":"This task is crucial for understanding your data. Data quality is the most important driver for success in any data science project. So, this task lets you address data quality from the beginning. This includes going back to the data owners and asking them for better quality data, if applicable.","title":"Initial Data Exploration"},{"location":"methodology/MLops/#best-practices","text":"","title":"Best practices."},{"location":"methodology/MLops/#technology-component","text":"Suggested technology components to perform this task: Jupyter Lab Python Seaborn","title":"Technology Component"},{"location":"methodology/MLops/#data-preparation","text":"This task is an important step in transforming the data from the source system into data suitable for analytics. In traditional data warehousing, this process includes accessing the online transaction processing (OLTP) system\u2019s databases, transforming the data from a highly normalized data model into a Star or Snowflake Schema, and storing the data to a data warehouse. In data science projects, this step is usually much simpler. The data arrives in an exported format (for example, JSON or CSV). But, sometimes de-normalization must be done as well. The result usually ends up in a bulk storage like Cloud Object Store. Data preparation further guidelines","title":"Data preparation"},{"location":"methodology/MLops/#technology-component_1","text":"Suggested technology components to perform this task: Apache SparkSQL COS (Cloud Object Store)","title":"Technology Component"},{"location":"methodology/MLops/#feature-creation","text":"This task transforms input columns of various relations into additional columns to improve model performance. A subset of those features can be created in an initial task (for example, one-hot encoding of categorical variables or normalization of numerical variables). Some others require business understanding or multiple iterations to be considered. This task is one of those benefiting the most from the highly iterative nature of this method. Further Guidelines","title":"Feature creation"},{"location":"methodology/MLops/#technology-component_2","text":"Suggested technology components to perform this task: scikit-learn, pandas Apache Spark","title":"Technology Component"},{"location":"methodology/MLops/#model-definition","text":"This task defines the machine learning or deep learning model. Because this is a highly iterative method, various iterations within this task or including up- and downstream tasks are possible. I recommend starting with simple models first for baseline creation after those models are evaluated. Further Guidelines","title":"Model definition"},{"location":"methodology/MLops/#technology-component_3","text":"Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow","title":"Technology Component"},{"location":"methodology/MLops/#model-training","text":"This task trains the model. The task is set apart from model definition and evaluation for various reasons. First, training is a computationally intense task that might be scaled on computer clusters or GPUs. This can happen on a single thread or on a parallel framework like Watson Machine Learning or Apache Spark. In the most simple case Model Definition and Model Training is just a couple of lines of code away. Therefore, an architectural cut is sometimes unavoidable. (For example, model definition happens in Keras, but training happens on a Keras model export using Apache SystemML on top of Apache Spark running on a GPU cluster.) In hyperparameter tuning and hyperparameter space exploration, the downstream task \u201cModel Evaluation\u201d can be part of this asset. Once you think you have achieved a descent model performance save the notebook according to the process model\u2019s naming convention and proceed to the model evaluation task.","title":"Model Training"},{"location":"methodology/MLops/#technology-component_4","text":"Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow","title":"Technology Component"},{"location":"methodology/MLops/#model-evaluation","text":"This task evaluates the model\u2019s performance. Given the nature of the task, different metrics must be applied, for example, categorical-cross entropy for a multi-class classification problem. It\u2019s important to divide the data set into training, test, and validation (if cross-validation isn\u2019t used) and keep track of the performance of different feature engineering, model definition, and training parameters. In addition to model performance, Trusted AI (Adversarial Robustness, Model Bias)parameter need to be evaluated as well. Further Guidelines","title":"Model evaluation"},{"location":"methodology/MLops/#technology-component_5","text":"Suggested technology components to perform this task: scikit-learn Apache SparkML TensorFlow AI Fairness 360 Adversarial Robustness Toolbox","title":"Technology Component"},{"location":"methodology/MLops/#model-deployment","text":"This task deploys the model. The task depends heavily on the use case, especially, on the stakeholder\u2019s expectation on consuming the data product. So, valid ways of deployment include: An interactive Jupyter Notebook An export of an already run, static Jupyter Notebook resembling into a report A REST endpoint allowing scoring (and training) of the model (for example, backed by a docker container running on Kubernetes) A full-fledged web or mobile application Further Guidelines","title":"Model deployment"},{"location":"methodology/MLops/#technology-component_6","text":"Suggested technology components to perform this task: Jupyter Seldon","title":"Technology Component"},{"location":"methodology/MLops/#project-asset-naming-convention","text":"Need a structure to name your assets? Here\u2019s our recommended convention. Note that we recommend to always use project_name, while the others are optional. [project_name].data_exp.<technology>.<version>.<extension> [project_name].etl.<technology>.<version>.<extension> [project_name].feature_eng.<technology>.<version>.<extension> [project_name].model_def.<technology>.<version>.<extension> [project_name].model_train.<technology>.<version>.<extension> [project_name].model_evaluate.<technology>.<version>.<extension> [project_name].model_deployment.<technology>.<version>.<extension>","title":"Project Asset Naming Convention"},{"location":"methodology/dataops/","text":"Data Integration Development and Operations","title":"DataOps"},{"location":"methodology/dataops/#data-integration-development-and-operations","text":"","title":"Data Integration Development and Operations"},{"location":"methodology/devops/","text":"Solution Development and Operations","title":"DevOps"},{"location":"methodology/devops/#solution-development-and-operations","text":"","title":"Solution Development and Operations"},{"location":"monitoring/","text":"Monitoring","title":"Bias monitoring to ensure equitable"},{"location":"monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"preparation/","text":"Data Preparation The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped. This article complements the prepare your data for AI and data science practice and we recommend adopting both practices. Data Cleansing During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Data types: Are data types of columns matching their content? E.g. is age stored as integer and not as string? Ranges: Does the value distribution of values in a column make sense? Use stats (e.g. min, max, mean, standard deviation) and visualizations (e.g. box-plot, histogram) for help Set memberships: Are only allowed values chosen for categorical or ordinal fields? E.g. Female, Male, Unknown Cross-field validation: Some fields can impact validity of other fields. E.g. a male person can\u2019t be pregnant Dealing with missing values Dealing with outliers Correcting typos, and match regular expressions: Some files need to stick to a pattern expressed by a regular expression. E.g. a lower-case character followed by 6 digits Grouping sparse classes Dropping duplicates: Are duplicates present where undesired like a primary key? E.g. client IDs Key considerations Missing Values Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE). Outliers If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups. Grouping Sparse Classes Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used. Data Transformations It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Filtering: Sometimes imputing values doesn\u2019t perform well, therefore deletion of low quality records is a better strategy Power functions Imputed time-series quantization: Time series often contain streams with measurements at different timestamps. Therefore, it is beneficial to quantize measurements to a common \u201cheart beat\u201d and impute the corresponding values. This can be done by sampling from the source time series distributions on the respective quantized time steps Discretizing: Continuous fields might confuse the model, e.g. a discrete set of age ranges sometimes performs better than continuous values, especially on smaller amounts of data and with simpler models Key Considerations: Categorical Encoding Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings. Dealing with Skewed Data Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation. Scaling Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization. Bias Mitigation If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover. Feature Engineering Feature engineering is the process of creating new features based upon knowledge about current features and the required task. Feature Creation and Feature Engineering is one of the most important tasks in machine learning since it hugely impacts model performance. This also holds for deep learning, although to a lesser extent. Features can be changed or new features can be created from existing ones. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships Feature Extraction You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF). Capturing Feature Relationships Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns. Other techniques One-hot-encoding: Categorical integer features should be transformed into \u201cone-hot\u201d vectors. In relational terms this results in addition of additional columns \u2013 one columns for each distinct category Time-to-Frequency transformation: Time-series (and sometimes also sequence data) is recorded in the time domain but can easily transformed into the frequency domain e.g. using FFT (Fast Fourier Transformation) Month-From-Date: Creating an additional feature containing the month independent from data captures seasonal aspects. Sometimes further discretization in to quarters helps as well Aggregate-on-Target: Simply aggregating fields the target variable (or even other fields) can improve performance, e.g. count number of data points per ZIP code or take the median of all values by geographical region As feature engineering is an art on itself, this list cannot be exhaustive.","title":"Data Preparation"},{"location":"preparation/#data-preparation","text":"The purpose of the Data Preparation stage is to get the data into the best format for machine learning, this includes three stages: Data Cleansing, Data Transformation, and Feature Engineering. Quality data is more important than using complicated algorithms so this is an incredibly important stage and should not be skipped. This article complements the prepare your data for AI and data science practice and we recommend adopting both practices.","title":"Data Preparation"},{"location":"preparation/#data-cleansing","text":"During the Data Understanding activities, you explored your data and detected uncomplete or incorrect values. This stage is where you address those issues, activities include: Data types: Are data types of columns matching their content? E.g. is age stored as integer and not as string? Ranges: Does the value distribution of values in a column make sense? Use stats (e.g. min, max, mean, standard deviation) and visualizations (e.g. box-plot, histogram) for help Set memberships: Are only allowed values chosen for categorical or ordinal fields? E.g. Female, Male, Unknown Cross-field validation: Some fields can impact validity of other fields. E.g. a male person can\u2019t be pregnant Dealing with missing values Dealing with outliers Correcting typos, and match regular expressions: Some files need to stick to a pattern expressed by a regular expression. E.g. a lower-case character followed by 6 digits Grouping sparse classes Dropping duplicates: Are duplicates present where undesired like a primary key? E.g. client IDs","title":"Data Cleansing"},{"location":"preparation/#key-considerations","text":"","title":"Key considerations"},{"location":"preparation/#missing-values","text":"Most machine learning models require all features to be complete, therefore, missing values must be dealt with. The simplest solution is to remove all rows that have a missing value but important information could be lost or bias introduced. During the data exploration phase, you should have explored possible reasons for this missing data to help guide whether or not it is acceptable to drop these data points. Alternatively, you can impute the value; provide an appropriate substitute for the missing data. Common imputations are using the mean, median, or mode. For categorical data, a new category (e.g. \u201cUnknown\u201d) could be created. More complicated solutions include using K-Nearest Neighbors (KNN) or Multivariate Imputation by Chained Equation (MICE).","title":"Missing Values"},{"location":"preparation/#outliers","text":"If your outlier investigation during Data Exploration only finds low-frequency outliers that are likely to be erroneous, these can be treated like missing data be either removed or replaced. However, if you believe important information will be lost by removing or changing them, you may wish to keep them and use a machine learning algorithm and optimization metric robust to outliers. The process of removing outliers from your data set is referred to as trimming or truncation. Outliers can be replaced by techniques similar to missing data, e.g. with mean, mode or median. More complicated techniques include Winsorizing \u2013 replacing extreme values with minimum and maximum percentiles \u2013 and discretization (binning) \u2013 dividing the continuous variable into discrete groups.","title":"Outliers"},{"location":"preparation/#grouping-sparse-classes","text":"Categorical features can often have a large number of distinct values, some of which have a low frequency. This can be particularly common when the data has been input as free text and prone to typos. In the Data Transformation stage, we will discuss how categorical data is converted to a format a machine learning model can read. However, this often involves creating a new feature for each distinct value in that category; if each categorical feature has a lot of distinct values, this transformation results in a lot of additional features. Many machine learning algorithms can struggle with too many features, this is referred to as the curse of dimensionality. Consequently, you may wish to group qualitatively similar values. This is likely to be a manual effort working with a subject matter expert. Though for typos, or inconsistencies in capitalizations, pattern or fuzzy matching tools can be used.","title":"Grouping Sparse Classes"},{"location":"preparation/#data-transformations","text":"It is rare to have collected data solely to make predictions. Consequently, the data you have available may not be in the right format or may require transformations to make it more useful. Data Transformation activities and techniques include: Categorical encoding Dealing with skewed data Bias mitigation Scaling Rank transformation Filtering: Sometimes imputing values doesn\u2019t perform well, therefore deletion of low quality records is a better strategy Power functions Imputed time-series quantization: Time series often contain streams with measurements at different timestamps. Therefore, it is beneficial to quantize measurements to a common \u201cheart beat\u201d and impute the corresponding values. This can be done by sampling from the source time series distributions on the respective quantized time steps Discretizing: Continuous fields might confuse the model, e.g. a discrete set of age ranges sometimes performs better than continuous values, especially on smaller amounts of data and with simpler models","title":"Data Transformations"},{"location":"preparation/#key-considerations_1","text":"","title":"Key Considerations:"},{"location":"preparation/#categorical-encoding","text":"Label encoding converts categorical variables to numerical representation, something that is machine-readable. The first thing to understand is whether or not your categories are ordinal, e.g. level of education has an order with master\u2019s degree being higher ranked than bachelor\u2019s degree. To keep this relationship, you will want to rank them based on their associated magnitude and use their ranking as an input to your model. If the categories are not ordinal, you can one-hot-encode your categories; this will create a new column for each unique value in the column. For neural networks, or working with text data, you may wish to use embeddings.","title":"Categorical Encoding"},{"location":"preparation/#dealing-with-skewed-data","text":"Normality is often assumed with statistical techniques. If you\u2019re using regression algorithms such as linear regression or neural networks, you are likely to see large improvements if you transform variables with skewed distributions. To approximate a more symmetric distribution, you can use roots (i.e. square-root, cube root), logarithms (i.e. base e, or base 10), reciprocals (i.e. positive or negative), or Box-Cox transformation.","title":"Dealing with Skewed Data"},{"location":"preparation/#scaling","text":"Scaling is a method of transforming data into a particular range. Regression algorithms and algorithms using Euclidean distances (e.g. KNN, or K-Means) are sensitive to the variation in magnitude and range across features. The goal of scaling is to change the values of each numerical feature in the data set to a common scale. By doing so, changes in different features become more comparable. Scaling can be done with normalization (or min-max scaling) or z-score standardization.","title":"Scaling"},{"location":"preparation/#bias-mitigation","text":"If you\u2019ve discovered that there is bias in your data, you can mitigate this with various preprocessing techniques. These often replace the current values, or labels, with those that will result in a fairer model. Examples of pre-processing bias mitigation algorithms are Reweighing, Optimized preprocessing, Learning fair representations and Disparate impact remover.","title":"Bias Mitigation"},{"location":"preparation/#feature-engineering","text":"Feature engineering is the process of creating new features based upon knowledge about current features and the required task. Feature Creation and Feature Engineering is one of the most important tasks in machine learning since it hugely impacts model performance. This also holds for deep learning, although to a lesser extent. Features can be changed or new features can be created from existing ones. It is important to have a clear understanding of the data to do this step, this may require you working with a subject matter expert. This activity may also require you to find new data sources. Two key activities are: Feature extraction, and Capturing Feature Relationships","title":"Feature Engineering"},{"location":"preparation/#feature-extraction","text":"You may find that columns in your data are not useful as they are, possibly because they are too granular. For example, a timestamp is unlikely to be useful whilst the time of day, or day of the week might be. In text analytics, feature extraction is creating vectors from the raw text strings. This could simply be creating a new column that indicates if particular phrases are mentioned, or columns could be created for each word (Bag-of-Words) and their value is a representation of their frequency (TF, or TF-IDF).","title":"Feature Extraction"},{"location":"preparation/#capturing-feature-relationships","text":"Rather than expect the model to find relationships between two features, they can be explicitly called out. You are then helping your algorithm focus on what you, or the subject matter expert you\u2019re working with, know is important. This could be the sum, the difference, the product or the quotient. For example, a machine learning model may not easily find the connection between the longitude and latitude values of two address but by providing the distance between the two, you better enable it to derive patterns.","title":"Capturing Feature Relationships"},{"location":"preparation/#other-techniques","text":"One-hot-encoding: Categorical integer features should be transformed into \u201cone-hot\u201d vectors. In relational terms this results in addition of additional columns \u2013 one columns for each distinct category Time-to-Frequency transformation: Time-series (and sometimes also sequence data) is recorded in the time domain but can easily transformed into the frequency domain e.g. using FFT (Fast Fourier Transformation) Month-From-Date: Creating an additional feature containing the month independent from data captures seasonal aspects. Sometimes further discretization in to quarters helps as well Aggregate-on-Target: Simply aggregating fields the target variable (or even other fields) can improve performance, e.g. count number of data points per ZIP code or take the median of all values by geographical region As feature engineering is an art on itself, this list cannot be exhaustive.","title":"Other techniques"},{"location":"preparation/data-understanding/","text":"Data Understanding Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. As presented in detail in this practice: \"Understand data needs to support AI and data science solutions\" each business analytics problem has specific data requirements, and different patterns may apply. The article address the activities of collecting data, describe the data, visualize and verify data quality. In this article we are complementing those tasks with some specifics considerations to address as a Data Scientist. There are two key stages of Data Understanding: a Data Assessment and Data Exploration . Data Assessment The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today? Key Considerations Collecting Ground Truth Data If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible. Data Relevance Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value. Quantity of Data If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price. Another rule of thumb is that you need ten times as degrees of freedom (or parameters/weights) your model has. In the case of linear regression for example, the number of degrees of freedom equals the number of features. In deep learning, every neuron adds and exponential growth to the degrees of freedom. Ethics It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow user groups, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those individuals in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them. Data Exploration Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they matching the distribution you expect (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others? Key Considerations Missing Values An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data is imputing values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, mode or an interpolation are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely. Outliers An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees. Un-balanced Data A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-understanding","text":"Quality data is fundamental to any data science engagement. To gain actionable insights, the appropriate data must be sourced and cleansed. As presented in detail in this practice: \"Understand data needs to support AI and data science solutions\" each business analytics problem has specific data requirements, and different patterns may apply. The article address the activities of collecting data, describe the data, visualize and verify data quality. In this article we are complementing those tasks with some specifics considerations to address as a Data Scientist. There are two key stages of Data Understanding: a Data Assessment and Data Exploration .","title":"Data Understanding"},{"location":"preparation/data-understanding/#data-assessment","text":"The first step in data understanding is Data Assessment. This should be undertaken before the kick-off of a project as it is an important step to validate its feasibility. This task evaluates what data is available and how it aligns to the business problem. It should answer the following questions: What data is available? How much data is available? Do you have access to the ground truth, the values you\u2019re trying to predict? What format will the data be in? Where does it reside? How can the data be accessed? Which fields are most important? How do the multiple data sources get joined? What important metrics are reported using this data? If applicable, how does the data map to the current method of completing the task today?","title":"Data Assessment"},{"location":"preparation/data-understanding/#key-considerations","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#collecting-ground-truth-data","text":"If you wish to make predictions using machine learning, you require a labeled data set. For each of your examples, you need the correct value, or appropriate category, that the machine learning model should predict; this is called the Ground Truth. This may already be available to you as it\u2019s an action or event (e.g. a value indicated whether or not the customer churned) or it might be something you need to collect (e.g. the topic of an email). If this is something that needs to be collected or manually labeled, a plan should be made to understand how this would be achieved and the time it will take to complete. This task could be time-consuming and costly, making the project unfeasible.","title":"Collecting Ground Truth Data"},{"location":"preparation/data-understanding/#data-relevance","text":"Write down all the different data points that will be made available and evaluate if it intuitively makes sense that a machine learning model could predict with this data. Is there proven evidence that there is a connection between these data points and what you wish to achieve? If you add unrelated features to a machine learning model, you\u2019re adding noise, making the algorithm look for connections that aren\u2019t there. This can result in decreased performance. Conversely, if a human is undertaking this task today, explore what they use to make the decision. Is that data available to be used in the model? When building a model, it is good to start simple \u2013 use only the obvious features first and see how this performs before adding those you\u2019re less sure about. This allows you to evaluate if the additional features add value.","title":"Data Relevance"},{"location":"preparation/data-understanding/#quantity-of-data","text":"If you\u2019re trying to build a machine learning model, there must be sufficient data. There is no formula to calculate how much should be collected but as an example, if you\u2019re using traditional machine learning approaches (random forests, logistic regression) to classify your data you want hundreds of examples (ideally more) of each classification. When using deep learning techniques, the number of examples needed significantly increases. Also, you want lots of variation in the features; for example, if you are predicting house prices and one of your inputs is neighborhood, you want to make sure you have good coverage of all neighborhoods so the model can learn how this impacts the price. Another rule of thumb is that you need ten times as degrees of freedom (or parameters/weights) your model has. In the case of linear regression for example, the number of degrees of freedom equals the number of features. In deep learning, every neuron adds and exponential growth to the degrees of freedom.","title":"Quantity of Data"},{"location":"preparation/data-understanding/#ethics","text":"It is important at the beginning of a project to consider potential harms from your tool. These harms can be caused by designing for too narrow user groups, having an insufficient representation of a sub-population, or human labelers favoring a privileged group. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. Similarly, if a group is under-represented, the machine learning model has fewer examples to learn from, resulting in reduced accuracy for those individuals in this group. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of people. Ensure you have evaluated risks and have techniques in place to mitigate them.","title":"Ethics"},{"location":"preparation/data-understanding/#data-exploration","text":"Once you have access to data, you can start Data Exploration. This is a phase for creating meaningful summaries of your data, this is particularly important if you are unfamiliar with the data. This is also the time you should test your assumptions. The types of activities and possible questions to ask are: Count the number of records \u2013 is this what you expected? What are the datatypes \u2013 will you need to change these for a machine learning model? Look for missing values \u2013 how should you deal with these? Verify the distribution of each column \u2013 are they matching the distribution you expect (e.g. normally distributed)? Search for outliers \u2013 are there anomalies in your data? Are all values valid (e.g. no ages less than 0)? Validated if your data is balanced \u2013 are different groups represented in your data? Are there enough examples of each class you wish to predict? Is there bias in your data \u2013 are subgroups in your data treated more favorable than others?","title":"Data Exploration"},{"location":"preparation/data-understanding/#key-considerations_1","text":"","title":"Key Considerations"},{"location":"preparation/data-understanding/#missing-values","text":"An ideal dataset would be complete, with valid values for every observation. However, in reality, you will come across many \u201cNULL\u201d or \u201cNaN\u201d values. The simplest way to deal with missing data is to remove all rows that have a missing value but valuable information can be lost or you could introduce bias. Consequently, it is important to try to understand if there is a reason or pattern for the missing values. For example, particular groups of people may not respond to certain questions in a survey; removing them will prevent learning trends within these groups. An alternative to removing data is imputing values; replacing missing values with an appropriate substitute. For continuous variables, the mean, median, mode or an interpolation are often used. Whilst, for categorical data it is frequently the mode or a new category (e.g. \"NA\"). If columns have a high proportion of values missing, you may wish to remove them entirely.","title":"Missing Values"},{"location":"preparation/data-understanding/#outliers","text":"An outlier is a data point that is significantly different from other observations. Once you identify outliers, you should also investigate what may have caused them. They could indicate bad data: data that was incorrectly collected. If this is the case, you may wish to remove these data points or replace them (similar to how you impute values for missing data). Alternatively, these values could be interesting and useful for your machine learning model. Some machine learning algorithms, such as linear regression, can be sensitive to outliers. Consequently, you may wish to use only algorithms more robust to outliers, such as random forest or gradient boosted trees.","title":"Outliers"},{"location":"preparation/data-understanding/#un-balanced-data","text":"A dataset is unbalanced if the number of data points available for each class is not similar. This is common with classification problems such as fraud detection; the majority of transactions are normal, whilst a small proportion is fraudulent. Machine learning algorithms learn from examples; the more examples it has, the more confident it can be in the patterns it has discovered. Conversely, if you only provide the machine learning algorithm a few examples, it will, at best, learn only weak trends. If your data is unbalanced, as in the fraud detection model, it may not be able to identify what patterns indicate fraud. In addition, algorithms are incentivized by performance metrics such as accuracy: if 99.9% of transactions are not-fraud, a model can be 99.9% accurate by simply labeling all transactions as \u201cnon-fraud\u201d with no need to search for further patterns. Features may also be unbalanced, preventing the algorithm from learning how these categories impact the output. Stacey Ronaghan","title":"Un-balanced Data"},{"location":"preparation/dev-model/","text":"Select model Different machine learning algorithms are searching for different trends and patterns. Consequently, one algorithm isn\u2019t the best across all datasets or for all use-cases. To find the best solution, we conduct a lot of experiments, evaluating different machine learning algorithms and tuning their hyper-parameters. Here are some guidelines which are required to follow: Choose, justify and apply a model performance indicator (e.g. F1 score, true positive rate, within cluster sum of squared error, \u2026) to assess your model and justify the choice of an algorithm Implement your algorithm in at least one deep learning and at least one non-deep learning algorithm, compare and document model performance Apply at least one additional iteration in the process model involving at least the feature creation task and record impact on model performance (e.g. data normalizing, PCA, \u2026) Depending on the algorithm class and data set size you might choose specific technologies / frameworks to solve your problem. This chapter introduces various important topics: Train, Test & Validation Data Algorithm Exploration Hyper-parameter Optimization Ensembles Terminology Descriptive analytics This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years. Predictive analytics This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels. Prescriptive analytics Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis. Supervised learning Learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Classification The problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification). Regression When goal of the learning problem, is to predict continuous value output Unsupervised learning Giving a dataset, try to find tendency in the data, by using techniques like clustering. Feature Feature is an attribute used as input for the model to train. Other names include dimension or column. Bias Bias is the expected difference between the parameters of a model that perfectly fits your data and those your algorithm has learned. Low bias algorithms (Decision Trees, K-nearest Neighbors & Support Vector Machines) tend to find more complex patterns than high bias algorithms. Variance This is how much the algorithm is impacted by the training data; how much the parameters change with new training data. Low variance algorithms (e.g. Linear Regression, Logistic Regression & Naive Bayes) tend to find less complex patterns than high variance algorithms. Underfitting The model is too simple to capture the patterns within the data; this will perform poorly on data it has been trained on as well as unseen data. High bias, low variance. High training error and high test error. Overfitting The model is too complicated or too specific, capturing trends that do not generalize; it will accurately predict data it has been trained on but not on unseen data. Low bias, high variance. Low training error and high test error. Bias-Variance Trade-off The Bias-Variance Trade-off refers to finding a model with the right complexity, minimizing both the train and test error. Further Reading: Bias-Variance Tradeoff Infographic Train, Validation & Test Machine learning algorithms learn from examples and, if you have good data, the more examples you provide, the better it will be at finding patterns in the data. However, you must be cautious of overfitting; overfitting is where a model can accurately make predictions for data it has been trained on but unable to generalize to data is hasn\u2019t seen before. This is why we split our data into training data, validation data and test data. Validation data and test data are often referred to interchangeably, however, they are described below as having distinct purposes. Training data This is the data used to train the model, to fit the model parameters. It will account for the largest proportion of data as you wish for the model to see as many examples as possible. Validation data This is the data used to fit hyper-parameters and for feature selection. Although the model never sees this data during training, by selecting particular features or hyper-parameters based on this data, you are introducing bias and again risk overfitting. Test data This is the data used to evaluate and compare your tuned models. As this data has not been seen during training nor tuning, it can provide insight into whether your models generalize well to unseen data. Cross-Validation The purpose of cross-validation is to evaluate if a particular algorithm is suited for your data and use-case. It is also used for hyper-parameter tuning and feature selection. The data is split into train and validation sets (though you should have some test data put to one side too) and a model built with each of the slices of data. The final evaluation of the algorithm is the average performance of each of the models. Hold-out Method The simplest version of cross-validation is the hold-out method where we randomly split our data into two sets, a training set and a validation set. This is the quickest method as it only requires building a model once. However, with only one validation data set, there is a risk that it contains particularly easy, or difficult, observations to predict. Consequently, we could find we are overfitting to this validation data and on a test set it would perform poorly. K-fold Cross-Validation The k-fold cross-validation method involves splitting the data into k-subsets. You then train a model k times, each time using one of the k-subsets as its validation data. The training data will be all other observations not in the validation set. Your final evaluation is the average across all k folds. Leave-one-out cross-validation This is the most extreme version of K-fold cross-validation, where your k is N (the number of observations in your dataset). You train a model N separate times using all data except for one observation and then validate its accuracy with its prediction for that observation. Although you are thorough evaluating how well this algorithm works with your data set, this method is expensive as it requires you to build N models. Stratified cross-validation Stratified cross-validation enforces that the k-fold sets have similar proportions of observations for each class in either categorical features or the label. Algorithm Exploration The algorithms you explore should be driven by your use-case. By first identifying what you are trying to achieve, you can then narrow the scope of searching for solutions. Although not a complete list of possible methods, below are links that introduce algorithms for Regression, Classification, Clustering, Recommendations, and Anomaly Detection. A colleague and I also created this tool to help guide algorithm selection ( Click here to visit Algorithm Explorer ). Regression Regression algorithms are machine learning techniques for predicting continuous numerical values. They are supervised learning tasks which means they require labeled training examples. Further reading: Machine Learning: Trying to predict a numerical value Classification Classification algorithms are machine learning techniques for predicting which category the input data belongs to. They are supervised learning tasks which means they require labeled training examples. Further reading: Machine Learning: Trying to predict a classify your data Clustering Clustering algorithms are machine learning techniques to divide data into a number of groups where points in the groups have similar traits. They are unsupervised learning tasks and therefore do not require labeled training examples. Further reading: Machine Learning: Trying to discover structure in your data Recommendation Engines Recommendation Engines are created to predict a preference or rating that indicates a user\u2019s interest in an item/product. The algorithms used to create this system find similarities between either the users, the items, or both. Further reading: Machine Learning: Trying to make recommendations Anomaly Detection Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers. Further reading: Machine Learning: Trying to detect outliers or unusual behavior Hyper-Parameter Optimization Although the terms parameters and hyper-parameters are occasionally used interchangeably, we are going to distinguish between the two. Parameters are properties the algorithm is learning during training. For linear regression, these are the weights and biases; whilst for random forests, these are the variables and thresholds at each node. Hyper-parameters , on the other hand, are properties that must be set before training. For k-means clustering, you must define the value of k; whilst for neural networks, an example is the learning rate. Hyper-parameter optimization is the process of finding the best possible values for these hyper-parameters to optimize your performance metric (e.g. highest accuracy, lowest RMSE, etc.). To do this, we train a model for different combinations of values and evaluate which find the best solution. Three methods used to search for the best combinations are Grid Search, Random Search, and Bayesian Optimization. Grid Search You specify values for each hyper-parameter, and all combinations of those values will be evaluated. For example, if you wish to evaluate hyper-parameters for random forest, you could specify provide three options for the number of trees hyper-parameter (10, 20 and 50) and for the maximum depth of each tree you also provide three options (no limit, 10 and 20). This would result in a random forest model being built for each of the 9 possible combinations: (10, no limit), (10, 10), (10, 20), (20, no limit), (20, 10), (20, 20), (50, no limit), (50, 10), and (50, 20). The combination that provides the best performance will be those you use for your final model. Pros: Simple to use. You will find the best combination of the values you\u2019ve provided. You can run each of the experiments in parallel. Cons: Computationally expensive as so many models are being built. If a particular hyper-parameter is not important, you are exploring different possibilities unnecessarily. Random Search You specify ranges or options for each hyper-parameter and random values of each are selected. Continuing with the random forest example, you might provide the range for the number of trees to be between 10 and 50 and max_depth to be either no limit, 10 or 20. This time, rather than it compute all permutations, you can specify the number of iterations you wish to run. Say we only want five, then we might test something like (19, 20), (32 no limit), (40, no limit), (10, 20), (27, 10). Pros: Simple to use. More efficient and can outperform grid search when only a few hyper-parameters affect the overall performance. You can run each of the experiments in parallel. Cons: It involves random sampling so will only find the best combination if it searches that space. Coarse to Fine For both grid search and random search, you can also use the coarse to fine technique. This involves exploring a broader range of variables with wide intervals or all possible options. Once you\u2019ve got your results from this initial search you explore the results to see if there are any patterns or particular regions that look promising. If so, you can repeat the process but refine your search. For the random forest example above, we might notice that results are promising when the maximum depth has no limit and when the number of trees hyper-parameter is either 10 or 20. The search process is repeated but keeping the maximum depth hyper-parameter constant and increasing the granularity of the number of tree options, testing values of 12, 14, 16, 18, to see if we can find a better result. Pros: Can find more optimized hyper-parameters, improving the performance metric. Cons: Evaluation of the results to find the best regions to explore can be cumbersome. Bayesian Optimization Bayesian Optimization uses the prior knowledge of success with hyper-parameter combinations to choose the next best. The technique uses a machine learning approach, building a model where the hyper-parameters are the features and the performance is the target variable. After each experiment, a new data point is added and a new model built. It assumes similar combinations will have similar results and prioritize exploring regions where promising results have already been seen. However, it also takes into consideration uncertainty as a possibility for large gain; if there are large areas that have not yet been explored, it will also prioritize these as. Taking just one hyper-parameter, the number of trees, the algorithm might first try 10 and get a pretty good performance. It then tries 32 and the performance is significantly better. Bayesian optimization builds a model based on the first two data points to predict performance. The model is likely to be linear with just the two data points so the next value chosen is 40, with the expectation that the performance will continue to improve as the number of trees increase. It does not. Now, it builds another model that suggests there might be improvement around 32 where it has seen the best result so far, however, there\u2019s still a large gap between 10 and 32 that hasn\u2019t been explored and due to large uncertainty, it chooses 21. Again, the model is tweaked with this new data and another value chosen\u2026 Pros: Can find more optimized hyper-parameters, improving the performance metric. It can reduce the time spent searching for an optimum solution when the number of parameters is high and each experiment is computationally expensive. Cons: You cannot run each experiment in parallel as the next combination of hyper-parameter values is determined by the prior runs. It also requires tuning \u2014 choosing a scale for each hyper-parameter and an appropriate kernel. Ensemble Learning Ensembles combine several machine learning models, each finding different patterns within the data, to provide a more accurate solution. These techniques can both improve performance, as they capture more trends, as well as reduce overfitting, as the final prediction is a consensus from many models. Bagging Bagging (bootstrap aggregations) is the method of building multiple models in parallel and average their prediction as the final prediction. These models can be built with the same algorithm (i.e. the Random Forest algorithm builds many decision trees) or you can build different types of models (e.g. a Linear Regression model and an SVM model). Boosting Boosting builds models sequentially evaluating the success of earlier models; the next model prioritizes learning trends for predicting examples that the current models perform poorly on. There are three common techniques for this: AdaBoost, Gradient Boosting and XGBoosted. Stacking Stacking involves building multiple models and using their outputs as features into a final model. For example, your goal is to create a classifier and you build a KNN model as well as a Na\u00efve Bayes model. Rather than choose between the two, you can pass their two predictions into a final Logistic Regression model. This final model may result in better results than the two intermediate models.","title":"Model selection"},{"location":"preparation/dev-model/#select-model","text":"Different machine learning algorithms are searching for different trends and patterns. Consequently, one algorithm isn\u2019t the best across all datasets or for all use-cases. To find the best solution, we conduct a lot of experiments, evaluating different machine learning algorithms and tuning their hyper-parameters. Here are some guidelines which are required to follow: Choose, justify and apply a model performance indicator (e.g. F1 score, true positive rate, within cluster sum of squared error, \u2026) to assess your model and justify the choice of an algorithm Implement your algorithm in at least one deep learning and at least one non-deep learning algorithm, compare and document model performance Apply at least one additional iteration in the process model involving at least the feature creation task and record impact on model performance (e.g. data normalizing, PCA, \u2026) Depending on the algorithm class and data set size you might choose specific technologies / frameworks to solve your problem. This chapter introduces various important topics: Train, Test & Validation Data Algorithm Exploration Hyper-parameter Optimization Ensembles","title":"Select model"},{"location":"preparation/dev-model/#terminology","text":"","title":"Terminology"},{"location":"preparation/dev-model/#descriptive-analytics","text":"This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years.","title":"Descriptive analytics"},{"location":"preparation/dev-model/#predictive-analytics","text":"This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels.","title":"Predictive analytics"},{"location":"preparation/dev-model/#prescriptive-analytics","text":"Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis.","title":"Prescriptive analytics"},{"location":"preparation/dev-model/#supervised-learning","text":"Learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (label y ), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets.","title":"Supervised learning"},{"location":"preparation/dev-model/#classification","text":"The problem is when we are trying to predict one of a small number of discrete-valued outputs. In other words, if our label is binary (binary classification) or caegorical (multi-class classification).","title":"Classification"},{"location":"preparation/dev-model/#regression","text":"When goal of the learning problem, is to predict continuous value output","title":"Regression"},{"location":"preparation/dev-model/#unsupervised-learning","text":"Giving a dataset, try to find tendency in the data, by using techniques like clustering.","title":"Unsupervised learning"},{"location":"preparation/dev-model/#feature","text":"Feature is an attribute used as input for the model to train. Other names include dimension or column.","title":"Feature"},{"location":"preparation/dev-model/#bias","text":"Bias is the expected difference between the parameters of a model that perfectly fits your data and those your algorithm has learned. Low bias algorithms (Decision Trees, K-nearest Neighbors & Support Vector Machines) tend to find more complex patterns than high bias algorithms.","title":"Bias"},{"location":"preparation/dev-model/#variance","text":"This is how much the algorithm is impacted by the training data; how much the parameters change with new training data. Low variance algorithms (e.g. Linear Regression, Logistic Regression & Naive Bayes) tend to find less complex patterns than high variance algorithms.","title":"Variance"},{"location":"preparation/dev-model/#underfitting","text":"The model is too simple to capture the patterns within the data; this will perform poorly on data it has been trained on as well as unseen data. High bias, low variance. High training error and high test error.","title":"Underfitting"},{"location":"preparation/dev-model/#overfitting","text":"The model is too complicated or too specific, capturing trends that do not generalize; it will accurately predict data it has been trained on but not on unseen data. Low bias, high variance. Low training error and high test error.","title":"Overfitting"},{"location":"preparation/dev-model/#bias-variance-trade-off","text":"The Bias-Variance Trade-off refers to finding a model with the right complexity, minimizing both the train and test error. Further Reading: Bias-Variance Tradeoff Infographic","title":"Bias-Variance Trade-off"},{"location":"preparation/dev-model/#train-validation-test","text":"Machine learning algorithms learn from examples and, if you have good data, the more examples you provide, the better it will be at finding patterns in the data. However, you must be cautious of overfitting; overfitting is where a model can accurately make predictions for data it has been trained on but unable to generalize to data is hasn\u2019t seen before. This is why we split our data into training data, validation data and test data. Validation data and test data are often referred to interchangeably, however, they are described below as having distinct purposes. Training data This is the data used to train the model, to fit the model parameters. It will account for the largest proportion of data as you wish for the model to see as many examples as possible.","title":"Train, Validation &amp; Test"},{"location":"preparation/dev-model/#validation-data","text":"This is the data used to fit hyper-parameters and for feature selection. Although the model never sees this data during training, by selecting particular features or hyper-parameters based on this data, you are introducing bias and again risk overfitting.","title":"Validation data"},{"location":"preparation/dev-model/#test-data","text":"This is the data used to evaluate and compare your tuned models. As this data has not been seen during training nor tuning, it can provide insight into whether your models generalize well to unseen data.","title":"Test data"},{"location":"preparation/dev-model/#cross-validation","text":"The purpose of cross-validation is to evaluate if a particular algorithm is suited for your data and use-case. It is also used for hyper-parameter tuning and feature selection. The data is split into train and validation sets (though you should have some test data put to one side too) and a model built with each of the slices of data. The final evaluation of the algorithm is the average performance of each of the models.","title":"Cross-Validation"},{"location":"preparation/dev-model/#hold-out-method","text":"The simplest version of cross-validation is the hold-out method where we randomly split our data into two sets, a training set and a validation set. This is the quickest method as it only requires building a model once. However, with only one validation data set, there is a risk that it contains particularly easy, or difficult, observations to predict. Consequently, we could find we are overfitting to this validation data and on a test set it would perform poorly.","title":"Hold-out Method"},{"location":"preparation/dev-model/#k-fold-cross-validation","text":"The k-fold cross-validation method involves splitting the data into k-subsets. You then train a model k times, each time using one of the k-subsets as its validation data. The training data will be all other observations not in the validation set. Your final evaluation is the average across all k folds.","title":"K-fold Cross-Validation"},{"location":"preparation/dev-model/#leave-one-out-cross-validation","text":"This is the most extreme version of K-fold cross-validation, where your k is N (the number of observations in your dataset). You train a model N separate times using all data except for one observation and then validate its accuracy with its prediction for that observation. Although you are thorough evaluating how well this algorithm works with your data set, this method is expensive as it requires you to build N models.","title":"Leave-one-out cross-validation"},{"location":"preparation/dev-model/#stratified-cross-validation","text":"Stratified cross-validation enforces that the k-fold sets have similar proportions of observations for each class in either categorical features or the label.","title":"Stratified cross-validation"},{"location":"preparation/dev-model/#algorithm-exploration","text":"The algorithms you explore should be driven by your use-case. By first identifying what you are trying to achieve, you can then narrow the scope of searching for solutions. Although not a complete list of possible methods, below are links that introduce algorithms for Regression, Classification, Clustering, Recommendations, and Anomaly Detection. A colleague and I also created this tool to help guide algorithm selection ( Click here to visit Algorithm Explorer ).","title":"Algorithm Exploration"},{"location":"preparation/dev-model/#regression_1","text":"Regression algorithms are machine learning techniques for predicting continuous numerical values. They are supervised learning tasks which means they require labeled training examples. Further reading: Machine Learning: Trying to predict a numerical value","title":"Regression"},{"location":"preparation/dev-model/#classification_1","text":"Classification algorithms are machine learning techniques for predicting which category the input data belongs to. They are supervised learning tasks which means they require labeled training examples. Further reading: Machine Learning: Trying to predict a classify your data","title":"Classification"},{"location":"preparation/dev-model/#clustering","text":"Clustering algorithms are machine learning techniques to divide data into a number of groups where points in the groups have similar traits. They are unsupervised learning tasks and therefore do not require labeled training examples. Further reading: Machine Learning: Trying to discover structure in your data","title":"Clustering"},{"location":"preparation/dev-model/#recommendation-engines","text":"Recommendation Engines are created to predict a preference or rating that indicates a user\u2019s interest in an item/product. The algorithms used to create this system find similarities between either the users, the items, or both. Further reading: Machine Learning: Trying to make recommendations","title":"Recommendation Engines"},{"location":"preparation/dev-model/#anomaly-detection","text":"Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers. Further reading: Machine Learning: Trying to detect outliers or unusual behavior","title":"Anomaly Detection"},{"location":"preparation/dev-model/#hyper-parameter-optimization","text":"Although the terms parameters and hyper-parameters are occasionally used interchangeably, we are going to distinguish between the two. Parameters are properties the algorithm is learning during training. For linear regression, these are the weights and biases; whilst for random forests, these are the variables and thresholds at each node. Hyper-parameters , on the other hand, are properties that must be set before training. For k-means clustering, you must define the value of k; whilst for neural networks, an example is the learning rate. Hyper-parameter optimization is the process of finding the best possible values for these hyper-parameters to optimize your performance metric (e.g. highest accuracy, lowest RMSE, etc.). To do this, we train a model for different combinations of values and evaluate which find the best solution. Three methods used to search for the best combinations are Grid Search, Random Search, and Bayesian Optimization.","title":"Hyper-Parameter Optimization"},{"location":"preparation/dev-model/#grid-search","text":"You specify values for each hyper-parameter, and all combinations of those values will be evaluated. For example, if you wish to evaluate hyper-parameters for random forest, you could specify provide three options for the number of trees hyper-parameter (10, 20 and 50) and for the maximum depth of each tree you also provide three options (no limit, 10 and 20). This would result in a random forest model being built for each of the 9 possible combinations: (10, no limit), (10, 10), (10, 20), (20, no limit), (20, 10), (20, 20), (50, no limit), (50, 10), and (50, 20). The combination that provides the best performance will be those you use for your final model. Pros: Simple to use. You will find the best combination of the values you\u2019ve provided. You can run each of the experiments in parallel. Cons: Computationally expensive as so many models are being built. If a particular hyper-parameter is not important, you are exploring different possibilities unnecessarily.","title":"Grid Search"},{"location":"preparation/dev-model/#random-search","text":"You specify ranges or options for each hyper-parameter and random values of each are selected. Continuing with the random forest example, you might provide the range for the number of trees to be between 10 and 50 and max_depth to be either no limit, 10 or 20. This time, rather than it compute all permutations, you can specify the number of iterations you wish to run. Say we only want five, then we might test something like (19, 20), (32 no limit), (40, no limit), (10, 20), (27, 10). Pros: Simple to use. More efficient and can outperform grid search when only a few hyper-parameters affect the overall performance. You can run each of the experiments in parallel. Cons: It involves random sampling so will only find the best combination if it searches that space.","title":"Random Search"},{"location":"preparation/dev-model/#coarse-to-fine","text":"For both grid search and random search, you can also use the coarse to fine technique. This involves exploring a broader range of variables with wide intervals or all possible options. Once you\u2019ve got your results from this initial search you explore the results to see if there are any patterns or particular regions that look promising. If so, you can repeat the process but refine your search. For the random forest example above, we might notice that results are promising when the maximum depth has no limit and when the number of trees hyper-parameter is either 10 or 20. The search process is repeated but keeping the maximum depth hyper-parameter constant and increasing the granularity of the number of tree options, testing values of 12, 14, 16, 18, to see if we can find a better result. Pros: Can find more optimized hyper-parameters, improving the performance metric. Cons: Evaluation of the results to find the best regions to explore can be cumbersome.","title":"Coarse to Fine"},{"location":"preparation/dev-model/#bayesian-optimization","text":"Bayesian Optimization uses the prior knowledge of success with hyper-parameter combinations to choose the next best. The technique uses a machine learning approach, building a model where the hyper-parameters are the features and the performance is the target variable. After each experiment, a new data point is added and a new model built. It assumes similar combinations will have similar results and prioritize exploring regions where promising results have already been seen. However, it also takes into consideration uncertainty as a possibility for large gain; if there are large areas that have not yet been explored, it will also prioritize these as. Taking just one hyper-parameter, the number of trees, the algorithm might first try 10 and get a pretty good performance. It then tries 32 and the performance is significantly better. Bayesian optimization builds a model based on the first two data points to predict performance. The model is likely to be linear with just the two data points so the next value chosen is 40, with the expectation that the performance will continue to improve as the number of trees increase. It does not. Now, it builds another model that suggests there might be improvement around 32 where it has seen the best result so far, however, there\u2019s still a large gap between 10 and 32 that hasn\u2019t been explored and due to large uncertainty, it chooses 21. Again, the model is tweaked with this new data and another value chosen\u2026 Pros: Can find more optimized hyper-parameters, improving the performance metric. It can reduce the time spent searching for an optimum solution when the number of parameters is high and each experiment is computationally expensive. Cons: You cannot run each experiment in parallel as the next combination of hyper-parameter values is determined by the prior runs. It also requires tuning \u2014 choosing a scale for each hyper-parameter and an appropriate kernel.","title":"Bayesian Optimization"},{"location":"preparation/dev-model/#ensemble-learning","text":"Ensembles combine several machine learning models, each finding different patterns within the data, to provide a more accurate solution. These techniques can both improve performance, as they capture more trends, as well as reduce overfitting, as the final prediction is a consensus from many models.","title":"Ensemble Learning"},{"location":"preparation/dev-model/#bagging","text":"Bagging (bootstrap aggregations) is the method of building multiple models in parallel and average their prediction as the final prediction. These models can be built with the same algorithm (i.e. the Random Forest algorithm builds many decision trees) or you can build different types of models (e.g. a Linear Regression model and an SVM model).","title":"Bagging"},{"location":"preparation/dev-model/#boosting","text":"Boosting builds models sequentially evaluating the success of earlier models; the next model prioritizes learning trends for predicting examples that the current models perform poorly on. There are three common techniques for this: AdaBoost, Gradient Boosting and XGBoosted.","title":"Boosting"},{"location":"preparation/dev-model/#stacking","text":"Stacking involves building multiple models and using their outputs as features into a final model. For example, your goal is to create a classifier and you build a KNN model as well as a Na\u00efve Bayes model. Rather than choose between the two, you can pass their two predictions into a final Logistic Regression model. This final model may result in better results than the two intermediate models.","title":"Stacking"},{"location":"topology/","text":"Data Topology Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics. Core elements of a Data Topology There are three Core elements of a data topology: Zone Map Data Flow Data Layer Zones and Zone Maps Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation. Data Flow The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help to detect circular data flows occurring across zones, to be flagged for investigation as data integrity may potentially be compromised if not well managed (designed and governed). Data Layer The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, edge node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity, here data will likely be highly distributed across an organization and certainly across an enterprise. For example, mixed deployment data layers include: Cloud - Public cloud, unconstrained Fog - Private cloud, constrained by available infrastructure Edge - Smart device, the most constrained by the limitations of compute capabilities, network bandwidth and availability Characteristics to Consider When Designing a Data Topology Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints Keeping an organic data topology A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can be removed after a given period of time (such as removing data from a raw zone after 7 business days) A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added Leaf zone guides A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. the location (virtual or physical) should be singular. A conceptual non-leaf zone can be added, that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations. Simplifying security with leaf zones A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach. Enterprises and Organizations When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly looking \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the system includes data ingestion from a company providing social media data, you can\u2019t complain on data quality from a particular tweet if they spelt your company name wrong. You have to have a data quality assessment process in place correcting spelling mistakes or simply remove those data points.","title":"Data Topology"},{"location":"topology/#data-topology","text":"Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI, A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business from operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc. A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value. A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics.","title":"Data Topology"},{"location":"topology/#core-elements-of-a-data-topology","text":"There are three Core elements of a data topology: Zone Map Data Flow Data Layer","title":"Core elements of a Data Topology"},{"location":"topology/#zones-and-zone-maps","text":"Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data. Zones are inherently abstract and conceptual in nature as a zone is not a deployed object. A Zone Map identifies and names each zone. Figure 1 shows the primitive zones that are used in a zone map. It is only a leaf zone that is associated with the instantiation of data. Non-primitive zones include a virtual zone that may cluster multiple zones together and reflect groups for data virtualization or data federation.","title":"Zones and Zone Maps"},{"location":"topology/#data-flow","text":"The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the zones. This can also help to detect circular data flows occurring across zones, to be flagged for investigation as data integrity may potentially be compromised if not well managed (designed and governed).","title":"Data Flow"},{"location":"topology/#data-layer","text":"The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include: - Public cloud - On premise private cloud - Edge - Device We can think of this as being a cloud, fog, edge node topology, where the different nodes have different characteristics with compute power, storage capacity, and may be constrained by network connectivity, here data will likely be highly distributed across an organization and certainly across an enterprise. For example, mixed deployment data layers include: Cloud - Public cloud, unconstrained Fog - Private cloud, constrained by available infrastructure Edge - Smart device, the most constrained by the limitations of compute capabilities, network bandwidth and availability","title":"Data Layer"},{"location":"topology/#characteristics-to-consider-when-designing-a-data-topology","text":"Designing a data topology is an iterative process Group users (or end-points) into communities of interest to determine shared needs Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics Map and align communities of interest to data zones Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities Work backwards in the data pipeline to identify areas of synergy and re-use Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints","title":"Characteristics to Consider When Designing a Data Topology"},{"location":"topology/#keeping-an-organic-data-topology","text":"A data topology is intended to be organic in nature. Although a static topology is a choice, an organic data topology promotes the development of disciplines for addressing an enterprise with changing needs and priorities over time Zones can be regarded as being ephemeral or temporal New zones can be added as required A new zone can be added as a diagrammatic placeholder without instantiation Old zones can be removed or deprecated Covers zones that have been expired, sunsetted, retired, archived Leaf zones are intended to have independent aging policies, For example: data can be removed after a given period of time (such as removing data from a raw zone after 7 business days) A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added","title":"Keeping an organic data topology"},{"location":"topology/#leaf-zone-guides","text":"A leaf zone is the zone that reflects the instantiation of data. In that regard, the leaf zone is the least abstract or conceptual of all zone types. For simplicity purposes, general recommendations to consider when establishing a leaf zone are to: limit the database technology to a single type. the location (virtual or physical) should be singular. A conceptual non-leaf zone can be added, that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations.","title":"Leaf zone guides"},{"location":"topology/#simplifying-security-with-leaf-zones","text":"A leaf zone can also be used to help simplify certain complex security profiles. There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values. In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value. As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy. By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach.","title":"Simplifying security with leaf zones"},{"location":"topology/#enterprises-and-organizations","text":"When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise . An organization is often inwardly looking \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem. When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise. Within an organization, all data is often assumed to be governable through some type of data governance program. Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program. This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data. For example, if the system includes data ingestion from a company providing social media data, you can\u2019t complain on data quality from a particular tweet if they spelt your company name wrong. You have to have a data quality assessment process in place correcting spelling mistakes or simply remove those data points.","title":"Enterprises and Organizations"}]}