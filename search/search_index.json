{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data - AI - Analytics Reference Architecture In this reference architecture, we are focusing on defining architecture patterns and best practices to build data and AI intensive applications. We are addressing how to integrate data governance, machine learning practices and the full life cycle of a cloud native solution development under the same reference architecture to present a holistic point of view on how to do it. When we consider development of Data and AI intensive applications or intelligent Application it is helpful to think of how the combination of three underlying architecture patterns Cloud Native application architecture patterns Data architecture patterns AI architecture patterns provides the right foundation to enable us to develop these intelligent applications in a highly agile cloud native way. By considering the nature of joins between the architectures we can also understand how the different roles such as Software Engineer, Data Engineer, and Data Scientist relate and work together in the development of such solutions. The following diagram illustrates the top-level view of a Data centric and AI reference architecture. There is no argument about it, AI without Data will not exist. The architecture illustrates the need for strong data management capabilities inside a 'data platform', on which AI capabilities are plugged in. The data platform addresses the data collection and transformation to move data to local highly scalable store . But it is necessary to avoid moving data when there is no need to do transformations or no impact to the data sources by adding readers, so a virtualization capability is necessary to open a view on remote data sources. On the AI side, data scientists need to perform data analysis , which includes making sense of the data using data visualization . To build a model they need to define features, and the AI layer supports feature engineering . Then to build the model, the development environment helps to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to machine learning cluster . Once the model provides acceptable accuracy level, it can be published as a service. The model management capability supports the meta-data definition and the life cycle management of the model. When the model is deployed, monitring capability, ensures the model is still accurate and even not biased. The intelligent application on the right side, can run on cloud, fog, or mist. It accesses the deployed model, access Data using APIs, and even consumes pre-built models, congitive services, like a speech to text and text to speech service, an image recognition , a tone analyzer services, the Natural Language Understanding ( NLU ), and chatbot . Data is fundamental What makes managing data so challenging and complex is that, by itself, data doesn't do anything. Data is inert; it is not self-organizing or even self-understanding. In the DIKW pyramid, data is the base with the least amount of perceived usefulness. Information has higher value than data, knowledge has higher value than information, and wisdom has the highest perceived value of all. Data requires something else\u2014a program, a machine, or even a person\u2014to move up the value chain and become information. The IBM AI Ladder also begins with data. You get higher business value when you perform business-assisted functions such as analytics, machine learning, or artificial intelligence on top of the data. Data as a differentiator Your data needs to become a corporate asset. Data has the power to transform your organization, add monetary value, and enable your workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. The scale of preserved data across a complex hybrid cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, your organization can become data-centric and data-driven in a short time. Businesses need to move rapidly. Your organization must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, whether large or small. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects. Data platform Principles There is a spectrum from single source of the truth to data hyper personalisation . Fundamentally we need to embrace the fact that different roles need specialised data stores with redundancy and replication between them Exercise specialisation through connectivity Different application patterns apply different data specialisation. There is a clear dependency between AI and Data management, but for an intelligent application context there are a Data concern, a AI model management concern, a multi cloud deployment concerns. As you constrain scalability and network connectivity you also constrain data store, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in the time series. Collect, organize, and analyze data Today, our users may have access to terabytes, petabytes, or even exabytes of data. But if that data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed, that data is not just useless\u2014it can become a liability. These activities form the focus of data and analytics platforms, as shown in the three pillars of IBM Analytics offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a commonf application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods Making data enabled and active There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions. Patterns Intelligent application Business intelligence Stream analytics","title":"Introduction"},{"location":"#data-ai-analytics-reference-architecture","text":"In this reference architecture, we are focusing on defining architecture patterns and best practices to build data and AI intensive applications. We are addressing how to integrate data governance, machine learning practices and the full life cycle of a cloud native solution development under the same reference architecture to present a holistic point of view on how to do it. When we consider development of Data and AI intensive applications or intelligent Application it is helpful to think of how the combination of three underlying architecture patterns Cloud Native application architecture patterns Data architecture patterns AI architecture patterns provides the right foundation to enable us to develop these intelligent applications in a highly agile cloud native way. By considering the nature of joins between the architectures we can also understand how the different roles such as Software Engineer, Data Engineer, and Data Scientist relate and work together in the development of such solutions. The following diagram illustrates the top-level view of a Data centric and AI reference architecture. There is no argument about it, AI without Data will not exist. The architecture illustrates the need for strong data management capabilities inside a 'data platform', on which AI capabilities are plugged in. The data platform addresses the data collection and transformation to move data to local highly scalable store . But it is necessary to avoid moving data when there is no need to do transformations or no impact to the data sources by adding readers, so a virtualization capability is necessary to open a view on remote data sources. On the AI side, data scientists need to perform data analysis , which includes making sense of the data using data visualization . To build a model they need to define features, and the AI layer supports feature engineering . Then to build the model, the development environment helps to select and combine the different algorithms and to tune the hyper parameters. The execution can be done on local cluster or can be executed, at the big data scale level, to machine learning cluster . Once the model provides acceptable accuracy level, it can be published as a service. The model management capability supports the meta-data definition and the life cycle management of the model. When the model is deployed, monitring capability, ensures the model is still accurate and even not biased. The intelligent application on the right side, can run on cloud, fog, or mist. It accesses the deployed model, access Data using APIs, and even consumes pre-built models, congitive services, like a speech to text and text to speech service, an image recognition , a tone analyzer services, the Natural Language Understanding ( NLU ), and chatbot .","title":"Data - AI - Analytics Reference Architecture"},{"location":"#data-is-fundamental","text":"What makes managing data so challenging and complex is that, by itself, data doesn't do anything. Data is inert; it is not self-organizing or even self-understanding. In the DIKW pyramid, data is the base with the least amount of perceived usefulness. Information has higher value than data, knowledge has higher value than information, and wisdom has the highest perceived value of all. Data requires something else\u2014a program, a machine, or even a person\u2014to move up the value chain and become information. The IBM AI Ladder also begins with data. You get higher business value when you perform business-assisted functions such as analytics, machine learning, or artificial intelligence on top of the data.","title":"Data is fundamental"},{"location":"#data-as-a-differentiator","text":"Your data needs to become a corporate asset. Data has the power to transform your organization, add monetary value, and enable your workforce to accomplish extraordinary things. Data-driven cultures can realize higher business returns. The scale of preserved data across a complex hybrid cloud topology requires discipline, even for an organization that embraces agile and adaptive philosophies. Data can and should be used to drive analytical insights. But what considerations and planning activities are required to enable the generation of insights, the ability to take action, and the courage to make decisions? Although the planning and implementation activities to maximize the usefulness of your data can require some deep thinking, your organization can become data-centric and data-driven in a short time. Businesses need to move rapidly. Your organization must respond to changing needs as quickly as possible or risk becoming irrelevant. This applies to both private or public organizations, whether large or small. Data and the related analytics are key to differentiation, but traditional approaches are often ad hoc, naive, complex, difficult, and brittle. This can result in delays, business challenges, lost opportunities, and the rise of unauthorized projects.","title":"Data as a differentiator"},{"location":"#data-platform","text":"","title":"Data platform"},{"location":"#principles","text":"There is a spectrum from single source of the truth to data hyper personalisation . Fundamentally we need to embrace the fact that different roles need specialised data stores with redundancy and replication between them Exercise specialisation through connectivity Different application patterns apply different data specialisation. There is a clear dependency between AI and Data management, but for an intelligent application context there are a Data concern, a AI model management concern, a multi cloud deployment concerns. As you constrain scalability and network connectivity you also constrain data store, data structure and data access. The value and way of storing and representing data may change with its age. Value also comes in the recognition of patterns in the time series.","title":"Principles"},{"location":"#collect-organize-and-analyze-data","text":"Today, our users may have access to terabytes, petabytes, or even exabytes of data. But if that data is not collected, organized, managed, controlled, enriched, governed, measured, and analyzed, that data is not just useless\u2014it can become a liability. These activities form the focus of data and analytics platforms, as shown in the three pillars of IBM Analytics offerings: Hybrid Data Management Collect all types of data, structured and unstructured Include all open sources of data Single platform with a commonf application layer Write once and deploy anywhere Unified Governance and Integration Satisfy all matters of finding, cataloging and masking data Integrate fluid data sets Deliver built-in compliance Leverage advanced machine learning capabilities Data Science and Business Analytics Deliver descriptive, prescriptive and predictive insights across all types of data Enable advanced analytics and data science methods","title":"Collect, organize, and analyze data"},{"location":"#making-data-enabled-and-active","text":"There are five key tenets to making data enabled and active: Developing a data strategy Developing a data architecture Developing a data topology for analytics Developing an approach to unified governance Developing an approach to maximizing the accessibility of data consumption If data is an enabler, then analytics can be considered one of the core capabilities that is being enabled. Analytics can be a complex and involved discipline that encompasses a broad and diverse set of tools, methods, and techniques. One end of the IBM AI Ladder is enabled through data in a static format such as a pre-built report; the other end is enabled through deep-learning and advanced artificial intelligence. Between these two ends, the enablement methods include diagnostic analytics, machine learning, statistics, qualitative analysis, cognitive analysis, and more. A robot, a software interface, or a human may need to apply multiple techniques within a single task or across the role that they perform in driving insight, taking action, monitoring, and making decisions.","title":"Making data enabled and active"},{"location":"#patterns","text":"","title":"Patterns"},{"location":"#intelligent-application","text":"","title":"Intelligent application"},{"location":"#business-intelligence","text":"","title":"Business intelligence"},{"location":"#stream-analytics","text":"","title":"Stream analytics"},{"location":"deployment/","text":"Deployment","title":"Physical model execution"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"methodology/","text":"Methodology Data Sciences Introduction The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper managers, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories: Descriptive analytics This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years. Predictive analytics This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels. Prescriptive analytics Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis. Concepts Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (y), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs Regression classification problem when the goal is to predict continuous value output A feature is an attribute to use for classifying Algorithm selection The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem. Challenges There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. Are we considering a scoring service or a classification one? Is it a batch processing to update static records or real time processing on data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model and data and code: version management How to assess the features needed for the training and test sets. How to leverage real time cognitive / deep learning classification inside scoring service The Garage Method for Cloud with DataFirst Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem. Personas Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist Differences between analysts and data scientists The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of adta Structured mostly numeric data All types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Typical educationl background Mindset --- --- ---","title":"Methodology"},{"location":"methodology/#methodology","text":"","title":"Methodology"},{"location":"methodology/#data-sciences-introduction","text":"The goals for data science is to infer from data, actionable insights for the business execution improvement. The main stakeholders are business users, upper managers, who want to get improvement to some important metrics and indicators to control their business goals and objectives. Data scientists have to work closely with business users and be able to explain and represent findings clearly and with good visualization, pertinent for the business users. Data science falls into these three categories:","title":"Data Sciences Introduction"},{"location":"methodology/#descriptive-analytics","text":"This is likely the most common type of analytics leveraged to create dashboards and reports. They describe and summarize events that have already occurred. For example, think of a grocery store owner who wants to know how items of each product were sold in all store within a region in the last five years.","title":"Descriptive analytics"},{"location":"methodology/#predictive-analytics","text":"This is all about using mathematical and statistical methods to forecast future outcomes. The grocery store owner wants to understand how many products could potentially be sold in the next couple of months so that he can make a decision on inventory levels.","title":"Predictive analytics"},{"location":"methodology/#prescriptive-analytics","text":"Prescriptive analytics is used to optimize business decisions by simulating scenarios based on a set of constraints. The grocery store owner wants to creating a staffing schedule for his employees, but to do so he will have to account for factors like availability, vacation time, number of hours of work, potential emergencies and so on (constraints) and create a schedule that works for everyone while ensuring that his business is able to function on a day to day basis.","title":"Prescriptive analytics"},{"location":"methodology/#concepts","text":"Supervised learning : learn a model from labeled training data that allows us to make predictions about unseen or future data. We give to the algorithm a dataset with a right answers (y), during the training, and we validate the model accuracy with a test data set with right answers. So a data set needs to be split in training and test sets. Unsupervised learning : giving a dataset, try to find tendency in the data, by using techniques like clustering. Classification problem is when we are trying to predict one of a small number of discrete-valued outputs Regression classification problem when the goal is to predict continuous value output A feature is an attribute to use for classifying","title":"Concepts"},{"location":"methodology/#algorithm-selection","text":"The application from https://samrose3.github.io/algorithm-explorer will guide you on how to select what algorithm may help to address a specific problem.","title":"Algorithm selection"},{"location":"methodology/#challenges","text":"There are a set of standard challenges while developing an IT solution which integrates results from analytics model. We are listing some that we want to address, document and support as requirements. Are we considering a scoring service or a classification one? Is it a batch processing to update static records or real time processing on data stream or transactional data How to control resource allocation for Machine Learning job. How to manage consistency between model and data and code: version management How to assess the features needed for the training and test sets. How to leverage real time cognitive / deep learning classification inside scoring service","title":"Challenges"},{"location":"methodology/#the-garage-method-for-cloud-with-datafirst","text":"Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The Garage Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.","title":"The Garage Method for Cloud with DataFirst"},{"location":"methodology/#personas","text":"Our architecture and mothodology discussions need to clearly address the major personas touching any elements of the architecture: Developer Architect Business analysts Data scientist","title":"Personas"},{"location":"methodology/#differences-between-analysts-and-data-scientists","text":"The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below presents the results. Analysts Data Scientists Types of adta Structured mostly numeric data All types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Typical educationl background Mindset --- --- ---","title":"Differences between analysts and data scientists"},{"location":"monitoring/","text":"Monitoring","title":"Bias monitoring to ensure equitable"},{"location":"monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"preparation/","text":"Data Preparation","title":"Model selection"},{"location":"preparation/#data-preparation","text":"","title":"Data Preparation"},{"location":"runtimes/","text":"Runtime architectures Intelligent application and real time analytics The following diagram illustrates the high-level view of a typical intellingent applicatin involving real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in the next section. The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsibles to have its own data source. A scoring service, most likely a function as service, runs the model. The action to perform once the scoring is returned, is business rules intensif, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: The event sources could be IoT devices, or Edge computing in mist. A first level of data transformation can be performed at the edge level in the mist. This is an optional level. A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics apply a machine learning model on the event data. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is not strong devision on the accuracy or responses are not bias. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format. Modeling architecture In the following diagram we are how to gather the data for exposing to the Data Scientist all the needed data from the different data sources: being public data, enterprise - private, or coming from event backbone. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Runtime application"},{"location":"runtimes/#runtime-architectures","text":"","title":"Runtime architectures"},{"location":"runtimes/#intelligent-application-and-real-time-analytics","text":"The following diagram illustrates the high-level view of a typical intellingent applicatin involving real time analytics like a predictive scoring service, and event stream analytics. The components involved in this diagram represents a typical cloud native application using different microservices and scoring function built around a model created by one or more Data Scientist using machine learning techniques. We address how to support the model creation in the next section. The top of a diagram defines the components of an intelligent web application: End user is an internal or external business user. It uses a single page application, or mobile application connected to a back-end to front end service. The back-end to front end service delivers data model for the user interface and perform service orchestration. As a cloud native app, and applying clear separation of concerns, we have different microservices, each being responsibles to have its own data source. A scoring service, most likely a function as service, runs the model. The action to perform once the scoring is returned, is business rules intensif, so as best practice, we propose to externalize the rule execution within a decision service. The lower part of the diagram illustrates real time analytics on event streams, and continuous data injection from event sources: The event sources could be IoT devices, or Edge computing in mist. A first level of data transformation can be performed at the edge level in the mist. This is an optional level. A second level of data transformation, run in the fog, and publish events to an event backbone while also persisting data to a data repository for future analytics work. The event backbone is used for microservice to microservice communication, as a event sourcing capability, and support pub/sub protocol. One potential consumer of those events could be a streaming analytics apply a machine learning model on the event data. The action part of this streaming analytics component is to publish enriched events. Aggregates, count, any analytics computations can be done in real time. The AI Analytics layer needs to include a component to assess the performance of the model, and for example ensure there is not strong devision on the accuracy or responses are not bias. Finally, as we address data injection, there are patterns where the data transformation is done post event backbone to persist data in yet another format.","title":"Intelligent application and real time analytics"},{"location":"runtimes/#modeling-architecture","text":"In the following diagram we are how to gather the data for exposing to the Data Scientist all the needed data from the different data sources: being public data, enterprise - private, or coming from event backbone. The data transformation can be controled by a workflow engine, using timer to trigger operations during down time. The data repositories represent a data lake to be used as source for defining training and test sets for the machine learning. The model building integrates with a ML cluster to run the different algorithm for selecting the best model.","title":"Modeling architecture"},{"location":"topology/","text":"Data Topology","title":"Model representation"},{"location":"topology/#data-topology","text":"","title":"Data Topology"}]}